// ==================== CLASS DEFINITIONS ====================

/**
 * Service for ingesting documents into the vector store using LangChain4j
 */
@ApplicationScoped
public class DocumentIngestionService {
    // Implementation details...
}

// ==================== RECORD DEFINITIONS ====================

record IngestResult(
    boolean success,
    int documentsIngested,
    int segmentsCreated,
    long durationMs,
    String message
) {}

record DocumentSource(
    SourceType type,
    String path,
    String content,
    Map<String, String> metadata
) {}

// ==================== ORGANIZED STRUCTURE ====================

/**
 * ============================================================================
 * COMPLETE RAG SYSTEM STRUCTURE
 * ============================================================================
 */

// 1. MAIN SERVICE CLASSES
public class DocumentIngestionService { /* Implementation */ }
public class RagQueryService { /* Implementation */ }
public class RagExecutionService { /* Implementation */ }
public class LangChain4jModelFactory { /* Implementation */ }
public class LangChain4jEmbeddingStoreFactory { /* Implementation */ }
public class RagUsageExamples { /* Implementation */ }

// 2. RECORD DEFINITIONS (Data Classes)
record IngestResult(boolean success, int documentsIngested, int segmentsCreated, long durationMs, String message) {}
record DocumentSource(SourceType type, String path, String content, Map<String, String> metadata) {}
record RagQueryRequest(String tenantId, String query, RagMode ragMode, SearchStrategy searchStrategy,
                      RetrievalConfig retrievalConfig, GenerationConfig generationConfig,
                      List<String> collections, Map<String, Object> filters) {}
record ConversationTurn(String userMessage, String assistantMessage, Instant timestamp) {}

// 3. ENUM DEFINITIONS
enum SourceType { PDF, TEXT, URL, MARKDOWN, HTML }
enum RagMode { STANDARD, AGENT, MULTI_HOP, HYBRID }
enum SearchStrategy { SEMANTIC, HYBRID, SEMANTIC_RERANK, MULTI_QUERY }
enum ChunkingStrategy { RECURSIVE, SENTENCE, PARAGRAPH }
enum RerankingModel { COHERE_RERANK, JINA_AI_RERANK, MIXEDBREAD_RERANK }
enum CitationStyle { INLINE_NUMBERED, FOOTNOTE, APA, MLA }

// 4. CONFIGURATION EXAMPLES (from application.properties)
/*
 * # LangChain4j OpenAI Configuration
 * langchain4j.openai.api-key=${OPENAI_API_KEY}
 *
 * # LangChain4j Anthropic Configuration
 * langchain4j.anthropic.api-key=${ANTHROPIC_API_KEY}
 *
 * # LangChain4j Azure OpenAI Configuration
 * langchain4j.azure.api-key=${AZURE_OPENAI_API_KEY}
 * langchain4j.azure.endpoint=${AZURE_OPENAI_ENDPOINT}
 * langchain4j.azure.chat-deployment=gpt-4
 * langchain4j.azure.embedding-deployment=text-embedding-ada-002
 *
 * # LangChain4j Logging
 * langchain4j.log-requests=false
 * langchain4j.log-responses=false
 *
 * # Vector Store Configuration
 * langchain4j.vectorstore.backend=postgres
 * langchain4j.vectorstore.embedding-dimension=1536
 *
 * # PostgreSQL Configuration
 * langchain4j.vectorstore.postgres.host=localhost
 * langchain4j.vectorstore.postgres.port=5432
 * langchain4j.vectorstore.postgres.database=silat
 * langchain4j.vectorstore.postgres.user=postgres
 * langchain4j.vectorstore.postgres.password=${DB_PASSWORD}
 * langchain4j.vectorstore.postgres.table=embeddings
 *
 * # Pinecone Configuration (alternative)
 * langchain4j.vectorstore.pinecone.api-key=${PINECONE_API_KEY}
 * langchain4j.vectorstore.pinecone.environment=${PINECONE_ENVIRONMENT}
 * langchain4j.vectorstore.pinecone.project-id=${PINECONE_PROJECT_ID}
 * langchain4j.vectorstore.pinecone.index=silat
 *
 * # Weaviate Configuration (alternative)
 * langchain4j.vectorstore.weaviate.api-key=${WEAVIATE_API_KEY}
 * langchain4j.vectorstore.weaviate.scheme=https
 * langchain4j.vectorstore.weaviate.host=weaviate-cluster.weaviate.network
 * langchain4j.vectorstore.weaviate.class-name=Document
 */

record RagQueryRequest(
    String tenantId,
    String query,
    RagMode ragMode,
    SearchStrategy searchStrategy,
    RetrievalConfig retrievalConfig,
    GenerationConfig generationConfig,
    List<String> collections,
    Map<String, Object> filters
) {}

record ConversationTurn(
    String userMessage,
    String assistantMessage,
    Instant timestamp
) {}

// ==================== COMPLETE USAGE EXAMPLES ====================

/**
 * Complete examples showing how to use the RAG system
 */
public class RagUsageExamples {
    
    /**
     * Example 1: Simple document ingestion and query
     */
    public static void example1_SimpleRag(
            DocumentIngestionService ingestionService,
            RagQueryService queryService) {
        
        String tenantId = "acme-corp";
        
        // 1. Ingest documents
        List<Path> pdfPaths = List.of(
            Path.of("/docs/product-manual.pdf"),
            Path.of("/docs/faq.pdf")
        );
        
        Map<String, String> metadata = Map.of(
            "collection", "product-docs",
            "category", "support"
        );
        
        ingestionService.ingestPdfDocuments(tenantId, pdfPaths, metadata)
            .subscribe().with(
                result -> System.out.printf(
                    "Ingested %d documents, created %d segments%n",
                    result.documentsIngested(),
                    result.segmentsCreated()
                ),
                error -> System.err.println("Ingestion failed: " + error)
            );
        
        // 2. Query the documents
        String query = "How do I reset my password?";
        
        queryService.query(tenantId, query, "product-docs")
            .subscribe().with(
                response -> {
                    System.out.println("Answer: " + response.answer());
                    System.out.println("\nSources:");
                    response.sourceDocuments().forEach(doc ->
                        System.out.println("  - " + doc.getTitle())
                    );
                },
                error -> System.err.println("Query failed: " + error)
            );
    }
    
    /**
     * Example 2: Advanced RAG with custom configuration
     */
    public static void example2_AdvancedRag(RagQueryService queryService) {
        
        String tenantId = "enterprise-client";
        String query = "What are the security best practices for API integration?";
        
        // Custom retrieval configuration
        RetrievalConfig retrievalConfig = new RetrievalConfig(
            10,           // topK
            0.75f,        // minSimilarity
            1024,         // maxChunkSize
            100,          // chunkOverlap
            true,         // enableReranking
            RerankingModel.COHERE_RERANK,
            true,         // enableHybridSearch
            0.6f,         // hybridAlpha (favor vector search)
            true,         // enableMultiQuery
            5,            // numQueryVariations
            false, 2048, Map.of(), List.of(), true, true
        );
        
        // Custom generation configuration
        GenerationConfig generationConfig = new GenerationConfig(
            "anthropic",
            "claude-3-sonnet-20240229",
            0.5f,         // temperature (more focused)
            2048,         // maxTokens
            0.95f, 0.0f, 0.0f,
            List.of(),
            "You are a security expert. Provide detailed, accurate answers with best practices.",
            null,
            false, true, CitationStyle.INLINE_NUMBERED,
            false, true, Map.of()
        );
        
        RagQueryRequest request = new RagQueryRequest(
            tenantId,
            query,
            RagMode.STANDARD,
            SearchStrategy.SEMANTIC_RERANK,
            retrievalConfig,
            generationConfig,
            List.of("security-docs", "api-docs"),
            Map.of("verified", true)
        );
        
        queryService.advancedQuery(request)
            .subscribe().with(
                response -> {
                    System.out.println("Answer with citations:");
                    System.out.println(response.answer());
                    System.out.println("\nMetrics:");
                    System.out.println("  Duration: " + 
                        response.metrics().totalDurationMs() + "ms");
                    System.out.println("  Docs retrieved: " + 
                        response.metrics().documentsRetrieved());
                    System.out.println("  Tokens generated: " + 
                        response.metrics().tokensGenerated());
                },
                error -> System.err.println("Query failed: " + error)
            );
    }
    
    /**
     * Example 3: Conversational RAG
     */
    public static void example3_ConversationalRag(RagQueryService queryService) {
        
        String tenantId = "support-bot";
        String sessionId = UUID.randomUUID().toString();
        
        List<ConversationTurn> history = new ArrayList<>();
        
        // First turn
        String query1 = "What is our refund policy?";
        queryService.conversationalQuery(tenantId, query1, sessionId, history)
            .subscribe().with(
                response -> {
                    history.add(new ConversationTurn(
                        query1,
                        response.answer(),
                        Instant.now()
                    ));
                    
                    // Second turn with context
                    String query2 = "How long does it take?";
                    queryService.conversationalQuery(tenantId, query2, sessionId, history)
                        .subscribe().with(
                            response2 -> System.out.println("Answer: " + response2.answer()),
                            error -> System.err.println("Query failed: " + error)
                        );
                },
                error -> System.err.println("Query failed: " + error)
            );
    }
    
    /**
     * Example 4: Batch document ingestion
     */
    public static void example4_BatchIngestion(DocumentIngestionService ingestionService) {
        
        String tenantId = "knowledge-base";
        
        List<DocumentSource> sources = List.of(
            new DocumentSource(
                SourceType.PDF,
                "/docs/handbook.pdf",
                null,
                Map.of("collection", "handbook", "version", "2024")
            ),
            new DocumentSource(
                SourceType.TEXT,
                null,
                "Company mission: We build great products...",
                Map.of("collection", "about", "type", "mission")
            ),
            new DocumentSource(
                SourceType.URL,
                "https://example.com/docs",
                null,
                Map.of("collection", "external", "source", "website")
            )
        );
        
        ingestionService.batchIngest(tenantId, sources)
            .subscribe().with(
                result -> System.out.printf(
                    "Batch ingestion completed: %d docs, %d segments in %dms%n",
                    result.documentsIngested(),
                    result.segmentsCreated(),
                    result.durationMs()
                ),
                error -> System.err.println("Batch ingestion failed: " + error)
            );
    }
}

/**
 * ============================================================================
 * APPLICATION.PROPERTIES CONFIGURATION
 * ============================================================================
 *
 * # LangChain4j OpenAI Configuration
 * langchain4j.openai.api-key=${OPENAI_API_KEY}
 * 
 * # LangChain4j Anthropic Configuration
 * langchain4j.anthropic.api-key=${ANTHROPIC_API_KEY}
 * 
 * # LangChain4j Azure OpenAI Configuration
 * langchain4j.azure.api-key=${AZURE_OPENAI_API_KEY}
 * langchain4j.azure.endpoint=${AZURE_OPENAI_ENDPOINT}
 * langchain4j.azure.chat-deployment=gpt-4
 * langchain4j.azure.embedding-deployment=text-embedding-ada-002
 * 
 * # LangChain4j Logging
 * langchain4j.log-requests=false
 * langchain4j.log-responses=false
 * 
 * # Vector Store Configuration
 * langchain4j.vectorstore.backend=postgres
 * langchain4j.vectorstore.embedding-dimension=1536
 * 
 * # PostgreSQL Configuration
 * langchain4j.vectorstore.postgres.host=localhost
 * langchain4j.vectorstore.postgres.port=5432
 * langchain4j.vectorstore.postgres.database=silat
 * langchain4j.vectorstore.postgres.user=postgres
 * langchain4j.vectorstore.postgres.password=${DB_PASSWORD}
 * langchain4j.vectorstore.postgres.table=embeddings
 * 
 * # Pinecone Configuration (alternative)
 * langchain4j.vectorstore.pinecone.api-key=${PINECONE_API_KEY}
 * langchain4j.vectorstore.pinecone.environment=${PINECONE_ENVIRONMENT}
 * langchain4j.vectorstore.pinecone.project-id=${PINECONE_PROJECT_ID}
 * langchain4j.vectorstore.pinecone.index=silat
 * 
 * # Weaviate Configuration (alternative)
 * langchain4j.vectorstore.weaviate.api-key=${WEAVIATE_API_KEY}
 * langchain4j.vectorstore.weaviate.scheme=https
 * langchain4j.vectorstore.weaviate.host=weaviate-cluster.weaviate.network
 * langchain4j.vectorstore.weaviate.class-name=Document
 */

package tech.kayys.silat.executor.rag.langchain;

import dev.langchain4j.data.document.Document;
import dev.langchain4j.data.document.DocumentSplitter;
import dev.langchain4j.data.document.splitter.DocumentSplitters;
import dev.langchain4j.data.embedding.Embedding;
import dev.langchain4j.data.message.AiMessage;
import dev.langchain4j.data.message.ChatMessage;
import dev.langchain4j.data.message.SystemMessage;
import dev.langchain4j.data.message.UserMessage;
import dev.langchain4j.data.segment.TextSegment;
import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.embedding.EmbeddingModel;
import dev.langchain4j.model.openai.OpenAiChatModel;
import dev.langchain4j.model.openai.OpenAiEmbeddingModel;
import dev.langchain4j.model.anthropic.AnthropicChatModel;
import dev.langchain4j.model.azure.AzureOpenAiChatModel;
import dev.langchain4j.model.azure.AzureOpenAiEmbeddingModel;
import dev.langchain4j.rag.DefaultRetrievalAugmentor;
import dev.langchain4j.rag.RetrievalAugmentor;
import dev.langchain4j.rag.content.Content;
import dev.langchain4j.rag.content.retriever.ContentRetriever;
import dev.langchain4j.rag.content.retriever.EmbeddingStoreContentRetriever;
import dev.langchain4j.rag.query.Query;
import dev.langchain4j.rag.query.router.QueryRouter;
import dev.langchain4j.rag.query.transformer.QueryTransformer;
import dev.langchain4j.store.embedding.EmbeddingStore;
import dev.langchain4j.store.embedding.pgvector.PgVectorEmbeddingStore;
import dev.langchain4j.store.embedding.pinecone.PineconeEmbeddingStore;
import dev.langchain4j.store.embedding.weaviate.WeaviateEmbeddingStore;

import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tech.kayys.silat.executor.AbstractWorkflowExecutor;
import tech.kayys.silat.executor.Executor;
import tech.kayys.silat.executor.rag.domain.*;
import tech.kayys.silat.core.domain.*;
import tech.kayys.silat.core.engine.NodeExecutionResult;
import tech.kayys.silat.core.engine.NodeExecutionTask;
import tech.kayys.silat.core.scheduler.CommunicationType;

import java.time.Duration;
import java.time.Instant;
import java.util.*;
import java.util.stream.Collectors;

/**
 * ============================================================================
 * SILAT RAG EXECUTOR WITH LANGCHAIN4J
 * ============================================================================
 * 
 * Production-ready RAG implementation using LangChain4j framework.
 * 
 * Features:
 * - Seamless LangChain4j integration
 * - Multi-provider support (OpenAI, Anthropic, Azure)
 * - Advanced retrieval with query transformation
 * - Multiple embedding stores (PgVector, Pinecone, Weaviate)
 * - Custom retrieval augmentors
 * - Query routing and transformation
 * - Document splitting strategies
 * - Metadata filtering
 * - Hybrid search capabilities
 */
@Executor(
    executorType = "rag-langchain4j-executor",
    communicationType = CommunicationType.GRPC,
    maxConcurrentTasks = 50,
    supportedNodeTypes = {"RAG_QUERY", "SEMANTIC_SEARCH", "QA_GENERATION"},
    version = "2.0.0"
)
@ApplicationScoped
public class LangChain4jRagExecutor extends AbstractWorkflowExecutor {
    
    private static final Logger LOG = LoggerFactory.getLogger(LangChain4jRagExecutor.class);
    
    @Inject
    LangChain4jModelFactory modelFactory;
    
    @Inject
    LangChain4jEmbeddingStoreFactory storeFactory;
    
    @Inject
    LangChain4jQueryTransformerFactory transformerFactory;
    
    @Inject
    LangChain4jContentRetrieverFactory retrieverFactory;
    
    @Inject
    LangChain4jCacheService cacheService;
    
    @Inject
    LangChain4jMetricsCollector metricsCollector;
    
    @Override
    public Uni<NodeExecutionResult> execute(NodeExecutionTask task) {
        long startTime = System.currentTimeMillis();
        
        LOG.info("Executing LangChain4j RAG task: run={}, node={}", 
            task.runId().value(), task.nodeId().value());
        
        return Uni.createFrom().item(() -> parseRagRequest(task))
            .flatMap(request -> 
                // Check cache first
                cacheService.get(request)
                    .onItem().ifNotNull().transform(cached -> {
                        LOG.info("Cache hit for query: {}", request.query());
                        metricsCollector.recordCacheHit();
                        return cached;
                    })
                    .onItem().ifNull().switchTo(() -> 
                        // Execute RAG pipeline with LangChain4j
                        executeRagWithLangChain4j(request, startTime)
                            .flatMap(response -> 
                                cacheService.put(request, response)
                                    .replaceWith(response)
                            )
                    )
            )
            .map(response -> successResult(task, response))
            .onFailure().recoverWithItem(error -> {
                LOG.error("LangChain4j RAG execution failed", error);
                metricsCollector.recordFailure();
                return failureResult(task, error);
            });
    }
    
    /**
     * Execute RAG pipeline using LangChain4j components
     */
    private Uni<RagResponse> executeRagWithLangChain4j(
            RagRequest request, 
            long startTime) {
        
        return Uni.createFrom().item(() -> {
            Map<String, Long> timings = new HashMap<>();
            Instant timestamp = Instant.now();
            
            // Stage 1: Get models from factory
            long modelStart = System.currentTimeMillis();
            ChatLanguageModel chatModel = modelFactory.createChatModel(
                request.generationConfig()
            );
            EmbeddingModel embeddingModel = modelFactory.createEmbeddingModel(
                request.tenantId(),
                getEmbeddingModelName(request)
            );
            timings.put("modelInit", System.currentTimeMillis() - modelStart);
            
            // Stage 2: Get embedding store
            long storeStart = System.currentTimeMillis();
            EmbeddingStore<TextSegment> embeddingStore = storeFactory.getStore(
                request.tenantId(),
                request.retrievalConfig()
            );
            timings.put("storeInit", System.currentTimeMillis() - storeStart);
            
            // Stage 3: Create content retriever with advanced features
            long retrieverStart = System.currentTimeMillis();
            ContentRetriever contentRetriever = createContentRetriever(
                embeddingStore,
                embeddingModel,
                request
            );
            timings.put("retrieverInit", System.currentTimeMillis() - retrieverStart);
            
            // Stage 4: Create retrieval augmentor
            long augmentorStart = System.currentTimeMillis();
            RetrievalAugmentor retrievalAugmentor = createRetrievalAugmentor(
                contentRetriever,
                request
            );
            timings.put("augmentorInit", System.currentTimeMillis() - augmentorStart);
            
            // Stage 5: Build chat messages
            List<ChatMessage> messages = buildChatMessages(request);
            
            // Stage 6: Augment query with retrieved content
            long augmentStart = System.currentTimeMillis();
            dev.langchain4j.rag.AugmentationResult augmentationResult = 
                retrievalAugmentor.augment(
                    new UserMessage(request.query()),
                    new HashMap<>()
                );
            timings.put("augmentation", System.currentTimeMillis() - augmentStart);
            
            // Stage 7: Generate response
            long generationStart = System.currentTimeMillis();
            dev.langchain4j.model.output.Response<AiMessage> aiResponse = 
                chatModel.generate(messages);
            timings.put("generation", System.currentTimeMillis() - generationStart);
            
            String answer = aiResponse.content().text();
            
            // Stage 8: Extract retrieved documents
            List<RetrievedDocument> retrievedDocs = extractRetrievedDocuments(
                augmentationResult.contents()
            );
            
            // Stage 9: Extract citations
            List<Citation> citations = request.generationConfig().enableCitations() ?
                extractCitations(answer, retrievedDocs) :
                List.of();
            
            // Stage 10: Build metrics
            RagMetrics metrics = buildMetrics(
                timings,
                retrievedDocs,
                answer,
                aiResponse.tokenUsage()
            );
            
            metricsCollector.recordSuccess(metrics);
            
            return new RagResponse(
                request.query(),
                answer,
                retrievedDocs,
                citations,
                metrics,
                request.sessionId(),
                timestamp,
                request.metadata(),
                List.of(),
                Optional.empty()
            );
            
        }).onFailure().transform(error -> {
            LOG.error("LangChain4j RAG pipeline failed", error);
            return new RuntimeException("RAG execution failed: " + error.getMessage(), error);
        });
    }
    
    /**
     * Create advanced content retriever with LangChain4j
     */
    private ContentRetriever createContentRetriever(
            EmbeddingStore<TextSegment> embeddingStore,
            EmbeddingModel embeddingModel,
            RagRequest request) {
        
        RetrievalConfig config = request.retrievalConfig();
        
        // Build content retriever with filters
        EmbeddingStoreContentRetriever.Builder builder = 
            EmbeddingStoreContentRetriever.builder()
                .embeddingStore(embeddingStore)
                .embeddingModel(embeddingModel)
                .maxResults(config.topK())
                .minScore(config.minSimilarity());
        
        // Add metadata filter if provided
        if (!request.filters().isEmpty()) {
            builder.filter(metadata -> 
                matchesFilters(metadata.toMap(), request.filters())
            );
        }
        
        return builder.build();
    }
    
    /**
     * Create retrieval augmentor with query transformation
     */
    private RetrievalAugmentor createRetrievalAugmentor(
            ContentRetriever contentRetriever,
            RagRequest request) {
        
        DefaultRetrievalAugmentor.Builder builder = 
            DefaultRetrievalAugmentor.builder()
                .contentRetriever(contentRetriever);
        
        // Add query transformer for multi-query strategy
        if (request.searchStrategy() == SearchStrategy.MULTI_QUERY &&
            request.retrievalConfig().enableMultiQuery()) {
            
            QueryTransformer queryTransformer = transformerFactory
                .createMultiQueryTransformer(
                    request.retrievalConfig().numQueryVariations()
                );
            builder.queryTransformer(queryTransformer);
        }
        
        // Add query router for collection-based routing
        if (request.allowedCollections() != null && 
            !request.allowedCollections().isEmpty()) {
            
            QueryRouter queryRouter = transformerFactory
                .createCollectionRouter(request.allowedCollections());
            builder.queryRouter(queryRouter);
        }
        
        return builder.build();
    }
    
    /**
     * Build chat messages for LangChain4j
     */
    private List<ChatMessage> buildChatMessages(RagRequest request) {
        List<ChatMessage> messages = new ArrayList<>();
        
        // Add system message if provided
        if (request.generationConfig().systemPrompt() != null) {
            messages.add(new SystemMessage(
                request.generationConfig().systemPrompt()
            ));
        }
        
        // Add user message
        messages.add(new UserMessage(request.query()));
        
        return messages;
    }
    
    /**
     * Extract retrieved documents from LangChain4j Content
     */
    private List<RetrievedDocument> extractRetrievedDocuments(
            List<Content> contents) {
        
        return contents.stream()
            .map(content -> {
                TextSegment segment = content.textSegment();
                
                Map<String, Object> metadata = segment.metadata() != null ?
                    segment.metadata().toMap() : new HashMap<>();
                
                return new RetrievedDocument(
                    metadata.getOrDefault("id", UUID.randomUUID()).toString(),
                    segment.text(),
                    metadata,
                    content.score() != null ? content.score().floatValue() : 0f,
                    0f, // rerank score
                    (int) metadata.getOrDefault("chunkIndex", 0),
                    (String) metadata.getOrDefault("collection", "default"),
                    (String) metadata.get("sourceId"),
                    (String) metadata.get("sourceType"),
                    Instant.now(),
                    List.of(),
                    Map.of()
                );
            })
            .collect(Collectors.toList());
    }
    
    /**
     * Extract citations from generated answer
     */
    private List<Citation> extractCitations(
            String answer,
            List<RetrievedDocument> documents) {
        
        List<Citation> citations = new ArrayList<>();
        
        // Simple citation extraction (can be enhanced)
        for (int i = 0; i < documents.size(); i++) {
            RetrievedDocument doc = documents.get(i);
            
            // Check if document content appears in answer
            if (answer.contains(doc.content().substring(
                0, Math.min(50, doc.content().length())))) {
                
                citations.add(new Citation(
                    i + 1,
                    doc.id(),
                    doc.content().substring(0, Math.min(150, doc.content().length())),
                    doc.getTitle(),
                    doc.getAuthor(),
                    doc.getUrl(),
                    doc.score()
                ));
            }
        }
        
        return citations;
    }
    
    /**
     * Build execution metrics
     */
    private RagMetrics buildMetrics(
            Map<String, Long> timings,
            List<RetrievedDocument> documents,
            String answer,
            dev.langchain4j.model.output.TokenUsage tokenUsage) {
        
        long totalTime = timings.values().stream()
            .mapToLong(Long::longValue).sum();
        
        int tokensGenerated = tokenUsage != null ? 
            tokenUsage.outputTokenCount() : estimateTokens(answer);
        
        int tokensInContext = tokenUsage != null ?
            tokenUsage.inputTokenCount() : 0;
        
        return new RagMetrics(
            totalTime,
            timings.getOrDefault("modelInit", 0L),
            timings.getOrDefault("augmentation", 0L),
            0L, // reranking not tracked separately
            timings.getOrDefault("generation", 0L),
            documents.size(),
            documents.size(),
            tokensInContext,
            tokensGenerated,
            calculateAverageScore(documents),
            Map.of(
                "storeInit", timings.getOrDefault("storeInit", 0L),
                "retrieverInit", timings.getOrDefault("retrieverInit", 0L),
                "augmentorInit", timings.getOrDefault("augmentorInit", 0L)
            )
        );
    }
    
    /**
     * Check if metadata matches filters
     */
    private boolean matchesFilters(
            Map<String, Object> metadata,
            Map<String, Object> filters) {
        
        for (Map.Entry<String, Object> filter : filters.entrySet()) {
            Object value = metadata.get(filter.getKey());
            if (value == null || !value.equals(filter.getValue())) {
                return false;
            }
        }
        return true;
    }
    
    private String getEmbeddingModelName(RagRequest request) {
        return (String) request.metadata()
            .getOrDefault("embeddingModel", "text-embedding-3-small");
    }
    
    private float calculateAverageScore(List<RetrievedDocument> documents) {
        if (documents.isEmpty()) return 0f;
        return (float) documents.stream()
            .mapToDouble(RetrievedDocument::score)
            .average()
            .orElse(0.0);
    }
    
    private int estimateTokens(String text) {
        return text.length() / 4;
    }
    
    private RagRequest parseRagRequest(NodeExecutionTask task) {
        // Same as original implementation
        Map<String, Object> context = task.context();
        String query = (String) context.get("query");
        
        if (query == null || query.isBlank()) {
            throw new IllegalArgumentException("Query is required");
        }
        
        RagMode mode = context.containsKey("ragMode") ?
            RagMode.valueOf((String) context.get("ragMode")) :
            RagMode.STANDARD;
        
        SearchStrategy strategy = context.containsKey("searchStrategy") ?
            SearchStrategy.valueOf((String) context.get("searchStrategy")) :
            SearchStrategy.HYBRID;
        
        @SuppressWarnings("unchecked")
        Map<String, Object> retrievalCfg = 
            (Map<String, Object>) context.getOrDefault("retrievalConfig", Map.of());
        RetrievalConfig retrievalConfig = parseRetrievalConfig(retrievalCfg);
        
        @SuppressWarnings("unchecked")
        Map<String, Object> generationCfg = 
            (Map<String, Object>) context.getOrDefault("generationConfig", Map.of());
        GenerationConfig generationConfig = parseGenerationConfig(generationCfg);
        
        String tenantId = (String) context.getOrDefault("tenantId", "default");
        String userId = (String) context.get("userId");
        String sessionId = (String) context.getOrDefault("sessionId", 
            UUID.randomUUID().toString());
        
        @SuppressWarnings("unchecked")
        List<String> collections = (List<String>) context.get("collections");
        
        @SuppressWarnings("unchecked")
        Map<String, Object> filters = 
            (Map<String, Object>) context.getOrDefault("filters", Map.of());
        
        @SuppressWarnings("unchecked")
        Map<String, Object> metadata = 
            (Map<String, Object>) context.getOrDefault("metadata", Map.of());
        
        return new RagRequest(
            query, mode, strategy, retrievalConfig, generationConfig,
            metadata, tenantId, userId, sessionId, collections, filters
        );
    }
    
    private RetrievalConfig parseRetrievalConfig(Map<String, Object> config) {
        if (config.isEmpty()) {
            return RetrievalConfig.defaults();
        }
        
        return new RetrievalConfig(
            (int) config.getOrDefault("topK", 5),
            ((Number) config.getOrDefault("minSimilarity", 0.7)).floatValue(),
            (int) config.getOrDefault("maxChunkSize", 512),
            (int) config.getOrDefault("chunkOverlap", 50),
            (boolean) config.getOrDefault("enableReranking", true),
            config.containsKey("rerankingModel") ?
                RerankingModel.valueOf((String) config.get("rerankingModel")) :
                RerankingModel.CROSS_ENCODER_MS_MARCO,
            (boolean) config.getOrDefault("enableHybridSearch", true),
            ((Number) config.getOrDefault("hybridAlpha", 0.5)).floatValue(),
            (boolean) config.getOrDefault("enableMultiQuery", false),
            (int) config.getOrDefault("numQueryVariations", 3),
            (boolean) config.getOrDefault("enableParentRetrieval", false),
            (int) config.getOrDefault("parentChunkSize", 2048),
            new HashMap<>(),
            List.of(),
            (boolean) config.getOrDefault("enableCitation", true),
            (boolean) config.getOrDefault("enableDeduplication", true)
        );
    }
    
    private GenerationConfig parseGenerationConfig(Map<String, Object> config) {
        if (config.isEmpty()) {
            return GenerationConfig.defaults();
        }
        
        return new GenerationConfig(
            (String) config.getOrDefault("provider", "openai"),
            (String) config.getOrDefault("model", "gpt-4-turbo-preview"),
            ((Number) config.getOrDefault("temperature", 0.7)).floatValue(),
            (int) config.getOrDefault("maxTokens", 1024),
            ((Number) config.getOrDefault("topP", 0.95)).floatValue(),
            ((Number) config.getOrDefault("frequencyPenalty", 0.0)).floatValue(),
            ((Number) config.getOrDefault("presencePenalty", 0.0)).floatValue(),
            List.of(),
            (String) config.get("systemPrompt"),
            (String) config.get("userPromptTemplate"),
            (boolean) config.getOrDefault("enableStreaming", false),
            (boolean) config.getOrDefault("enableCitations", true),
            config.containsKey("citationStyle") ?
                CitationStyle.valueOf((String) config.get("citationStyle")) :
                CitationStyle.INLINE_NUMBERED,
            (boolean) config.getOrDefault("enableFactChecking", false),
            (boolean) config.getOrDefault("enableGuardrails", true),
            new HashMap<>()
        );
    }
    
    private NodeExecutionResult successResult(
            NodeExecutionTask task,
            RagResponse response) {
        
        Map<String, Object> output = new HashMap<>();
        output.put("answer", response.answer());
        output.put("sourceDocuments", response.sourceDocuments());
        output.put("citations", response.citations());
        output.put("metrics", response.metrics());
        output.put("sessionId", response.sessionId());
        output.put("timestamp", response.timestamp().toString());
        
        return new NodeExecutionResult(
            task.runId(),
            task.nodeId(),
            task.attempt(),
            NodeExecutionStatus.COMPLETED,
            output,
            null,
            task.token()
        );
    }
    
    private NodeExecutionResult failureResult(
            NodeExecutionTask task,
            Throwable error) {
        
        return new NodeExecutionResult(
            task.runId(),
            task.nodeId(),
            task.attempt(),
            NodeExecutionStatus.FAILED,
            Map.of(),
            new ErrorInfo(
                "LANGCHAIN4J_RAG_FAILED",
                error.getMessage(),
                getStackTrace(error),
                Map.of()
            ),
            task.token()
        );
    }
    
    private String getStackTrace(Throwable e) {
        java.io.StringWriter sw = new java.io.StringWriter();
        e.printStackTrace(new java.io.PrintWriter(sw));
        return sw.toString();
    }
}

// ==================== MODEL FACTORY ====================

/**
 * Factory for creating LangChain4j models
 */
@ApplicationScoped
public class LangChain4jModelFactory {
    
    private static final Logger LOG = LoggerFactory.getLogger(LangChain4jModelFactory.class);
    
    @Inject
    LangChain4jConfig config;
    
    /**
     * Create chat language model based on configuration
     */
    public ChatLanguageModel createChatModel(GenerationConfig genConfig) {
        
        LOG.debug("Creating chat model: provider={}, model={}", 
            genConfig.provider(), genConfig.model());
        
        return switch (genConfig.provider().toLowerCase()) {
            case "openai" -> createOpenAiChatModel(genConfig);
            case "anthropic", "claude" -> createAnthropicChatModel(genConfig);
            case "azure", "azure-openai" -> createAzureOpenAiChatModel(genConfig);
            default -> throw new IllegalArgumentException(
                "Unsupported provider: " + genConfig.provider());
        };
    }
    
    /**
     * Create embedding model
     */
    public EmbeddingModel createEmbeddingModel(String tenantId, String modelName) {
        
        LOG.debug("Creating embedding model: {} for tenant: {}", modelName, tenantId);
        
        if (modelName.startsWith("text-embedding")) {
            return OpenAiEmbeddingModel.builder()
                .apiKey(config.getOpenAiApiKey())
                .modelName(modelName)
                .timeout(Duration.ofSeconds(30))
                .logRequests(config.isLogRequests())
                .logResponses(config.isLogResponses())
                .build();
        } else if (modelName.startsWith("azure-")) {
            return AzureOpenAiEmbeddingModel.builder()
                .apiKey(config.getAzureApiKey())
                .endpoint(config.getAzureEndpoint())
                .deploymentName(config.getAzureEmbeddingDeployment())
                .timeout(Duration.ofSeconds(30))
                .logRequestsAndResponses(config.isLogRequests())
                .build();
        } else {
            // Default to OpenAI
            return OpenAiEmbeddingModel.builder()
                .apiKey(config.getOpenAiApiKey())
                .modelName("text-embedding-3-small")
                .build();
        }
    }
    
    private ChatLanguageModel createOpenAiChatModel(GenerationConfig genConfig) {
        return OpenAiChatModel.builder()
            .apiKey(config.getOpenAiApiKey())
            .modelName(genConfig.model())
            .temperature(genConfig.temperature())
            .maxTokens(genConfig.maxTokens())
            .topP(genConfig.topP())
            .frequencyPenalty(genConfig.frequencyPenalty())
            .presencePenalty(genConfig.presencePenalty())
            .timeout(Duration.ofSeconds(60))
            .logRequests(config.isLogRequests())
            .logResponses(config.isLogResponses())
            .build();
    }
    
    private ChatLanguageModel createAnthropicChatModel(GenerationConfig genConfig) {
        return AnthropicChatModel.builder()
            .apiKey(config.getAnthropicApiKey())
            .modelName(genConfig.model())
            .temperature(genConfig.temperature())
            .maxTokens(genConfig.maxTokens())
            .topP(genConfig.topP())
            .timeout(Duration.ofSeconds(60))
            .logRequests(config.isLogRequests())
            .logResponses(config.isLogResponses())
            .build();
    }
    
    private ChatLanguageModel createAzureOpenAiChatModel(GenerationConfig genConfig) {
        return AzureOpenAiChatModel.builder()
            .apiKey(config.getAzureApiKey())
            .endpoint(config.getAzureEndpoint())
            .deploymentName(config.getAzureChatDeployment())
            .temperature(genConfig.temperature())
            .maxTokens(genConfig.maxTokens())
            .topP(genConfig.topP())
            .timeout(Duration.ofSeconds(60))
            .logRequestsAndResponses(config.isLogRequests())
            .build();
    }
}

// ==================== EMBEDDING STORE FACTORY ====================

/**
 * Factory for creating LangChain4j embedding stores
 */
@ApplicationScoped
public class LangChain4jEmbeddingStoreFactory {
    
    private static final Logger LOG = LoggerFactory.getLogger(
        LangChain4jEmbeddingStoreFactory.class);
    
    @Inject
    LangChain4jConfig config;
    
    // Cache stores per tenant
    private final Map<String, EmbeddingStore<TextSegment>> storeCache = 
        new java.util.concurrent.ConcurrentHashMap<>();
    
    /**
     * Get or create embedding store for tenant
     */
    public EmbeddingStore<TextSegment> getStore(
            String tenantId,
            RetrievalConfig retrievalConfig) {
        
        return storeCache.computeIfAbsent(tenantId, tid -> createStore(tid, retrievalConfig));
    }
    
    private EmbeddingStore<TextSegment> createStore(
            String tenantId,
            RetrievalConfig retrievalConfig) {
        
        String backend = config.getVectorStoreBackend();
        
        LOG.info("Creating embedding store for tenant {}: backend={}", tenantId, backend);
        
        return switch (backend.toLowerCase()) {
            case "postgres", "pgvector" -> createPgVectorStore(tenantId);
            case "pinecone" -> createPineconeStore(tenantId);
            case "weaviate" -> createWeaviateStore(tenantId);
            default -> throw new IllegalArgumentException(
                "Unsupported vector store: " + backend);
        };
    }
    
    private EmbeddingStore<TextSegment> createPgVectorStore(String tenantId) {
        return PgVectorEmbeddingStore.builder()
            .host(config.getPgHost())
            .port(config.getPgPort())
            .database(config.getPgDatabase())
            .user(config.getPgUser())
            .password(config.getPgPassword())
            .table(config.getPgTable())
            .dimension(config.getEmbeddingDimension())
            .createTable(true)
            .dropTableFirst(false)
            .build();
    }
    
    private EmbeddingStore<TextSegment> createPineconeStore(String tenantId) {
        return PineconeEmbeddingStore.builder()
            .apiKey(config.getPineconeApiKey())
            .environment(config.getPineconeEnvironment())
            .projectId(config.getPineconeProjectId())
            .index(config.getPineconeIndexName())
            .nameSpace(tenantId) // Use tenant ID as namespace for isolation
            .build();
    }
    
    private EmbeddingStore<Text




 //------------
 
 
 package tech.kayys.silat.executor.rag.client;

import io.smallrye.mutiny.Uni;
import jakarta.ws.rs.*;
import jakarta.ws.rs.core.MediaType;
import org.eclipse.microprofile.rest.client.inject.RegisterRestClient;

import java.util.List;
import java.util.Map;

/**
 * ============================================================================
 * REST CLIENTS FOR EXTERNAL SERVICES
 * ============================================================================
 */

// ==================== OPENAI CLIENT ====================

@RegisterRestClient(configKey = "openai")
@Path("/v1")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public interface OpenAIClient {
    
    @POST
    @Path("/embeddings")
    Uni<OpenAIEmbeddingResponse> createEmbeddings(OpenAIEmbeddingRequest request);
    
    @POST
    @Path("/chat/completions")
    Uni<OpenAIChatResponse> createChatCompletion(OpenAIChatRequest request);
}

record OpenAIEmbeddingRequest(
    String model,
    List<String> input,
    String encoding_format
) {}

record OpenAIEmbeddingResponse(
    List<OpenAIEmbeddingData> data,
    String model,
    OpenAIUsage usage
) {}

record OpenAIEmbeddingData(
    float[] embedding,
    int index
) {}

record OpenAIChatRequest(
    String model,
    List<OpenAIChatMessage> messages,
    float temperature,
    int max_tokens,
    double top_p,
    double frequency_penalty,
    double presence_penalty,
    List<String> stop
) {}

record OpenAIChatMessage(
    String role,
    String content
) {}

record OpenAIChatResponse(
    String id,
    List<OpenAIChatChoice> choices,
    OpenAIUsage usage
) {}

record OpenAIChatChoice(
    int index,
    OpenAIChatMessage message,
    String finish_reason
) {}

record OpenAIUsage(
    int prompt_tokens,
    int completion_tokens,
    int total_tokens
) {}

// ==================== ANTHROPIC CLIENT ====================

@RegisterRestClient(configKey = "anthropic")
@Path("/v1")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public interface AnthropicClient {
    
    @POST
    @Path("/messages")
    Uni<AnthropicResponse> createMessage(AnthropicRequest request);
}

record AnthropicRequest(
    String model,
    int max_tokens,
    String system,
    List<AnthropicMessage> messages,
    float temperature,
    double top_p,
    boolean stream
) {}

record AnthropicMessage(
    String role,
    String content
) {}

record AnthropicResponse(
    String id,
    String type,
    String role,
    List<AnthropicContent> content,
    String model,
    AnthropicUsage usage
) {}

record AnthropicContent(
    String type,
    String text
) {}

record AnthropicUsage(
    int input_tokens,
    int output_tokens
) {}

// ==================== COHERE CLIENT ====================

@RegisterRestClient(configKey = "cohere")
@Path("/v1")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public interface CohereClient {
    
    @POST
    @Path("/embed")
    Uni<CohereEmbedResponse> embed(CohereEmbedRequest request);
    
    @POST
    @Path("/rerank")
    Uni<CohereRerankResponse> rerank(CohereRerankRequest request);
}

record CohereEmbedRequest(
    List<String> texts,
    String model,
    String input_type,
    String embedding_types
) {}

record CohereEmbedResponse(
    List<float[]> embeddings,
    String id,
    Map<String, Object> meta
) {}

record CohereRerankRequest(
    String model,
    String query,
    List<String> documents,
    int top_n,
    boolean return_documents
) {}

record CohereRerankResponse(
    String id,
    List<CohereRerankResult> results,
    Map<String, Object> meta
) {}

record CohereRerankResult(
    int index,
    float relevance_score,
    String document
) {
    public float relevanceScore() {
        return relevance_score;
    }
}

// ==================== VOYAGE AI CLIENT ====================

@RegisterRestClient(configKey = "voyage")
@Path("/v1")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public interface VoyageClient {
    
    @POST
    @Path("/embeddings")
    Uni<VoyageEmbedResponse> embed(VoyageEmbedRequest request);
    
    @POST
    @Path("/rerank")
    Uni<VoyageRerankResponse> rerank(VoyageRerankRequest request);
}

record VoyageEmbedRequest(
    List<String> input,
    String model,
    String input_type
) {}

record VoyageEmbedResponse(
    List<float[]> embeddings,
    String model,
    VoyageUsage usage
) {}

record VoyageRerankRequest(
    String query,
    List<String> documents,
    String model,
    int top_k
) {}

record VoyageRerankResponse(
    List<VoyageRerankResult> results,
    String model
) {}

record VoyageRerankResult(
    int index,
    float relevance_score,
    String document
) {
    public float relevanceScore() {
        return relevance_score;
    }
}

record VoyageUsage(
    int total_tokens
) {}

// ==================== HUGGING FACE CLIENT ====================

@RegisterRestClient(configKey = "huggingface")
@Path("/models")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public interface HuggingFaceClient {
    
    @POST
    @Path("/{modelId}")
    Uni<HuggingFaceResponse> inference(
        @PathParam("modelId") String modelId,
        HuggingFaceRequest request
    );
    
    default Uni<HuggingFaceResponse> inference(HuggingFaceRequest request) {
        return inference(request.model(), request);
    }
}

record HuggingFaceRequest(
    String model,
    List<List<String>> inputs
) {}

record HuggingFaceResponse(
    List<Float> scores
) {}

// ==================== AZURE OPENAI CLIENT ====================

@RegisterRestClient(configKey = "azure-openai")
@Path("/openai/deployments")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public interface AzureOpenAIClient {
    
    @POST
    @Path("/{deploymentId}/chat/completions")
    Uni<OpenAIChatResponse> createChatCompletion(
        @PathParam("deploymentId") String deploymentId,
        @QueryParam("api-version") String apiVersion,
        OpenAIChatRequest request
    );
}

// ==================== PINECONE CLIENT ====================

@RegisterRestClient(configKey = "pinecone")
@Path("/")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public interface PineconeClient {
    
    @POST
    @Path("/vectors/query")
    Uni<PineconeQueryResponse> query(
        @PathParam("indexName") String indexName,
        PineconeQueryRequest request
    );
    
    @POST
    @Path("/vectors/upsert")
    Uni<PineconeUpsertResponse> upsert(
        @PathParam("indexName") String indexName,
        PineconeUpsertRequest request
    );
    
    @POST
    @Path("/vectors/delete")
    Uni<Void> delete(
        @PathParam("indexName") String indexName,
        PineconeDeleteRequest request
    );
    
    @GET
    @Path("/describe_index_stats")
    Uni<PineconeIndexStats> describeIndexStats(
        @PathParam("indexName") String indexName
    );
}

record PineconeQueryRequest(
    float[] vector,
    int topK,
    Map<String, Object> filter,
    boolean includeMetadata
) {}

record PineconeQueryResponse(
    List<PineconeMatch> matches,
    String namespace
) {}

record PineconeMatch(
    String id,
    float score,
    Map<String, Object> metadata
) {}

record PineconeVector(
    String id,
    float[] values,
    Map<String, Object> metadata
) {}

record PineconeUpsertRequest(
    List<PineconeVector> vectors
) {}

record PineconeUpsertResponse(
    int upsertedCount
) {}

record PineconeDeleteRequest(
    List<String> ids,
    Map<String, Object> filter
) {}

record PineconeIndexStats(
    long totalVectorCount,
    int dimension
) {}

// ==================== WEAVIATE CLIENT ====================

@RegisterRestClient(configKey = "weaviate")
@Path("/v1")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public interface WeaviateClient {
    
    @POST
    @Path("/graphql")
    Uni<WeaviateGraphQLResponse> graphql(WeaviateGraphQLRequest request);
    
    @POST
    @Path("/objects")
    Uni<WeaviateObject> createObject(WeaviateObject object);
    
    @DELETE
    @Path("/objects/{className}/{id}")
    Uni<Void> deleteObject(
        @PathParam("className") String className,
        @PathParam("id") String id
    );
}

record WeaviateGraphQLRequest(
    String query
) {}

record WeaviateGraphQLResponse(
    Map<String, Object> data,
    List<Map<String, Object>> errors
) {}

record WeaviateObject(
    String className,
    Map<String, Object> properties,
    float[] vector
) {}

// ==================== CONFIGURATION CLASSES ====================

package tech.kayys.silat.executor.rag.config;

import jakarta.enterprise.context.ApplicationScoped;
import org.eclipse.microprofile.config.inject.ConfigProperty;

@ApplicationScoped
public class OpenAIConfig {
    
    @ConfigProperty(name = "openai.api.key")
    String apiKey;
    
    @ConfigProperty(name = "openai.api.url", defaultValue = "https://api.openai.com/v1")
    String apiUrl;
    
    @ConfigProperty(name = "openai.organization", defaultValue = "")
    String organization;
    
    public String getApiKey() {
        return apiKey;
    }
    
    public String getApiUrl() {
        return apiUrl;
    }
}

@ApplicationScoped
public class AzureOpenAIConfig {
    
    @ConfigProperty(name = "azure.openai.endpoint")
    String endpoint;
    
    @ConfigProperty(name = "azure.openai.api.key")
    String apiKey;
    
    @ConfigProperty(name = "azure.openai.deployment.id")
    String deploymentId;
    
    @ConfigProperty(name = "azure.openai.api.version", defaultValue = "2024-02-15-preview")
    String apiVersion;
    
    public String deploymentId() {
        return deploymentId;
    }
    
    public String apiVersion() {
        return apiVersion;
    }
}

@ApplicationScoped
public class PineconeConfig {
    
    @ConfigProperty(name = "pinecone.api.key")
    String apiKey;
    
    @ConfigProperty(name = "pinecone.environment")
    String environment;
    
    @ConfigProperty(name = "pinecone.index.name")
    String indexName;
    
    public String indexName() {
        return indexName;
    }
}

@ApplicationScoped
public class WeaviateConfig {
    
    @ConfigProperty(name = "weaviate.url")
    String url;
    
    @ConfigProperty(name = "weaviate.api.key", defaultValue = "")
    String apiKey;
    
    @ConfigProperty(name = "weaviate.class.name", defaultValue = "Document")
    String className;
    
    public String className() {
        return className;
    }
}

// ==================== CACHING SERVICES ====================

package tech.kayys.silat.executor.rag.cache;

import io.quarkus.cache.CacheResult;
import io.quarkus.cache.CacheKey;
import io.quarkus.redis.datasource.ReactiveRedisDataSource;
import io.quarkus.redis.datasource.hash.ReactiveHashCommands;
import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tech.kayys.silat.executor.rag.domain.*;

import java.time.Duration;
import java.util.*;

/**
 * Embedding cache service using Redis
 */
@ApplicationScoped
public class EmbeddingCacheService {
    
    private static final Logger LOG = LoggerFactory.getLogger(EmbeddingCacheService.class);
    private static final Duration TTL = Duration.ofHours(24);
    
    @Inject
    ReactiveRedisDataSource redis;
    
    /**
     * Get cached embeddings for batch of texts
     */
    public Uni<Map<String, float[]>> getBatch(
            List<String> texts,
            String model,
            String tenantId) {
        
        ReactiveHashCommands<String, String, byte[]> hash = redis.hash(byte[].class);
        String cacheKey = buildCacheKey(model, tenantId);
        
        List<Uni<Map.Entry<String, float[]>>> unis = texts.stream()
            .map(text -> {
                String fieldKey = hashText(text);
                return hash.hget(cacheKey, fieldKey)
                    .map(bytes -> bytes != null ? 
                        Map.entry(text, bytesToFloatArray(bytes)) : null);
            })
            .toList();
        
        return Uni.combine().all().unis(unis).combinedWith(results -> {
            Map<String, float[]> cached = new HashMap<>();
            for (Object result : results) {
                if (result != null) {
                    @SuppressWarnings("unchecked")
                    Map.Entry<String, float[]> entry = (Map.Entry<String, float[]>) result;
                    cached.put(entry.getKey(), entry.getValue());
                }
            }
            return cached;
        });
    }
    
    /**
     * Store embeddings in cache
     */
    public Uni<Void> putBatch(
            Map<String, float[]> embeddings,
            String model,
            String tenantId) {
        
        if (embeddings.isEmpty()) {
            return Uni.createFrom().voidItem();
        }
        
        ReactiveHashCommands<String, String, byte[]> hash = redis.hash(byte[].class);
        String cacheKey = buildCacheKey(model, tenantId);
        
        Map<String, byte[]> toCache = new HashMap<>();
        embeddings.forEach((text, embedding) -> 
            toCache.put(hashText(text), floatArrayToBytes(embedding))
        );
        
        return hash.hmset(cacheKey, toCache)
            .flatMap(v -> redis.key().expire(cacheKey, TTL))
            .replaceWithVoid();
    }
    
    private String buildCacheKey(String model, String tenantId) {
        return String.format("embeddings:%s:%s", tenantId, model);
    }
    
    private String hashText(String text) {
        // Simple hash for cache key
        return Integer.toHexString(text.hashCode());
    }
    
    private byte[] floatArrayToBytes(float[] array) {
        byte[] bytes = new byte[array.length * 4];
        java.nio.ByteBuffer buffer = java.nio.ByteBuffer.wrap(bytes);
        for (float f : array) {
            buffer.putFloat(f);
        }
        return bytes;
    }
    
    private float[] bytesToFloatArray(byte[] bytes) {
        float[] array = new float[bytes.length / 4];
        java.nio.ByteBuffer buffer = java.nio.ByteBuffer.wrap(bytes);
        for (int i = 0; i < array.length; i++) {
            array[i] = buffer.getFloat();
        }
        return array;
    }
}

/**
 * RAG response cache service
 */
@ApplicationScoped
public class RagCacheService {
    
    private static final Logger LOG = LoggerFactory.getLogger(RagCacheService.class);
    private static final Duration TTL = Duration.ofHours(1);
    
    @Inject
    ReactiveRedisDataSource redis;
    
    @Inject
    com.fasterxml.jackson.databind.ObjectMapper objectMapper;
    
    /**
     * Get cached RAG response
     */
    @CacheResult(cacheName = "rag-responses")
    public Uni<RagResponse> get(@CacheKey RagRequest request) {
        String cacheKey = buildCacheKey(request);
        
        return redis.value(String.class).get(cacheKey)
            .map(json -> {
                if (json == null) return null;
                
                try {
                    return objectMapper.readValue(json, RagResponse.class);
                } catch (Exception e) {
                    LOG.error("Failed to deserialize cached response", e);
                    return null;
                }
            });
    }
    
    /**
     * Store RAG response in cache
     */
    public Uni<Void> put(RagRequest request, RagResponse response) {
        String cacheKey = buildCacheKey(request);
        
        try {
            String json = objectMapper.writeValueAsString(response);
            
            return redis.value(String.class).set(cacheKey, json)
                .flatMap(v -> redis.key().expire(cacheKey, TTL))
                .replaceWithVoid();
        } catch (Exception e) {
            LOG.error("Failed to serialize response for caching", e);
            return Uni.createFrom().voidItem();
        }
    }
    
    private String buildCacheKey(RagRequest request) {
        // Build cache key from request parameters
        return String.format("rag:%s:%s:%d:%s",
            request.tenantId(),
            hashQuery(request.query()),
            request.retrievalConfig().topK(),
            request.generationConfig().model()
        );
    }
    
    private String hashQuery(String query) {
        return Integer.toHexString(query.toLowerCase().trim().hashCode());
    }
}

// ==================== APPLICATION.PROPERTIES ====================

/**
 * Required configuration in application.properties:
 *
 * # OpenAI Configuration
 * openai.api.key=${OPENAI_API_KEY}
 * openai.api.url=https://api.openai.com/v1
 * quarkus.rest-client.openai.url=${openai.api.url}
 * quarkus.rest-client.openai.scope=jakarta.inject.Singleton
 *
 * # Anthropic Configuration
 * anthropic.api.key=${ANTHROPIC_API_KEY}
 * quarkus.rest-client.anthropic.url=https://api.anthropic.com
 * quarkus.rest-client.anthropic.scope=jakarta.inject.Singleton
 *
 * # Cohere Configuration
 * cohere.api.key=${COHERE_API_KEY}
 * quarkus.rest-client.cohere.url=https://api.cohere.ai
 * quarkus.rest-client.cohere.scope=jakarta.inject.Singleton
 *
 * # Voyage AI Configuration
 * voyage.api.key=${VOYAGE_API_KEY}
 * quarkus.rest-client.voyage.url=https://api.voyageai.com
 * quarkus.rest-client.voyage.scope=jakarta.inject.Singleton
 *
 * # Hugging Face Configuration
 * huggingface.api.key=${HUGGINGFACE_API_KEY}
 * quarkus.rest-client.huggingface.url=https://api-inference.huggingface.co
 * quarkus.rest-client.huggingface.scope=jakarta.inject.Singleton
 *
 * # Azure OpenAI Configuration
 * azure.openai.endpoint=${AZURE_OPENAI_ENDPOINT}
 * azure.openai.api.key=${AZURE_OPENAI_API_KEY}
 * azure.openai.deployment.id=${AZURE_OPENAI_DEPLOYMENT_ID}
 * azure.openai.api.version=2024-02-15-preview
 * quarkus.rest-client.azure-openai.url=${azure.openai.endpoint}
 * quarkus.rest-client.azure-openai.scope=jakarta.inject.Singleton
 *
 * # Pinecone Configuration
 * pinecone.api.key=${PINECONE_API_KEY}
 * pinecone.environment=${PINECONE_ENVIRONMENT}
 * pinecone.index.name=silat-vectors
 * quarkus.rest-client.pinecone.url=https://${pinecone.index.name}-${pinecone.environment}.svc.pinecone.io
 * quarkus.rest-client.pinecone.scope=jakarta.inject.Singleton
 *
 * # Weaviate Configuration
 * weaviate.url=${WEAVIATE_URL:http://localhost:8080}
 * weaviate.api.key=${WEAVIATE_API_KEY:}
 * weaviate.class.name=Document
 * quarkus.rest-client.weaviate.url=${weaviate.url}
 * quarkus.rest-client.weaviate.scope=jakarta.inject.Singleton
 *
 * # PostgreSQL Configuration (for pgvector)
 * quarkus.datasource.db-kind=postgresql
 * quarkus.datasource.username=${DB_USER:postgres}
 * quarkus.datasource.password=${DB_PASSWORD:postgres}
 * quarkus.datasource.reactive.url=postgresql://${DB_HOST:localhost}:${DB_PORT:5432}/${DB_NAME:silat}
 *
 * # Redis Configuration (for caching)
 * quarkus.redis.hosts=redis://${REDIS_HOST:localhost}:${REDIS_PORT:6379}
 * quarkus.redis.password=${REDIS_PASSWORD:}
 * quarkus.redis.client-type=standalone
 *
 * # Vector Store Configuration
 * silat.vectorstore.default.backend=postgres
 *
 * # Cache Configuration
 * quarkus.cache.caffeine.rag-responses.maximum-size=1000
 * quarkus.cache.caffeine.rag-responses.expire-after-write=1H
 */

package tech.kayys.silat.executor.rag;

import jakarta.enterprise.context.ApplicationScoped;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tech.kayys.silat.executor.rag.domain.*;

import java.util.*;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

/**
 * ============================================================================
 * CONTEXT ASSEMBLER
 * ============================================================================
 * 
 * Assembles retrieved documents into structured context for LLM.
 */
@ApplicationScoped
public class ContextAssembler {
    
    private static final Logger LOG = LoggerFactory.getLogger(ContextAssembler.class);
    private static final int CHARS_PER_TOKEN = 4;
    
    /**
     * Assemble context from retrieved documents
     */
    public AssembledContext assemble(
            RagRequest request,
            List<RetrievedDocument> documents) {
        
        LOG.debug("Assembling context from {} documents", documents.size());
        
        // Deduplicate if enabled
        List<RetrievedDocument> deduped = request.retrievalConfig().enableDeduplication() ?
            deduplicate(documents) : documents;
        
        // Build formatted context
        StringBuilder contextBuilder = new StringBuilder();
        int totalTokens = 0;
        int maxTokens = request.generationConfig().maxTokens() * 2; // Reserve tokens for output
        
        for (int i = 0; i < deduped.size(); i++) {
            RetrievedDocument doc = deduped.get(i);
            
            // Format document
            String formatted = formatDocument(doc, i + 1, request.generationConfig().citationStyle());
            int docTokens = estimateTokens(formatted);
            
            // Check token limit
            if (totalTokens + docTokens > maxTokens) {
                LOG.warn("Reached token limit, truncating context at {} documents", i);
                break;
            }
            
            contextBuilder.append(formatted).append("\n\n");
            totalTokens += docTokens;
        }
        
        String formattedContext = contextBuilder.toString().trim();
        
        return new AssembledContext(
            formattedContext,
            deduped,
            totalTokens,
            Map.of(
                "documentsIncluded", deduped.size(),
                "documentsTotal", documents.size()
            )
        );
    }
    
    /**
     * Format single document for context
     */
    private String formatDocument(
            RetrievedDocument doc,
            int index,
            CitationStyle citationStyle) {
        
        StringBuilder sb = new StringBuilder();
        
        // Add citation marker
        sb.append("Document ").append(index);
        
        // Add metadata
        if (doc.metadata().containsKey("title")) {
            sb.append(" - ").append(doc.metadata().get("title"));
        }
        if (doc.metadata().containsKey("author")) {
            sb.append(" (").append(doc.metadata().get("author")).append(")");
        }
        
        sb.append(":\n");
        
        // Add content
        sb.append(doc.content());
        
        // Add source info
        if (doc.getUrl() != null) {
            sb.append("\nSource: ").append(doc.getUrl());
        }
        
        return sb.toString();
    }
    
    /**
     * Deduplicate similar documents
     */
    private List<RetrievedDocument> deduplicate(List<RetrievedDocument> documents) {
        List<RetrievedDocument> unique = new ArrayList<>();
        Set<String> seenContent = new HashSet<>();
        
        for (RetrievedDocument doc : documents) {
            // Use first 100 chars as fingerprint
            String fingerprint = doc.content().substring(
                0, Math.min(100, doc.content().length())
            );
            
            if (!seenContent.contains(fingerprint)) {
                unique.add(doc);
                seenContent.add(fingerprint);
            } else {
                LOG.debug("Deduplicating document: {}", doc.id());
            }
        }
        
        return unique;
    }
    
    private int estimateTokens(String text) {
        return text.length() / CHARS_PER_TOKEN;
    }
}

/**
 * ============================================================================
 * CITATION EXTRACTOR
 * ============================================================================
 * 
 * Extracts citations from generated answers.
 */
@ApplicationScoped
public class CitationExtractor {
    
    private static final Logger LOG = LoggerFactory.getLogger(CitationExtractor.class);
    
    // Citation patterns
    private static final Pattern NUMBERED_CITATION = Pattern.compile("\\[(\\d+)\\]");
    private static final Pattern AUTHOR_YEAR_CITATION = Pattern.compile("\\(([^)]+)\\)");
    
    /**
     * Extract citations from answer
     */
    public List<Citation> extract(
            String answer,
            List<RetrievedDocument> documents) {
        
        LOG.debug("Extracting citations from answer");
        
        List<Citation> citations = new ArrayList<>();
        
        // Extract numbered citations [1], [2], etc.
        Matcher matcher = NUMBERED_CITATION.matcher(answer);
        Set<Integer> citedIndices = new HashSet<>();
        
        while (matcher.find()) {
            int index = Integer.parseInt(matcher.group(1));
            citedIndices.add(index);
        }
        
        // Build citation objects
        for (int index : citedIndices) {
            if (index > 0 && index <= documents.size()) {
                RetrievedDocument doc = documents.get(index - 1);
                
                citations.add(new Citation(
                    index,
                    doc.id(),
                    extractSnippet(doc.content(), 150),
                    doc.getTitle(),
                    doc.getAuthor(),
                    doc.getUrl(),
                    doc.getFinalScore()
                ));
            }
        }
        
        return citations;
    }
    
    /**
     * Extract snippet from content
     */
    private String extractSnippet(String content, int maxLength) {
        if (content.length() <= maxLength) {
            return content;
        }
        
        String snippet = content.substring(0, maxLength);
        int lastSpace = snippet.lastIndexOf(' ');
        if (lastSpace > 0) {
            snippet = snippet.substring(0, lastSpace);
        }
        
        return snippet + "...";
    }
}

// ==================== VECTOR STORE IMPLEMENTATIONS ====================

package tech.kayys.silat.executor.rag.vectorstore;

import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tech.kayys.silat.executor.rag.domain.*;

import java.time.Instant;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.stream.Collectors;

/**
 * Vector store registry - manages multiple vector store backends
 */
@ApplicationScoped
public class VectorStoreRegistry {
    
    private static final Logger LOG = LoggerFactory.getLogger(VectorStoreRegistry.class);
    
    @Inject
    PostgresVectorStore postgresStore;
    
    @Inject
    PineconeVectorStore pineconeStore;
    
    @Inject
    WeaviateVectorStore weaviateStore;
    
    @Inject
    VectorStoreConfig config;
    
    // Tenant to store mapping
    private final Map<String, VectorStore> tenantStores = new ConcurrentHashMap<>();
    
    /**
     * Get vector store for tenant
     */
    public VectorStore getStore(String tenantId) {
        return tenantStores.computeIfAbsent(tenantId, this::createStore);
    }
    
    private VectorStore createStore(String tenantId) {
        String backend = config.getBackend(tenantId);
        
        LOG.info("Creating vector store for tenant {}: backend={}", tenantId, backend);
        
        return switch (backend.toLowerCase()) {
            case "postgres", "pgvector" -> postgresStore;
            case "pinecone" -> pineconeStore;
            case "weaviate" -> weaviateStore;
            default -> {
                LOG.warn("Unknown backend {}, using Postgres", backend);
                yield postgresStore;
            }
        };
    }
}

/**
 * PostgreSQL + pgvector implementation
 */
@ApplicationScoped
public class PostgresVectorStore implements VectorStore {
    
    private static final Logger LOG = LoggerFactory.getLogger(PostgresVectorStore.class);
    
    @Inject
    io.vertx.mutiny.pgclient.PgPool pgPool;
    
    @Override
    public Uni<List<RetrievedDocument>> similaritySearch(
            float[] queryEmbedding,
            int topK,
            float minSimilarity,
            List<String> collections,
            Map<String, Object> filters) {
        
        // Build query
        StringBuilder sql = new StringBuilder("""
            SELECT 
                d.id,
                d.content,
                d.metadata,
                d.collection_name,
                d.source_id,
                d.source_type,
                d.indexed_at,
                d.chunk_index,
                1 - (d.embedding <=> $1::vector) as similarity
            FROM documents d
            WHERE 1 = 1
            """);
        
        List<Object> params = new ArrayList<>();
        params.add(arrayToString(queryEmbedding));
        int paramIndex = 2;
        
        // Add collection filter
        if (collections != null && !collections.isEmpty()) {
            sql.append(" AND d.collection_name = ANY($").append(paramIndex++).append(")");
            params.add(collections.toArray(new String[0]));
        }
        
        // Add metadata filters
        for (Map.Entry<String, Object> filter : filters.entrySet()) {
            sql.append(" AND d.metadata->>'").append(filter.getKey())
                .append("' = $").append(paramIndex++);
            params.add(filter.getValue().toString());
        }
        
        // Add similarity threshold
        sql.append(" AND 1 - (d.embedding <=> $1::vector) >= $").append(paramIndex++);
        params.add(minSimilarity);
        
        // Order and limit
        sql.append(" ORDER BY similarity DESC LIMIT $").append(paramIndex);
        params.add(topK);
        
        return pgPool.preparedQuery(sql.toString())
            .execute(io.vertx.mutiny.sqlclient.Tuple.from(params.toArray()))
            .map(rowSet -> {
                List<RetrievedDocument> results = new ArrayList<>();
                
                for (io.vertx.mutiny.sqlclient.Row row : rowSet) {
                    results.add(rowToDocument(row));
                }
                
                return results;
            });
    }
    
    @Override
    public Uni<List<RetrievedDocument>> keywordSearch(
            String queryText,
            int topK,
            List<String> collections,
            Map<String, Object> filters) {
        
        // Full-text search using PostgreSQL
        String sql = """
            SELECT 
                d.id,
                d.content,
                d.metadata,
                d.collection_name,
                d.source_id,
                d.source_type,
                d.indexed_at,
                d.chunk_index,
                ts_rank(d.search_vector, plainto_tsquery('english', $1)) as score
            FROM documents d
            WHERE d.search_vector @@ plainto_tsquery('english', $1)
            """;
        
        if (collections != null && !collections.isEmpty()) {
            sql += " AND d.collection_name = ANY($2)";
        }
        
        sql += " ORDER BY score DESC LIMIT " + topK;
        
        io.vertx.mutiny.sqlclient.Tuple tuple = collections != null && !collections.isEmpty() ?
            io.vertx.mutiny.sqlclient.Tuple.of(queryText, collections.toArray(new String[0])) :
            io.vertx.mutiny.sqlclient.Tuple.of(queryText);
        
        return pgPool.preparedQuery(sql)
            .execute(tuple)
            .map(rowSet -> {
                List<RetrievedDocument> results = new ArrayList<>();
                for (io.vertx.mutiny.sqlclient.Row row : rowSet) {
                    results.add(rowToDocument(row));
                }
                return results;
            });
    }
    
    @Override
    public Uni<Void> index(List<VectorDocument> documents) {
        if (documents.isEmpty()) {
            return Uni.createFrom().voidItem();
        }
        
        LOG.info("Indexing {} documents in PostgreSQL", documents.size());
        
        String sql = """
            INSERT INTO documents (
                id, content, embedding, metadata, tenant_id,
                collection_name, source_id, source_type, 
                chunk_index, indexed_at, search_vector
            ) VALUES ($1, $2, $3::vector, $4::jsonb, $5, $6, $7, $8, $9, $10, to_tsvector('english', $2))
            ON CONFLICT (id) DO UPDATE SET
                content = EXCLUDED.content,
                embedding = EXCLUDED.embedding,
                metadata = EXCLUDED.metadata,
                indexed_at = EXCLUDED.indexed_at,
                search_vector = to_tsvector('english', EXCLUDED.content)
            """;
        
        List<Uni<Void>> unis = documents.stream()
            .map(doc -> {
                io.vertx.mutiny.sqlclient.Tuple params = io.vertx.mutiny.sqlclient.Tuple.of(
                    doc.id(),
                    doc.content(),
                    arrayToString(doc.embedding()),
                    io.vertx.core.json.JsonObject.mapFrom(doc.metadata()),
                    doc.tenantId(),
                    doc.collectionName(),
                    doc.metadata().get("sourceId"),
                    doc.metadata().get("sourceType"),
                    doc.metadata().get("chunkIndex"),
                    doc.indexedAt()
                );
                
                return pgPool.preparedQuery(sql)
                    .execute(params)
                    .replaceWithVoid();
            })
            .collect(Collectors.toList());
        
        return Uni.join().all(unis).andCollectFailures()
            .replaceWithVoid();
    }
    
    @Override
    public Uni<Void> delete(List<String> documentIds, String tenantId) {
        String sql = "DELETE FROM documents WHERE id = ANY($1) AND tenant_id = $2";
        
        return pgPool.preparedQuery(sql)
            .execute(io.vertx.mutiny.sqlclient.Tuple.of(
                documentIds.toArray(new String[0]),
                tenantId
            ))
            .replaceWithVoid();
    }
    
    @Override
    public Uni<Long> count(String tenantId, List<String> collections) {
        String sql = collections != null && !collections.isEmpty() ?
            "SELECT COUNT(*) FROM documents WHERE tenant_id = $1 AND collection_name = ANY($2)" :
            "SELECT COUNT(*) FROM documents WHERE tenant_id = $1";
        
        io.vertx.mutiny.sqlclient.Tuple params = collections != null && !collections.isEmpty() ?
            io.vertx.mutiny.sqlclient.Tuple.of(tenantId, collections.toArray(new String[0])) :
            io.vertx.mutiny.sqlclient.Tuple.of(tenantId);
        
        return pgPool.preparedQuery(sql)
            .execute(params)
            .map(rowSet -> rowSet.iterator().next().getLong(0));
    }
    
    /**
     * Convert database row to document
     */
    private RetrievedDocument rowToDocument(io.vertx.mutiny.sqlclient.Row row) {
        @SuppressWarnings("unchecked")
        Map<String, Object> metadata = row.getJsonObject("metadata") != null ?
            row.getJsonObject("metadata").getMap() : new HashMap<>();
        
        return new RetrievedDocument(
            row.getString("id"),
            row.getString("content"),
            metadata,
            row.getFloat("similarity") != null ? row.getFloat("similarity") : 
                (row.getFloat("score") != null ? row.getFloat("score") : 0f),
            0f, // rerank score
            row.getInteger("chunk_index") != null ? row.getInteger("chunk_index") : 0,
            row.getString("collection_name"),
            row.getString("source_id"),
            row.getString("source_type"),
            row.getLocalDateTime("indexed_at") != null ?
                Instant.parse(row.getLocalDateTime("indexed_at").toString()) : Instant.now(),
            List.of(),
            Map.of()
        );
    }
    
    /**
     * Convert float array to PostgreSQL vector string
     */
    private String arrayToString(float[] array) {
        StringBuilder sb = new StringBuilder("[");
        for (int i = 0; i < array.length; i++) {
            if (i > 0) sb.append(",");
            sb.append(array[i]);
        }
        sb.append("]");
        return sb.toString();
    }
}

/**
 * Pinecone vector store implementation
 */
@ApplicationScoped
public class PineconeVectorStore implements VectorStore {
    
    private static final Logger LOG = LoggerFactory.getLogger(PineconeVectorStore.class);
    
    @Inject
    @org.eclipse.microprofile.rest.client.inject.RestClient
    PineconeClient client;
    
    @Inject
    PineconeConfig config;
    
    @Override
    public Uni<List<RetrievedDocument>> similaritySearch(
            float[] queryEmbedding,
            int topK,
            float minSimilarity,
            List<String> collections,
            Map<String, Object> filters) {
        
        // Build filter
        Map<String, Object> pineconeFilter = new HashMap<>(filters);
        if (collections != null && !collections.isEmpty()) {
            pineconeFilter.put("collection", Map.of("$in", collections));
        }
        
        PineconeQueryRequest request = new PineconeQueryRequest(
            queryEmbedding,
            topK,
            pineconeFilter,
            true // include metadata
        );
        
        return client.query(config.indexName(), request)
            .map(response -> response.matches().stream()
                .filter(match -> match.score() >= minSimilarity)
                .map(this::matchToDocument)
                .collect(Collectors.toList())
            );
    }
    
    @Override
    public Uni<List<RetrievedDocument>> keywordSearch(
            String queryText,
            int topK,
            List<String> collections,
            Map<String, Object> filters) {
        
        // Pinecone doesn't support native keyword search
        // Fall back to metadata filtering
        LOG.warn("Pinecone doesn't support keyword search, using metadata filtering");
        return Uni.createFrom().item(List.of());
    }
    
    @Override
    public Uni<Void> index(List<VectorDocument> documents) {
        
        List<PineconeVector> vectors = documents.stream()
            .map(doc -> new PineconeVector(
                doc.id(),
                doc.embedding(),
                Map.of(
                    "content", doc.content(),
                    "collection", doc.collectionName(),
                    "tenantId", doc.tenantId(),
                    "metadata", doc.metadata()
                )
            ))
            .collect(Collectors.toList());
        
        PineconeUpsertRequest request = new PineconeUpsertRequest(vectors);
        
        return client.upsert(config.indexName(), request)
            .replaceWithVoid();
    }
    
    @Override
    public Uni<Void> delete(List<String> documentIds, String tenantId) {
        PineconeDeleteRequest request = new PineconeDeleteRequest(
            documentIds,
            Map.of("tenantId", tenantId)
        );
        
        return client.delete(config.indexName(), request)
            .replaceWithVoid();
    }
    
    @Override
    public Uni<Long> count(String tenantId, List<String> collections) {
        return client.describeIndexStats(config.indexName())
            .map(stats -> stats.totalVectorCount());
    }
    
    private RetrievedDocument matchToDocument(PineconeMatch match) {
        @SuppressWarnings("unchecked")
        Map<String, Object> metadata = 
            (Map<String, Object>) match.metadata().get("metadata");
        
        return new RetrievedDocument(
            match.id(),
            (String) match.metadata().get("content"),
            metadata != null ? metadata : new HashMap<>(),
            match.score(),
            0f,
            0,
            (String) match.metadata().get("collection"),
            match.id(),
            "pinecone",
            Instant.now(),
            List.of(),
            Map.of()
        );
    }
}

/**
 * Weaviate vector store implementation
 */
@ApplicationScoped
public class WeaviateVectorStore implements VectorStore {
    
    private static final Logger LOG = LoggerFactory.getLogger(WeaviateVectorStore.class);
    
    @Inject
    @org.eclipse.microprofile.rest.client.inject.RestClient
    WeaviateClient client;
    
    @Inject
    WeaviateConfig config;
    
    @Override
    public Uni<List<RetrievedDocument>> similaritySearch(
            float[] queryEmbedding,
            int topK,
            float minSimilarity,
            List<String> collections,
            Map<String, Object> filters) {
        
        // Build GraphQL query
        String query = buildWeaviateQuery(queryEmbedding, topK, collections, filters);
        
        WeaviateGraphQLRequest request = new WeaviateGraphQLRequest(query);
        
        return client.graphql(request)
            .map(response -> parseWeaviateResponse(response, minSimilarity));
    }
    
    @Override
    public Uni<List<RetrievedDocument>> keywordSearch(
            String queryText,
            int topK,
            List<String> collections,
            Map<String, Object> filters) {
        
        // Weaviate BM25 search
        String query = String.format("""
            {
                Get {
                    Document(
                        bm25: {query: "%s"}
                        limit: %d
                    ) {
                        _additional { id score }
                        content
                        metadata
                        collection
                    }
                }
            }
            """, queryText, topK);
        
        WeaviateGraphQLRequest request = new WeaviateGraphQLRequest(query);
        
        return client.graphql(request)
            .map(response -> parseWeaviateResponse(response, 0f));
    }
    
    @Override
    public Uni<Void> index(List<VectorDocument> documents) {
        
        List<Uni<Void>> unis = documents.stream()
            .map(doc -> {
                WeaviateObject object = new WeaviateObject(
                    config.className(),
                    Map.of(
                        "content", doc.content(),
                        "metadata", doc.metadata(),
                        "collection", doc.collectionName(),
                        "tenantId", doc.tenantId()
                    ),
                    doc.embedding()
                );
                
                return client.createObject(object)
                    .replaceWithVoid();
            })
            .collect(Collectors.toList());
        
        return Uni.join().all(unis).andCollectFailures()
            .replaceWithVoid();
    }
    
    @Override
    public Uni<Void> delete(List<String> documentIds, String tenantId) {
        // Weaviate batch delete
        List<Uni<Void>> unis = documentIds.stream()
            .map(id -> client.deleteObject(config.className(), id)
                .replaceWithVoid())
            .collect(Collectors.toList());
        
        return Uni.join().all(unis).andCollectFailures()
            .replaceWithVoid();
    }
    
    @Override
    public Uni<Long> count(String tenantId, List<String> collections) {
        String query = String.format("""
            {
                Aggregate {
                    Document(where: {
                        path: ["tenantId"],
                        operator: Equal,
                        valueString: "%s"
                    }) {
                        meta { count }
                    }
                }
            }
            """, tenantId);
        
        WeaviateGraphQLRequest request = new WeaviateGraphQLRequest(query);
        
        return client.graphql(request)
            .map(response -> {
                // Parse count from response
                return 0L; // Simplified
            });
    }
    
    private String buildWeaviateQuery(
            float[] embedding,
            int topK,
            List<String> collections,
            Map<String, Object> filters) {
        
        return String.format("""
            {
                Get {
                    Document(
                        nearVector: {vector: %s}
                        limit: %d
                    ) {
                        _additional { id certainty }
                        content
                        metadata
                        collection
                    }
                }
            }
            """, Arrays.toString(embedding), topK);
    }
    
    private List<RetrievedDocument> parseWeaviateResponse(
            WeaviateGraphQLResponse response,
            float minSimilarity) {
        
        // Simplified parsing
        return List.of();
    }
}

/**
 * Configuration for vector stores
 */
@ApplicationScoped
public class VectorStoreConfig {
    
    @org.eclipse.microprofile.config.inject.ConfigProperty(name = "silat.vectorstore.default.backend")
    String defaultBackend;
    
    public String getBackend(String tenantId) {
        // Could load from database or config
        return defaultBackend;
    }
}


package tech.kayys.silat.executor.rag.llm;

import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.rest.client.inject.RestClient;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.*;

/**
 * ============================================================================
 * LLM SERVICE
 * ============================================================================
 * 
 * Multi-provider LLM service for answer generation.
 * Supports OpenAI, Anthropic, Azure OpenAI, and local models.
 */
@ApplicationScoped
public class LlmService {
    
    private static final Logger LOG = LoggerFactory.getLogger(LlmService.class);
    
    @Inject
    OpenAILlmProvider openAIProvider;
    
    @Inject
    AnthropicLlmProvider anthropicProvider;
    
    @Inject
    AzureOpenAIProvider azureProvider;
    
    /**
     * Generate completion with automatic provider selection
     */
    public Uni<String> generate(LlmRequest request) {
        LOG.debug("Generating completion with provider: {}, model: {}", 
            request.provider(), request.model());
        
        LlmProvider provider = selectProvider(request.provider());
        
        return provider.complete(request)
            .onFailure().retry().atMost(2)
            .onFailure().invoke(e -> 
                LOG.error("LLM generation failed", e));
    }
    
    /**
     * Select provider based on configuration
     */
    private LlmProvider selectProvider(String providerName) {
        return switch (providerName.toLowerCase()) {
            case "openai" -> openAIProvider;
            case "anthropic", "claude" -> anthropicProvider;
            case "azure", "azure-openai" -> azureProvider;
            default -> {
                LOG.warn("Unknown provider {}, defaulting to OpenAI", providerName);
                yield openAIProvider;
            }
        };
    }
}

/**
 * LLM provider interface
 */
interface LlmProvider {
    Uni<String> complete(LlmRequest request);
    Uni<io.smallrye.mutiny.Multi<String>> streamComplete(LlmRequest request);
}

/**
 * OpenAI LLM provider
 */
@ApplicationScoped
class OpenAILlmProvider implements LlmProvider {
    
    private static final Logger LOG = LoggerFactory.getLogger(OpenAILlmProvider.class);
    
    @Inject
    @RestClient
    OpenAIClient client;
    
    @Override
    public Uni<String> complete(LlmRequest request) {
        
        List<OpenAIChatMessage> messages = new ArrayList<>();
        
        // Add system message if provided
        if (request.systemPrompt() != null) {
            messages.add(new OpenAIChatMessage(
                "system",
                request.systemPrompt()
            ));
        }
        
        // Add user message
        messages.add(new OpenAIChatMessage(
            "user",
            request.prompt()
        ));
        
        OpenAIChatRequest chatRequest = new OpenAIChatRequest(
            request.model(),
            messages,
            request.temperature(),
            request.maxTokens(),
            1.0,
            0.0,
            0.0,
            null
        );
        
        return client.createChatCompletion(chatRequest)
            .map(response -> {
                if (response.choices().isEmpty()) {
                    throw new IllegalStateException("No completion choices returned");
                }
                return response.choices().get(0).message().content();
            });
    }
    
    @Override
    public Uni<io.smallrye.mutiny.Multi<String>> streamComplete(LlmRequest request) {
        // Streaming implementation
        return Uni.createFrom().item(
            io.smallrye.mutiny.Multi.createFrom().empty()
        );
    }
}

/**
 * Anthropic Claude provider
 */
@ApplicationScoped
class AnthropicLlmProvider implements LlmProvider {
    
    private static final Logger LOG = LoggerFactory.getLogger(AnthropicLlmProvider.class);
    
    @Inject
    @RestClient
    AnthropicClient client;
    
    @Override
    public Uni<String> complete(LlmRequest request) {
        
        AnthropicRequest anthropicRequest = new AnthropicRequest(
            request.model(),
            request.maxTokens(),
            request.systemPrompt(),
            List.of(new AnthropicMessage(
                "user",
                request.prompt()
            )),
            request.temperature(),
            1.0,
            false
        );
        
        return client.createMessage(anthropicRequest)
            .map(response -> {
                if (response.content().isEmpty()) {
                    throw new IllegalStateException("No completion returned");
                }
                return response.content().get(0).text();
            });
    }
    
    @Override
    public Uni<io.smallrye.mutiny.Multi<String>> streamComplete(LlmRequest request) {
        return Uni.createFrom().item(
            io.smallrye.mutiny.Multi.createFrom().empty()
        );
    }
}

/**
 * Azure OpenAI provider
 */
@ApplicationScoped
class AzureOpenAIProvider implements LlmProvider {
    
    @Inject
    @RestClient
    AzureOpenAIClient client;
    
    @Inject
    AzureOpenAIConfig config;
    
    @Override
    public Uni<String> complete(LlmRequest request) {
        
        List<OpenAIChatMessage> messages = List.of(
            new OpenAIChatMessage("system", request.systemPrompt()),
            new OpenAIChatMessage("user", request.prompt())
        );
        
        OpenAIChatRequest chatRequest = new OpenAIChatRequest(
            request.model(),
            messages,
            request.temperature(),
            request.maxTokens(),
            1.0, 0.0, 0.0, null
        );
        
        return client.createChatCompletion(
            config.deploymentId(),
            config.apiVersion(),
            chatRequest
        )
        .map(response -> response.choices().get(0).message().content());
    }
    
    @Override
    public Uni<io.smallrye.mutiny.Multi<String>> streamComplete(LlmRequest request) {
        return Uni.createFrom().item(
            io.smallrye.mutiny.Multi.createFrom().empty()
        );
    }
}

// ==================== RERANKING SERVICE ====================

package tech.kayys.silat.executor.rag.rerank;

import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.rest.client.inject.RestClient;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tech.kayys.silat.executor.rag.domain.*;

import java.util.*;
import java.util.stream.Collectors;

/**
 * Reranking service using cross-encoders
 */
@ApplicationScoped
public class RerankingService {
    
    private static final Logger LOG = LoggerFactory.getLogger(RerankingService.class);
    
    @Inject
    CrossEncoderRerankingProvider crossEncoderProvider;
    
    @Inject
    CohereRerankingProvider cohereProvider;
    
    @Inject
    VoyageRerankingProvider voyageProvider;
    
    /**
     * Rerank documents using specified model
     */
    public Uni<List<RetrievedDocument>> rerank(
            String query,
            List<RetrievedDocument> documents,
            RerankingModel model,
            int topK) {
        
        if (documents.isEmpty()) {
            return Uni.createFrom().item(documents);
        }
        
        LOG.debug("Reranking {} documents with model: {}", documents.size(), model);
        
        RerankingProvider provider = selectProvider(model);
        
        return provider.rerank(query, documents, model.getModelId())
            .map(scores -> {
                // Update rerank scores
                List<RetrievedDocument> reranked = new ArrayList<>();
                for (int i = 0; i < documents.size(); i++) {
                    RetrievedDocument doc = documents.get(i);
                    float rerankScore = scores.get(i);
                    
                    RetrievedDocument updated = new RetrievedDocument(
                        doc.id(),
                        doc.content(),
                        doc.metadata(),
                        doc.score(),
                        rerankScore,
                        doc.chunkIndex(),
                        doc.collectionName(),
                        doc.sourceId(),
                        doc.sourceType(),
                        doc.indexedAt(),
                        doc.highlights(),
                        doc.debugInfo()
                    );
                    reranked.add(updated);
                }
                
                // Sort by rerank score and take top-k
                return reranked.stream()
                    .sorted((a, b) -> Float.compare(b.rerankScore(), a.rerankScore()))
                    .limit(topK)
                    .collect(Collectors.toList());
            });
    }
    
    private RerankingProvider selectProvider(RerankingModel model) {
        return switch (model) {
            case COHERE_RERANK -> cohereProvider;
            case VOYAGE_RERANK -> voyageProvider;
            default -> crossEncoderProvider;
        };
    }
}

/**
 * Reranking provider interface
 */
interface RerankingProvider {
    Uni<List<Float>> rerank(
        String query,
        List<RetrievedDocument> documents,
        String model
    );
}

/**
 * Cross-encoder reranking using Hugging Face models
 */
@ApplicationScoped
class CrossEncoderRerankingProvider implements RerankingProvider {
    
    private static final Logger LOG = LoggerFactory.getLogger(CrossEncoderRerankingProvider.class);
    
    @Inject
    @RestClient
    HuggingFaceClient client;
    
    @Override
    public Uni<List<Float>> rerank(
            String query,
            List<RetrievedDocument> documents,
            String model) {
        
        // Prepare input pairs
        List<List<String>> pairs = documents.stream()
            .map(doc -> List.of(query, doc.content()))
            .collect(Collectors.toList());
        
        HuggingFaceRequest request = new HuggingFaceRequest(
            model,
            pairs
        );
        
        return client.inference(request)
            .map(response -> response.scores());
    }
}

/**
 * Cohere reranking provider
 */
@ApplicationScoped
class CohereRerankingProvider implements RerankingProvider {
    
    @Inject
    @RestClient
    CohereClient client;
    
    @Override
    public Uni<List<Float>> rerank(
            String query,
            List<RetrievedDocument> documents,
            String model) {
        
        List<String> docs = documents.stream()
            .map(RetrievedDocument::content)
            .collect(Collectors.toList());
        
        CohereRerankRequest request = new CohereRerankRequest(
            model,
            query,
            docs,
            documents.size(),
            true
        );
        
        return client.rerank(request)
            .map(response -> response.results().stream()
                .map(CohereRerankResult::relevanceScore)
                .collect(Collectors.toList())
            );
    }
}

/**
 * Voyage AI reranking provider
 */
@ApplicationScoped
class VoyageRerankingProvider implements RerankingProvider {
    
    @Inject
    @RestClient
    VoyageClient client;
    
    @Override
    public Uni<List<Float>> rerank(
            String query,
            List<RetrievedDocument> documents,
            String model) {
        
        List<String> docs = documents.stream()
            .map(RetrievedDocument::content)
            .collect(Collectors.toList());
        
        VoyageRerankRequest request = new VoyageRerankRequest(
            query,
            docs,
            model,
            documents.size()
        );
        
        return client.rerank(request)
            .map(response -> response.results().stream()
                .map(VoyageRerankResult::relevanceScore)
                .collect(Collectors.toList())
            );
    }
}

// ==================== QUERY ANALYZER ====================

package tech.kayys.silat.executor.rag;

import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tech.kayys.silat.executor.rag.domain.*;
import tech.kayys.silat.executor.rag.llm.LlmService;

import java.util.*;
import java.util.regex.Pattern;

/**
 * Query analyzer for routing and optimization
 */
@ApplicationScoped
public class QueryAnalyzer {
    
    private static final Logger LOG = LoggerFactory.getLogger(QueryAnalyzer.class);
    
    @Inject
    LlmService llmService;
    
    // Entity extraction patterns
    private static final Pattern EMAIL_PATTERN = 
        Pattern.compile("[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}");
    private static final Pattern URL_PATTERN = 
        Pattern.compile("https?://[^\\s]+");
    private static final Pattern DATE_PATTERN = 
        Pattern.compile("\\d{4}-\\d{2}-\\d{2}");
    
    /**
     * Analyze query to determine complexity and intent
     */
    public Uni<QueryAnalysis> analyze(String query, String tenantId) {
        
        // Extract basic features
        QueryComplexity complexity = determineComplexity(query);
        QueryIntent intent = determineIntent(query);
        List<String> entities = extractEntities(query);
        List<String> keywords = extractKeywords(query);
        
        boolean requiresFactual = checkRequiresFactual(query);
        boolean requiresMultiHop = checkRequiresMultiHop(query);
        boolean requiresTemporal = checkRequiresTemporal(query);
        
        float confidence = estimateConfidence(query, complexity);
        
        List<String> suggestedCollections = suggestCollections(
            query, intent, entities);
        
        return Uni.createFrom().item(new QueryAnalysis(
            query,
            complexity,
            intent,
            entities,
            keywords,
            requiresFactual,
            requiresMultiHop,
            requiresTemporal,
            confidence,
            suggestedCollections,
            Map.of(
                "wordCount", query.split("\\s+").length,
                "hasQuestionMark", query.contains("?")
            )
        ));
    }
    
    /**
     * Generate query variations for multi-query search
     */
    public Uni<List<String>> generateQueryVariations(
            String originalQuery,
            int numVariations) {
        
        // Use LLM to generate variations
        String prompt = String.format("""
            Generate %d alternative phrasings of the following question.
            Each variation should be semantically similar but use different wording.
            Return only the variations, one per line, without numbering.
            
            Original question: %s
            
            Variations:
            """, numVariations, originalQuery);
        
        LlmRequest request = new LlmRequest(
            prompt,
            "openai",
            "gpt-3.5-turbo",
            0.7f,
            200,
            "You are a helpful assistant that rephrases questions.",
            false,
            "system",
            Map.of()
        );
        
        return llmService.generate(request)
            .map(response -> {
                List<String> variations = new ArrayList<>();
                variations.add(originalQuery);
                
                String[] lines = response.split("\n");
                for (String line : lines) {
                    String trimmed = line.trim();
                    if (!trimmed.isEmpty() && !trimmed.equals(originalQuery)) {
                        variations.add(trimmed);
                    }
                }
                
                return variations.subList(0, Math.min(numVariations + 1, variations.size()));
            });
    }
    
    /**
     * Determine query complexity
     */
    private QueryComplexity determineComplexity(String query) {
        int wordCount = query.split("\\s+").length;
        
        if (wordCount <= 5) {
            return QueryComplexity.SIMPLE;
        } else if (wordCount <= 15) {
            return QueryComplexity.MODERATE;
        } else if (wordCount <= 30) {
            return QueryComplexity.COMPLEX;
        } else {
            return QueryComplexity.VERY_COMPLEX;
        }
    }
    
    /**
     * Determine query intent
     */
    private QueryIntent determineIntent(String query) {
        String lower = query.toLowerCase();
        
        if (lower.matches(".*\\b(what|who|when|where|which)\\b.*")) {
            return QueryIntent.FACTUAL;
        } else if (lower.matches(".*\\b(how|why|explain)\\b.*")) {
            return QueryIntent.EXPLORATORY;
        } else if (lower.matches(".*\\b(compare|difference|versus|vs)\\b.*")) {
            return QueryIntent.COMPARATIVE;
        } else if (lower.matches(".*\\b(how to|steps|guide|tutorial)\\b.*")) {
            return QueryIntent.PROCEDURAL;
        } else if (lower.matches(".*\\b(analyze|evaluate|assess)\\b.*")) {
            return QueryIntent.ANALYTICAL;
        } else {
            return QueryIntent.CONVERSATIONAL;
        }
    }
    
    /**
     * Extract entities from query
     */
    private List<String> extractEntities(String query) {
        List<String> entities = new ArrayList<>();
        
        // Extract emails
        var emailMatcher = EMAIL_PATTERN.matcher(query);
        while (emailMatcher.find()) {
            entities.add(emailMatcher.group());
        }
        
        // Extract URLs
        var urlMatcher = URL_PATTERN.matcher(query);
        while (urlMatcher.find()) {
            entities.add(urlMatcher.group());
        }
        
        // Extract dates
        var dateMatcher = DATE_PATTERN.matcher(query);
        while (dateMatcher.find()) {
            entities.add(dateMatcher.group());
        }
        
        // Extract capitalized words (potential named entities)
        String[] words = query.split("\\s+");
        for (String word : words) {
            if (word.length() > 1 && 
                Character.isUpperCase(word.charAt(0)) &&
                !word.equals(word.toUpperCase())) {
                entities.add(word);
            }
        }
        
        return entities;
    }
    
    /**
     * Extract keywords from query
     */
    private List<String> extractKeywords(String query) {
        // Simple keyword extraction - remove stopwords
        Set<String> stopwords = Set.of(
            "the", "a", "an", "and", "or", "but", "in", "on", "at",
            "to", "for", "of", "with", "by", "from", "as", "is", "was",
            "are", "were", "be", "been", "being", "have", "has", "had",
            "do", "does", "did", "will", "would", "should", "could",
            "may", "might", "must", "can"
        );
        
        String[] words = query.toLowerCase()
            .replaceAll("[^a-z0-9\\s]", "")
            .split("\\s+");
        
        return Arrays.stream(words)
            .filter(word -> word.length() > 2 && !stopwords.contains(word))
            .distinct()
            .toList();
    }
    
    private boolean checkRequiresFactual(String query) {
        String lower = query.toLowerCase();
        return lower.contains("what is") || 
               lower.contains("who is") ||
               lower.contains("when did") ||
               lower.contains("where is");
    }
    
    private boolean checkRequiresMultiHop(String query) {
        String lower = query.toLowerCase();
        return lower.contains("and then") ||
               lower.contains("after that") ||
               lower.contains("because") ||
               lower.split("\\?").length > 1;
    }
    
    private boolean checkRequiresTemporal(String query) {
        String lower = query.toLowerCase();
        return lower.matches(".*\\b(when|before|after|during|since|until)\\b.*") ||
               DATE_PATTERN.matcher(query).find();
    }
    
    private float estimateConfidence(String query, QueryComplexity complexity) {
        // Simple heuristic
        return switch (complexity) {
            case SIMPLE -> 0.9f;
            case MODERATE -> 0.75f;
            case COMPLEX -> 0.6f;
            case VERY_COMPLEX -> 0.5f;
        };
    }
    
    private List<String> suggestCollections(
            String query,
            QueryIntent intent,
            List<String> entities) {
        
        // Simple rule-based collection suggestion
        List<String> suggestions = new ArrayList<>();
        
        String lower = query.toLowerCase();
        if (lower.contains("document") || lower.contains("pdf")) {
            suggestions.add("documents");
        }
        if (lower.contains("code") || lower.contains("function")) {
            suggestions.add("code");
        }
        if (lower.contains("email") || lower.contains("message")) {
            suggestions.add("communications");
        }
        
        return suggestions;
    }
}

package tech.kayys.silat.executor.rag.embedding;

import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.rest.client.inject.RestClient;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tech.kayys.silat.executor.rag.domain.*;

import java.util.*;

/**
 * ============================================================================
 * EMBEDDING SERVICE
 * ============================================================================
 * 
 * Multi-provider embedding generation with caching and batching.
 * Supports OpenAI, Cohere, Voyage AI, and custom models.
 */
@ApplicationScoped
public class EmbeddingService {
    
    private static final Logger LOG = LoggerFactory.getLogger(EmbeddingService.class);
    
    @Inject
    OpenAIEmbeddingProvider openAIProvider;
    
    @Inject
    CohereEmbeddingProvider cohereProvider;
    
    @Inject
    VoyageAIProvider voyageProvider;
    
    @Inject
    EmbeddingCacheService cacheService;
    
    /**
     * Generate embeddings with automatic provider selection
     */
    public Uni<EmbeddingResponse> generateEmbeddings(EmbeddingRequest request) {
        LOG.debug("Generating embeddings for {} texts with model: {}", 
            request.texts().size(), request.model());
        
        long startTime = System.currentTimeMillis();
        
        // Check cache first
        return cacheService.getBatch(request.texts(), request.model(), request.tenantId())
            .flatMap(cached -> {
                List<String> uncachedTexts = new ArrayList<>();
                Map<String, float[]> cachedEmbeddings = new HashMap<>();
                
                for (int i = 0; i < request.texts().size(); i++) {
                    String text = request.texts().get(i);
                    if (cached.containsKey(text)) {
                        cachedEmbeddings.put(text, cached.get(text));
                    } else {
                        uncachedTexts.add(text);
                    }
                }
                
                LOG.debug("Cache hit: {}/{}", cachedEmbeddings.size(), request.texts().size());
                
                if (uncachedTexts.isEmpty()) {
                    // All cached
                    return Uni.createFrom().item(buildResponse(
                        request.texts(), cachedEmbeddings, request.model(), startTime));
                }
                
                // Generate embeddings for uncached texts
                EmbeddingProvider provider = selectProvider(request.model());
                
                return provider.generateEmbeddings(uncachedTexts, request.model(), request.config())
                    .flatMap(newEmbeddings -> {
                        // Store in cache
                        Map<String, float[]> toCache = new HashMap<>();
                        for (int i = 0; i < uncachedTexts.size(); i++) {
                            toCache.put(uncachedTexts.get(i), newEmbeddings.get(i));
                        }
                        
                        return cacheService.putBatch(toCache, request.model(), request.tenantId())
                            .map(v -> {
                                // Merge cached and new embeddings
                                cachedEmbeddings.putAll(toCache);
                                return buildResponse(
                                    request.texts(), cachedEmbeddings, 
                                    request.model(), startTime);
                            });
                    });
            });
    }
    
    /**
     * Select appropriate provider based on model
     */
    private EmbeddingProvider selectProvider(String model) {
        if (model.startsWith("text-embedding")) {
            return openAIProvider;
        } else if (model.startsWith("embed-")) {
            return cohereProvider;
        } else if (model.startsWith("voyage-")) {
            return voyageProvider;
        } else {
            LOG.warn("Unknown model {}, defaulting to OpenAI", model);
            return openAIProvider;
        }
    }
    
    /**
     * Build embedding response maintaining order
     */
    private EmbeddingResponse buildResponse(
            List<String> texts,
            Map<String, float[]> embeddings,
            String model,
            long startTime) {
        
        List<float[]> orderedEmbeddings = texts.stream()
            .map(embeddings::get)
            .toList();
        
        int dimensions = orderedEmbeddings.isEmpty() ? 0 : 
            orderedEmbeddings.get(0).length;
        
        return new EmbeddingResponse(
            orderedEmbeddings,
            model,
            dimensions,
            System.currentTimeMillis() - startTime,
            Map.of("cached", embeddings.size() - texts.size())
        );
    }
}

/**
 * Embedding provider interface
 */
interface EmbeddingProvider {
    Uni<List<float[]>> generateEmbeddings(
        List<String> texts, 
        String model, 
        Map<String, Object> config
    );
    
    int getMaxBatchSize();
    int getDimensions(String model);
}

/**
 * OpenAI embedding provider
 */
@ApplicationScoped
class OpenAIEmbeddingProvider implements EmbeddingProvider {
    
    private static final Logger LOG = LoggerFactory.getLogger(OpenAIEmbeddingProvider.class);
    private static final int MAX_BATCH_SIZE = 2048;
    
    @Inject
    @RestClient
    OpenAIClient client;
    
    @Inject
    OpenAIConfig config;
    
    @Override
    public Uni<List<float[]>> generateEmbeddings(
            List<String> texts,
            String model,
            Map<String, Object> providerConfig) {
        
        // Batch texts if needed
        if (texts.size() > MAX_BATCH_SIZE) {
            return batchGenerate(texts, model, providerConfig);
        }
        
        OpenAIEmbeddingRequest request = new OpenAIEmbeddingRequest(
            model,
            texts,
            "float"
        );
        
        return client.createEmbeddings(request)
            .map(response -> response.data().stream()
                .map(OpenAIEmbeddingData::embedding)
                .toList()
            )
            .onFailure().retry().atMost(3)
            .onFailure().invoke(e -> 
                LOG.error("OpenAI embedding generation failed", e));
    }
    
    /**
     * Batch large requests
     */
    private Uni<List<float[]>> batchGenerate(
            List<String> texts,
            String model,
            Map<String, Object> config) {
        
        List<List<String>> batches = new ArrayList<>();
        for (int i = 0; i < texts.size(); i += MAX_BATCH_SIZE) {
            batches.add(texts.subList(i, 
                Math.min(i + MAX_BATCH_SIZE, texts.size())));
        }
        
        List<Uni<List<float[]>>> unis = batches.stream()
            .map(batch -> generateEmbeddings(batch, model, config))
            .toList();
        
        return Uni.combine().all().unis(unis).combinedWith(results -> {
            List<float[]> combined = new ArrayList<>();
            for (Object result : results) {
                @SuppressWarnings("unchecked")
                List<float[]> batch = (List<float[]>) result;
                combined.addAll(batch);
            }
            return combined;
        });
    }
    
    @Override
    public int getMaxBatchSize() {
        return MAX_BATCH_SIZE;
    }
    
    @Override
    public int getDimensions(String model) {
        return switch (model) {
            case "text-embedding-3-small" -> 1536;
            case "text-embedding-3-large" -> 3072;
            case "text-embedding-ada-002" -> 1536;
            default -> 1536;
        };
    }
}

/**
 * Cohere embedding provider
 */
@ApplicationScoped
class CohereEmbeddingProvider implements EmbeddingProvider {
    
    private static final Logger LOG = LoggerFactory.getLogger(CohereEmbeddingProvider.class);
    
    @Inject
    @RestClient
    CohereClient client;
    
    @Override
    public Uni<List<float[]>> generateEmbeddings(
            List<String> texts,
            String model,
            Map<String, Object> config) {
        
        CohereEmbedRequest request = new CohereEmbedRequest(
            texts,
            model,
            "float",
            "search_document"
        );
        
        return client.embed(request)
            .map(response -> response.embeddings());
    }
    
    @Override
    public int getMaxBatchSize() {
        return 96;
    }
    
    @Override
    public int getDimensions(String model) {
        return switch (model) {
            case "embed-english-v3.0" -> 1024;
            case "embed-multilingual-v3.0" -> 1024;
            case "embed-english-light-v3.0" -> 384;
            default -> 1024;
        };
    }
}

/**
 * Voyage AI provider
 */
@ApplicationScoped
class VoyageAIProvider implements EmbeddingProvider {
    
    @Inject
    @RestClient
    VoyageClient client;
    
    @Override
    public Uni<List<float[]>> generateEmbeddings(
            List<String> texts,
            String model,
            Map<String, Object> config) {
        
        VoyageEmbedRequest request = new VoyageEmbedRequest(
            texts,
            model,
            "document"
        );
        
        return client.embed(request)
            .map(response -> response.embeddings());
    }
    
    @Override
    public int getMaxBatchSize() {
        return 128;
    }
    
    @Override
    public int getDimensions(String model) {
        return 1024;
    }
}

// ==================== VECTOR STORE SERVICE ====================

package tech.kayys.silat.executor.rag.vectorstore;

import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tech.kayys.silat.executor.rag.domain.*;

import java.util.*;
import java.util.stream.Collectors;

/**
 * Vector store service with multi-backend support
 */
@ApplicationScoped
public class VectorStoreService {
    
    private static final Logger LOG = LoggerFactory.getLogger(VectorStoreService.class);
    
    @Inject
    VectorStoreRegistry registry;
    
    @Inject
    VectorStoreConfig config;
    
    /**
     * Search vector store with hybrid strategy
     */
    public Uni<List<RetrievedDocument>> search(VectorSearchRequest request) {
        LOG.debug("Searching vector store: strategy={}, topK={}", 
            request.strategy(), request.topK());
        
        // Get appropriate vector store for tenant
        VectorStore store = registry.getStore(request.tenantId());
        
        return switch (request.strategy()) {
            case VECTOR_ONLY -> vectorSearch(store, request);
            case KEYWORD_ONLY -> keywordSearch(store, request);
            case HYBRID -> hybridSearch(store, request);
            case SEMANTIC_RERANK -> semanticRerankSearch(store, request);
            case MULTI_QUERY -> multiQuerySearch(store, request);
            case HYPOTHETICAL_ANSWER -> hydeSearch(store, request);
        };
    }
    
    /**
     * Pure vector similarity search
     */
    private Uni<List<RetrievedDocument>> vectorSearch(
            VectorStore store,
            VectorSearchRequest request) {
        
        return store.similaritySearch(
            request.queryEmbedding(),
            request.topK(),
            request.minSimilarity(),
            request.collections(),
            request.filters()
        );
    }
    
    /**
     * Keyword-based search
     */
    private Uni<List<RetrievedDocument>> keywordSearch(
            VectorStore store,
            VectorSearchRequest request) {
        
        return store.keywordSearch(
            request.queryText(),
            request.topK(),
            request.collections(),
            request.filters()
        );
    }
    
    /**
     * Hybrid search combining vector and keyword
     */
    private Uni<List<RetrievedDocument>> hybridSearch(
            VectorStore store,
            VectorSearchRequest request) {
        
        // Retrieve more candidates
        int candidateCount = request.topK() * 2;
        
        return Uni.combine().all().unis(
            vectorSearch(store, new VectorSearchRequest(
                request.queryEmbedding(),
                request.queryText(),
                candidateCount,
                request.minSimilarity(),
                request.tenantId(),
                request.collections(),
                request.filters(),
                SearchStrategy.VECTOR_ONLY,
                request.hybridAlpha()
            )),
            keywordSearch(store, new VectorSearchRequest(
                request.queryEmbedding(),
                request.queryText(),
                candidateCount,
                request.minSimilarity(),
                request.tenantId(),
                request.collections(),
                request.filters(),
                SearchStrategy.KEYWORD_ONLY,
                request.hybridAlpha()
            ))
        ).combinedWith((vectorResults, keywordResults) -> {
            // Fuse results using Reciprocal Rank Fusion
            return fuseResults(
                vectorResults,
                keywordResults,
                request.hybridAlpha(),
                request.topK()
            );
        });
    }
    
    /**
     * Reciprocal Rank Fusion for hybrid search
     */
    private List<RetrievedDocument> fuseResults(
            List<RetrievedDocument> vectorResults,
            List<RetrievedDocument> keywordResults,
            float alpha,
            int topK) {
        
        Map<String, FusionScore> scores = new HashMap<>();
        
        // Add vector scores
        for (int i = 0; i < vectorResults.size(); i++) {
            RetrievedDocument doc = vectorResults.get(i);
            float rrf = 1.0f / (60 + i + 1); // RRF with k=60
            scores.computeIfAbsent(doc.id(), k -> new FusionScore(doc))
                .vectorScore = rrf * alpha;
        }
        
        // Add keyword scores
        for (int i = 0; i < keywordResults.size(); i++) {
            RetrievedDocument doc = keywordResults.get(i);
            float rrf = 1.0f / (60 + i + 1);
            scores.computeIfAbsent(doc.id(), k -> new FusionScore(doc))
                .keywordScore = rrf * (1 - alpha);
        }
        
        // Sort by combined score and return top-k
        return scores.values().stream()
            .sorted((a, b) -> Float.compare(b.getTotalScore(), a.getTotalScore()))
            .limit(topK)
            .map(fs -> fs.doc)
            .collect(Collectors.toList());
    }
    
    /**
     * Semantic reranking search
     */
    private Uni<List<RetrievedDocument>> semanticRerankSearch(
            VectorStore store,
            VectorSearchRequest request) {
        
        // Retrieve more candidates for reranking
        int candidateCount = request.topK() * 3;
        
        return vectorSearch(store, new VectorSearchRequest(
            request.queryEmbedding(),
            request.queryText(),
            candidateCount,
            request.minSimilarity(),
            request.tenantId(),
            request.collections(),
            request.filters(),
            SearchStrategy.VECTOR_ONLY,
            request.hybridAlpha()
        ));
    }
    
    /**
     * Multi-query search (handled upstream)
     */
    private Uni<List<RetrievedDocument>> multiQuerySearch(
            VectorStore store,
            VectorSearchRequest request) {
        
        return vectorSearch(store, request);
    }
    
    /**
     * HyDE (Hypothetical Document Embeddings) search
     */
    private Uni<List<RetrievedDocument>> hydeSearch(
            VectorStore store,
            VectorSearchRequest request) {
        
        // HyDE would generate hypothetical answer embedding
        // For now, fall back to vector search
        return vectorSearch(store, request);
    }
    
    /**
     * Index documents in vector store
     */
    public Uni<Void> indexDocuments(
            List<VectorDocument> documents,
            String tenantId) {
        
        LOG.info("Indexing {} documents for tenant {}", documents.size(), tenantId);
        
        VectorStore store = registry.getStore(tenantId);
        
        return store.index(documents);
    }
}

/**
 * Fusion score holder for hybrid search
 */
class FusionScore {
    final RetrievedDocument doc;
    float vectorScore = 0f;
    float keywordScore = 0f;
    
    FusionScore(RetrievedDocument doc) {
        this.doc = doc;
    }
    
    float getTotalScore() {
        return vectorScore + keywordScore;
    }
}

/**
 * Vector store interface
 */
interface VectorStore {
    
    Uni<List<RetrievedDocument>> similaritySearch(
        float[] queryEmbedding,
        int topK,
        float minSimilarity,
        List<String> collections,
        Map<String, Object> filters
    );
    
    Uni<List<RetrievedDocument>> keywordSearch(
        String queryText,
        int topK,
        List<String> collections,
        Map<String, Object> filters
    );
    
    Uni<Void> index(List<VectorDocument> documents);
    
    Uni<Void> delete(List<String> documentIds, String tenantId);
    
    Uni<Long> count(String tenantId, List<String> collections);
}

package tech.kayys.silat.executor.rag;

import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tech.kayys.silat.executor.AbstractWorkflowExecutor;
import tech.kayys.silat.executor.Executor;
import tech.kayys.silat.executor.rag.domain.*;
import tech.kayys.silat.executor.rag.embedding.EmbeddingService;
import tech.kayys.silat.executor.rag.vectorstore.VectorStoreService;
import tech.kayys.silat.executor.rag.llm.LlmService;
import tech.kayys.silat.executor.rag.rerank.RerankingService;
import tech.kayys.silat.executor.rag.cache.RagCacheService;
import tech.kayys.silat.core.domain.*;
import tech.kayys.silat.core.engine.NodeExecutionResult;
import tech.kayys.silat.core.engine.NodeExecutionTask;
import tech.kayys.silat.core.scheduler.CommunicationType;

import java.time.Instant;
import java.util.*;
import java.util.stream.Collectors;

/**
 * ============================================================================
 * RAG WORKFLOW EXECUTOR
 * ============================================================================
 * 
 * Production-ready RAG executor for Silat workflow engine.
 * 
 * Pipeline stages:
 * 1. Query Analysis & Routing
 * 2. Embedding Generation
 * 3. Vector Search + Hybrid Search
 * 4. Re-ranking
 * 5. Context Assembly
 * 6. LLM Generation
 * 7. Citation Extraction
 * 
 * Features:
 * - Multi-tenant isolation
 * - Multiple vector store backends
 * - Hybrid search (vector + keyword)
 * - Cross-encoder re-ranking
 * - Citation tracking
 * - Response caching
 * - Streaming support
 */
@Executor(
    executorType = "rag-executor",
    communicationType = CommunicationType.GRPC,
    maxConcurrentTasks = 50,
    supportedNodeTypes = {"RAG_QUERY", "SEMANTIC_SEARCH", "QA_GENERATION"},
    version = "1.0.0"
)
@ApplicationScoped
public class RagWorkflowExecutor extends AbstractWorkflowExecutor {
    
    private static final Logger LOG = LoggerFactory.getLogger(RagWorkflowExecutor.class);
    
    @Inject
    EmbeddingService embeddingService;
    
    @Inject
    VectorStoreService vectorStoreService;
    
    @Inject
    LlmService llmService;
    
    @Inject
    RerankingService rerankingService;
    
    @Inject
    RagCacheService cacheService;
    
    @Inject
    QueryAnalyzer queryAnalyzer;
    
    @Inject
    ContextAssembler contextAssembler;
    
    @Inject
    CitationExtractor citationExtractor;
    
    @Override
    public Uni<NodeExecutionResult> execute(NodeExecutionTask task) {
        long startTime = System.currentTimeMillis();
        
        LOG.info("Executing RAG task: run={}, node={}", 
            task.runId().value(), task.nodeId().value());
        
        try {
            // Parse RAG request from task context
            RagRequest request = parseRagRequest(task);
            
            // Validate request
            validateRequest(request);
            
            // Check cache first
            return cacheService.get(request)
                .onItem().ifNotNull().transform(cached -> {
                    LOG.info("Cache hit for query: {}", request.query());
                    return successResult(task, cached);
                })
                .onItem().ifNull().switchTo(() -> 
                    // Cache miss - execute full RAG pipeline
                    executeRagPipeline(request, startTime)
                        .flatMap(response -> 
                            // Store in cache
                            cacheService.put(request, response)
                                .replaceWith(response)
                        )
                        .map(response -> successResult(task, response))
                );
                
        } catch (Exception e) {
            LOG.error("RAG execution failed", e);
            return Uni.createFrom().item(
                failureResult(task, e)
            );
        }
    }
    
    /**
     * Execute complete RAG pipeline
     */
    private Uni<RagResponse> executeRagPipeline(RagRequest request, long startTime) {
        LOG.debug("Starting RAG pipeline for query: {}", request.query());
        
        Instant timestamp = Instant.now();
        Map<String, Long> timings = new HashMap<>();
        
        // Stage 1: Query Analysis
        return analyzeQuery(request)
            .invoke(analysis -> {
                LOG.debug("Query analysis: complexity={}, intent={}", 
                    analysis.complexity(), analysis.intent());
                timings.put("analysis", System.currentTimeMillis() - startTime);
            })
            
            // Stage 2: Generate query embedding
            .flatMap(analysis -> generateQueryEmbedding(request, analysis)
                .invoke(v -> timings.put("embedding", 
                    System.currentTimeMillis() - startTime - timings.get("analysis")))
            )
            
            // Stage 3: Retrieve documents
            .flatMap(embedding -> retrieveDocuments(request, embedding)
                .invoke(docs -> {
                    LOG.debug("Retrieved {} documents", docs.size());
                    timings.put("retrieval", 
                        System.currentTimeMillis() - startTime - 
                        timings.get("analysis") - timings.get("embedding"));
                })
            )
            
            // Stage 4: Re-rank documents (if enabled)
            .flatMap(documents -> rerankDocuments(request, documents)
                .invoke(reranked -> {
                    LOG.debug("Re-ranked to {} documents", reranked.size());
                    timings.put("reranking", 
                        System.currentTimeMillis() - startTime - 
                        timings.values().stream().mapToLong(Long::longValue).sum());
                })
            )
            
            // Stage 5: Assemble context
            .map(documents -> contextAssembler.assemble(
                request, documents))
            
            // Stage 6: Generate answer
            .flatMap(context -> generateAnswer(request, context)
                .invoke(answer -> {
                    LOG.debug("Generated answer: {} chars", answer.length());
                    timings.put("generation", 
                        System.currentTimeMillis() - startTime - 
                        timings.values().stream().mapToLong(Long::longValue).sum());
                })
                .map(answer -> Map.entry(context, answer))
            )
            
            // Stage 7: Extract citations
            .map(entry -> {
                AssembledContext context = entry.getKey();
                String answer = entry.getValue();
                
                List<Citation> citations = request.generationConfig().enableCitations() ?
                    citationExtractor.extract(answer, context.documents()) :
                    List.of();
                
                // Build metrics
                RagMetrics metrics = buildMetrics(timings, context, answer);
                
                return new RagResponse(
                    request.query(),
                    answer,
                    context.documents(),
                    citations,
                    metrics,
                    request.sessionId(),
                    timestamp,
                    request.metadata(),
                    List.of(),
                    Optional.empty()
                );
            })
            
            .onFailure().recoverWithItem(error -> {
                LOG.error("RAG pipeline failed", error);
                return new RagResponse(
                    request.query(),
                    "",
                    List.of(),
                    List.of(),
                    null,
                    request.sessionId(),
                    timestamp,
                    request.metadata(),
                    List.of(),
                    Optional.of(error.getMessage())
                );
            });
    }
    
    /**
     * Analyze query to determine routing and strategy
     */
    private Uni<QueryAnalysis> analyzeQuery(RagRequest request) {
        return queryAnalyzer.analyze(request.query(), request.tenantId());
    }
    
    /**
     * Generate embedding for query
     */
    private Uni<float[]> generateQueryEmbedding(
            RagRequest request, 
            QueryAnalysis analysis) {
        
        // Handle multi-query strategy
        if (request.searchStrategy() == SearchStrategy.MULTI_QUERY &&
            request.retrievalConfig().enableMultiQuery()) {
            
            return queryAnalyzer.generateQueryVariations(
                request.query(), 
                request.retrievalConfig().numQueryVariations()
            )
            .flatMap(variations -> {
                // Generate embeddings for all variations
                EmbeddingRequest embReq = new EmbeddingRequest(
                    variations,
                    getEmbeddingModel(request),
                    request.tenantId(),
                    Map.of()
                );
                
                return embeddingService.generateEmbeddings(embReq)
                    .map(response -> 
                        // Average embeddings
                        averageEmbeddings(response.embeddings())
                    );
            });
        }
        
        // Standard single query embedding
        EmbeddingRequest embReq = new EmbeddingRequest(
            List.of(request.query()),
            getEmbeddingModel(request),
            request.tenantId(),
            Map.of()
        );
        
        return embeddingService.generateEmbeddings(embReq)
            .map(response -> response.embeddings().get(0));
    }
    
    /**
     * Retrieve documents from vector store
     */
    private Uni<List<RetrievedDocument>> retrieveDocuments(
            RagRequest request,
            float[] queryEmbedding) {
        
        VectorSearchRequest searchRequest = new VectorSearchRequest(
            queryEmbedding,
            request.query(),
            request.retrievalConfig().topK(),
            request.retrievalConfig().minSimilarity(),
            request.tenantId(),
            request.retrievalConfig().collections(),
            request.filters(),
            request.searchStrategy(),
            request.retrievalConfig().hybridAlpha()
        );
        
        return vectorStoreService.search(searchRequest);
    }
    
    /**
     * Re-rank documents using cross-encoder
     */
    private Uni<List<RetrievedDocument>> rerankDocuments(
            RagRequest request,
            List<RetrievedDocument> documents) {
        
        if (!request.retrievalConfig().enableReranking() || documents.isEmpty()) {
            return Uni.createFrom().item(documents);
        }
        
        return rerankingService.rerank(
            request.query(),
            documents,
            request.retrievalConfig().rerankingModel(),
            request.retrievalConfig().topK()
        );
    }
    
    /**
     * Generate answer using LLM
     */
    private Uni<String> generateAnswer(
            RagRequest request,
            AssembledContext context) {
        
        // Build prompt with context
        String prompt = buildPrompt(request, context);
        
        LlmRequest llmRequest = new LlmRequest(
            prompt,
            request.generationConfig().provider(),
            request.generationConfig().model(),
            request.generationConfig().temperature(),
            request.generationConfig().maxTokens(),
            request.generationConfig().systemPrompt(),
            request.generationConfig().enableStreaming(),
            request.tenantId(),
            request.metadata()
        );
        
        return llmService.generate(llmRequest);
    }
    
    /**
     * Build prompt with retrieved context
     */
    private String buildPrompt(RagRequest request, AssembledContext context) {
        String template = request.generationConfig().userPromptTemplate();
        
        // Replace placeholders
        String prompt = template
            .replace("{context}", context.formattedContext())
            .replace("{query}", request.query());
        
        // Add citation instructions if enabled
        if (request.generationConfig().enableCitations()) {
            prompt += "\n\nPlease cite your sources using the document IDs provided in the context.";
        }
        
        return prompt;
    }
    
    /**
     * Build execution metrics
     */
    private RagMetrics buildMetrics(
            Map<String, Long> timings,
            AssembledContext context,
            String answer) {
        
        long totalTime = timings.values().stream()
            .mapToLong(Long::longValue).sum();
        
        return new RagMetrics(
            totalTime,
            timings.getOrDefault("embedding", 0L),
            timings.getOrDefault("retrieval", 0L),
            timings.getOrDefault("reranking", 0L),
            timings.getOrDefault("generation", 0L),
            context.documents().size(),
            context.documents().size(), // All retrieved docs are used
            context.totalTokens(),
            estimateTokens(answer),
            calculateAverageScore(context.documents()),
            Map.of(
                "analysisMs", timings.getOrDefault("analysis", 0L)
            )
        );
    }
    
    /**
     * Parse RAG request from task context
     */
    private RagRequest parseRagRequest(NodeExecutionTask task) {
        Map<String, Object> context = task.context();
        
        String query = (String) context.get("query");
        if (query == null || query.isBlank()) {
            throw new IllegalArgumentException("Query is required");
        }
        
        // Parse RAG mode
        RagMode mode = context.containsKey("ragMode") ?
            RagMode.valueOf((String) context.get("ragMode")) :
            RagMode.STANDARD;
        
        // Parse search strategy
        SearchStrategy strategy = context.containsKey("searchStrategy") ?
            SearchStrategy.valueOf((String) context.get("searchStrategy")) :
            SearchStrategy.HYBRID;
        
        // Parse retrieval config
        @SuppressWarnings("unchecked")
        Map<String, Object> retrievalCfg = 
            (Map<String, Object>) context.getOrDefault("retrievalConfig", Map.of());
        RetrievalConfig retrievalConfig = parseRetrievalConfig(retrievalCfg);
        
        // Parse generation config
        @SuppressWarnings("unchecked")
        Map<String, Object> generationCfg = 
            (Map<String, Object>) context.getOrDefault("generationConfig", Map.of());
        GenerationConfig generationConfig = parseGenerationConfig(generationCfg);
        
        // Extract tenant ID from context
        String tenantId = (String) context.getOrDefault("tenantId", "default");
        String userId = (String) context.get("userId");
        String sessionId = (String) context.getOrDefault("sessionId", 
            UUID.randomUUID().toString());
        
        @SuppressWarnings("unchecked")
        List<String> collections = (List<String>) context.get("collections");
        
        @SuppressWarnings("unchecked")
        Map<String, Object> filters = 
            (Map<String, Object>) context.getOrDefault("filters", Map.of());
        
        @SuppressWarnings("unchecked")
        Map<String, Object> metadata = 
            (Map<String, Object>) context.getOrDefault("metadata", Map.of());
        
        return new RagRequest(
            query,
            mode,
            strategy,
            retrievalConfig,
            generationConfig,
            metadata,
            tenantId,
            userId,
            sessionId,
            collections,
            filters
        );
    }
    
    /**
     * Parse retrieval configuration
     */
    private RetrievalConfig parseRetrievalConfig(Map<String, Object> config) {
        if (config.isEmpty()) {
            return RetrievalConfig.defaults();
        }
        
        return new RetrievalConfig(
            (int) config.getOrDefault("topK", 5),
            ((Number) config.getOrDefault("minSimilarity", 0.7)).floatValue(),
            (int) config.getOrDefault("maxChunkSize", 512),
            (int) config.getOrDefault("chunkOverlap", 50),
            (boolean) config.getOrDefault("enableReranking", true),
            config.containsKey("rerankingModel") ?
                RerankingModel.valueOf((String) config.get("rerankingModel")) :
                RerankingModel.CROSS_ENCODER_MS_MARCO,
            (boolean) config.getOrDefault("enableHybridSearch", true),
            ((Number) config.getOrDefault("hybridAlpha", 0.5)).floatValue(),
            (boolean) config.getOrDefault("enableMultiQuery", false),
            (int) config.getOrDefault("numQueryVariations", 3),
            (boolean) config.getOrDefault("enableParentRetrieval", false),
            (int) config.getOrDefault("parentChunkSize", 2048),
            new HashMap<>(),
            List.of(),
            (boolean) config.getOrDefault("enableCitation", true),
            (boolean) config.getOrDefault("enableDeduplication", true)
        );
    }
    
    /**
     * Parse generation configuration
     */
    private GenerationConfig parseGenerationConfig(Map<String, Object> config) {
        if (config.isEmpty()) {
            return GenerationConfig.defaults();
        }
        
        return new GenerationConfig(
            (String) config.getOrDefault("provider", "openai"),
            (String) config.getOrDefault("model", "gpt-4-turbo-preview"),
            ((Number) config.getOrDefault("temperature", 0.7)).floatValue(),
            (int) config.getOrDefault("maxTokens", 1024),
            ((Number) config.getOrDefault("topP", 0.95)).floatValue(),
            ((Number) config.getOrDefault("frequencyPenalty", 0.0)).floatValue(),
            ((Number) config.getOrDefault("presencePenalty", 0.0)).floatValue(),
            List.of(),
            (String) config.get("systemPrompt"),
            (String) config.get("userPromptTemplate"),
            (boolean) config.getOrDefault("enableStreaming", false),
            (boolean) config.getOrDefault("enableCitations", true),
            config.containsKey("citationStyle") ?
                CitationStyle.valueOf((String) config.get("citationStyle")) :
                CitationStyle.INLINE_NUMBERED,
            (boolean) config.getOrDefault("enableFactChecking", false),
            (boolean) config.getOrDefault("enableGuardrails", true),
            new HashMap<>()
        );
    }
    
    /**
     * Validate RAG request
     */
    private void validateRequest(RagRequest request) {
        if (request.query().length() > 10000) {
            throw new IllegalArgumentException("Query too long (max 10000 chars)");
        }
        
        if (request.retrievalConfig().topK() > 100) {
            throw new IllegalArgumentException("topK too large (max 100)");
        }
    }
    
    /**
     * Get embedding model from config or default
     */
    private String getEmbeddingModel(RagRequest request) {
        return (String) request.metadata()
            .getOrDefault("embeddingModel", "text-embedding-3-small");
    }
    
    /**
     * Average multiple embeddings
     */
    private float[] averageEmbeddings(List<float[]> embeddings) {
        if (embeddings.isEmpty()) {
            return new float[0];
        }
        
        int dim = embeddings.get(0).length;
        float[] avg = new float[dim];
        
        for (float[] embedding : embeddings) {
            for (int i = 0; i < dim; i++) {
                avg[i] += embedding[i];
            }
        }
        
        for (int i = 0; i < dim; i++) {
            avg[i] /= embeddings.size();
        }
        
        return avg;
    }
    
    /**
     * Calculate average relevance score
     */
    private float calculateAverageScore(List<RetrievedDocument> documents) {
        if (documents.isEmpty()) return 0f;
        
        return (float) documents.stream()
            .mapToDouble(RetrievedDocument::getFinalScore)
            .average()
            .orElse(0.0);
    }
    
    /**
     * Estimate token count
     */
    private int estimateTokens(String text) {
        // Rough estimate: ~4 chars per token
        return text.length() / 4;
    }
    
    /**
     * Create success result
     */
    private NodeExecutionResult successResult(
            NodeExecutionTask task,
            RagResponse response) {
        
        Map<String, Object> output = new HashMap<>();
        output.put("answer", response.answer());
        output.put("sourceDocuments", response.sourceDocuments());
        output.put("citations", response.citations());
        output.put("metrics", response.metrics());
        output.put("sessionId", response.sessionId());
        output.put("timestamp", response.timestamp().toString());
        
        return new NodeExecutionResult(
            task.runId(),
            task.nodeId(),
            task.attempt(),
            NodeExecutionStatus.COMPLETED,
            output,
            null,
            task.token()
        );
    }
    
    /**
     * Create failure result
     */
    private NodeExecutionResult failureResult(
            NodeExecutionTask task,
            Exception error) {
        
        return new NodeExecutionResult(
            task.runId(),
            task.nodeId(),
            task.attempt(),
            NodeExecutionStatus.FAILED,
            Map.of(),
            new ErrorInfo(
                "RAG_EXECUTION_FAILED",
                error.getMessage(),
                getStackTrace(error),
                Map.of()
            ),
            task.token()
        );
    }
    
    private String getStackTrace(Exception e) {
        java.io.StringWriter sw = new java.io.StringWriter();
        e.printStackTrace(new java.io.PrintWriter(sw));
        return sw.toString();
    }
}

// ==================== SUPPORTING CLASSES ====================

/**
 * Assembled context from retrieved documents
 */
record AssembledContext(
    String formattedContext,
    List<RetrievedDocument> documents,
    int totalTokens,
    Map<String, Object> metadata
) {}

/**
 * Vector search request
 */
record VectorSearchRequest(
    float[] queryEmbedding,
    String queryText,
    int topK,
    float minSimilarity,
    String tenantId,
    List<String> collections,
    Map<String, Object> filters,
    SearchStrategy strategy,
    float hybridAlpha
) {}

/**
 * LLM generation request
 */
record LlmRequest(
    String prompt,
    String provider,
    String model,
    float temperature,
    int maxTokens,
    String systemPrompt,
    boolean streaming,
    String tenantId,
    Map<String, Object> metadata
) {}

package tech.kayys.silat.executor.rag;

import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tech.kayys.silat.executor.AbstractWorkflowExecutor;
import tech.kayys.silat.executor.Executor;
import tech.kayys.silat.executor.rag.domain.*;
import tech.kayys.silat.executor.rag.embedding.EmbeddingService;
import tech.kayys.silat.executor.rag.vectorstore.VectorStoreService;
import tech.kayys.silat.executor.rag.llm.LlmService;
import tech.kayys.silat.executor.rag.rerank.RerankingService;
import tech.kayys.silat.executor.rag.cache.RagCacheService;
import tech.kayys.silat.core.domain.*;
import tech.kayys.silat.core.engine.NodeExecutionResult;
import tech.kayys.silat.core.engine.NodeExecutionTask;
import tech.kayys.silat.core.scheduler.CommunicationType;

import java.time.Instant;
import java.util.*;
import java.util.stream.Collectors;

/**
 * ============================================================================
 * RAG WORKFLOW EXECUTOR
 * ============================================================================
 * 
 * Production-ready RAG executor for Silat workflow engine.
 * 
 * Pipeline stages:
 * 1. Query Analysis & Routing
 * 2. Embedding Generation
 * 3. Vector Search + Hybrid Search
 * 4. Re-ranking
 * 5. Context Assembly
 * 6. LLM Generation
 * 7. Citation Extraction
 * 
 * Features:
 * - Multi-tenant isolation
 * - Multiple vector store backends
 * - Hybrid search (vector + keyword)
 * - Cross-encoder re-ranking
 * - Citation tracking
 * - Response caching
 * - Streaming support
 */
@Executor(
    executorType = "rag-executor",
    communicationType = CommunicationType.GRPC,
    maxConcurrentTasks = 50,
    supportedNodeTypes = {"RAG_QUERY", "SEMANTIC_SEARCH", "QA_GENERATION"},
    version = "1.0.0"
)
@ApplicationScoped
public class RagWorkflowExecutor extends AbstractWorkflowExecutor {
    
    private static final Logger LOG = LoggerFactory.getLogger(RagWorkflowExecutor.class);
    
    @Inject
    EmbeddingService embeddingService;
    
    @Inject
    VectorStoreService vectorStoreService;
    
    @Inject
    LlmService llmService;
    
    @Inject
    RerankingService rerankingService;
    
    @Inject
    RagCacheService cacheService;
    
    @Inject
    QueryAnalyzer queryAnalyzer;
    
    @Inject
    ContextAssembler contextAssembler;
    
    @Inject
    CitationExtractor citationExtractor;
    
    @Override
    public Uni<NodeExecutionResult> execute(NodeExecutionTask task) {
        long startTime = System.currentTimeMillis();
        
        LOG.info("Executing RAG task: run={}, node={}", 
            task.runId().value(), task.nodeId().value());
        
        try {
            // Parse RAG request from task context
            RagRequest request = parseRagRequest(task);
            
            // Validate request
            validateRequest(request);
            
            // Check cache first
            return cacheService.get(request)
                .onItem().ifNotNull().transform(cached -> {
                    LOG.info("Cache hit for query: {}", request.query());
                    return successResult(task, cached);
                })
                .onItem().ifNull().switchTo(() -> 
                    // Cache miss - execute full RAG pipeline
                    executeRagPipeline(request, startTime)
                        .flatMap(response -> 
                            // Store in cache
                            cacheService.put(request, response)
                                .replaceWith(response)
                        )
                        .map(response -> successResult(task, response))
                );
                
        } catch (Exception e) {
            LOG.error("RAG execution failed", e);
            return Uni.createFrom().item(
                failureResult(task, e)
            );
        }
    }
    
    /**
     * Execute complete RAG pipeline
     */
    private Uni<RagResponse> executeRagPipeline(RagRequest request, long startTime) {
        LOG.debug("Starting RAG pipeline for query: {}", request.query());
        
        Instant timestamp = Instant.now();
        Map<String, Long> timings = new HashMap<>();
        
        // Stage 1: Query Analysis
        return analyzeQuery(request)
            .invoke(analysis -> {
                LOG.debug("Query analysis: complexity={}, intent={}", 
                    analysis.complexity(), analysis.intent());
                timings.put("analysis", System.currentTimeMillis() - startTime);
            })
            
            // Stage 2: Generate query embedding
            .flatMap(analysis -> generateQueryEmbedding(request, analysis)
                .invoke(v -> timings.put("embedding", 
                    System.currentTimeMillis() - startTime - timings.get("analysis")))
            )
            
            // Stage 3: Retrieve documents
            .flatMap(embedding -> retrieveDocuments(request, embedding)
                .invoke(docs -> {
                    LOG.debug("Retrieved {} documents", docs.size());
                    timings.put("retrieval", 
                        System.currentTimeMillis() - startTime - 
                        timings.get("analysis") - timings.get("embedding"));
                })
            )
            
            // Stage 4: Re-rank documents (if enabled)
            .flatMap(documents -> rerankDocuments(request, documents)
                .invoke(reranked -> {
                    LOG.debug("Re-ranked to {} documents", reranked.size());
                    timings.put("reranking", 
                        System.currentTimeMillis() - startTime - 
                        timings.values().stream().mapToLong(Long::longValue).sum());
                })
            )
            
            // Stage 5: Assemble context
            .map(documents -> contextAssembler.assemble(
                request, documents))
            
            // Stage 6: Generate answer
            .flatMap(context -> generateAnswer(request, context)
                .invoke(answer -> {
                    LOG.debug("Generated answer: {} chars", answer.length());
                    timings.put("generation", 
                        System.currentTimeMillis() - startTime - 
                        timings.values().stream().mapToLong(Long::longValue).sum());
                })
                .map(answer -> Map.entry(context, answer))
            )
            
            // Stage 7: Extract citations
            .map(entry -> {
                AssembledContext context = entry.getKey();
                String answer = entry.getValue();
                
                List<Citation> citations = request.generationConfig().enableCitations() ?
                    citationExtractor.extract(answer, context.documents()) :
                    List.of();
                
                // Build metrics
                RagMetrics metrics = buildMetrics(timings, context, answer);
                
                return new RagResponse(
                    request.query(),
                    answer,
                    context.documents(),
                    citations,
                    metrics,
                    request.sessionId(),
                    timestamp,
                    request.metadata(),
                    List.of(),
                    Optional.empty()
                );
            })
            
            .onFailure().recoverWithItem(error -> {
                LOG.error("RAG pipeline failed", error);
                return new RagResponse(
                    request.query(),
                    "",
                    List.of(),
                    List.of(),
                    null,
                    request.sessionId(),
                    timestamp,
                    request.metadata(),
                    List.of(),
                    Optional.of(error.getMessage())
                );
            });
    }
    
    /**
     * Analyze query to determine routing and strategy
     */
    private Uni<QueryAnalysis> analyzeQuery(RagRequest request) {
        return queryAnalyzer.analyze(request.query(), request.tenantId());
    }
    
    /**
     * Generate embedding for query
     */
    private Uni<float[]> generateQueryEmbedding(
            RagRequest request, 
            QueryAnalysis analysis) {
        
        // Handle multi-query strategy
        if (request.searchStrategy() == SearchStrategy.MULTI_QUERY &&
            request.retrievalConfig().enableMultiQuery()) {
            
            return queryAnalyzer.generateQueryVariations(
                request.query(), 
                request.retrievalConfig().numQueryVariations()
            )
            .flatMap(variations -> {
                // Generate embeddings for all variations
                EmbeddingRequest embReq = new EmbeddingRequest(
                    variations,
                    getEmbeddingModel(request),
                    request.tenantId(),
                    Map.of()
                );
                
                return embeddingService.generateEmbeddings(embReq)
                    .map(response -> 
                        // Average embeddings
                        averageEmbeddings(response.embeddings())
                    );
            });
        }
        
        // Standard single query embedding
        EmbeddingRequest embReq = new EmbeddingRequest(
            List.of(request.query()),
            getEmbeddingModel(request),
            request.tenantId(),
            Map.of()
        );
        
        return embeddingService.generateEmbeddings(embReq)
            .map(response -> response.embeddings().get(0));
    }
    
    /**
     * Retrieve documents from vector store
     */
    private Uni<List<RetrievedDocument>> retrieveDocuments(
            RagRequest request,
            float[] queryEmbedding) {
        
        VectorSearchRequest searchRequest = new VectorSearchRequest(
            queryEmbedding,
            request.query(),
            request.retrievalConfig().topK(),
            request.retrievalConfig().minSimilarity(),
            request.tenantId(),
            request.retrievalConfig().collections(),
            request.filters(),
            request.searchStrategy(),
            request.retrievalConfig().hybridAlpha()
        );
        
        return vectorStoreService.search(searchRequest);
    }
    
    /**
     * Re-rank documents using cross-encoder
     */
    private Uni<List<RetrievedDocument>> rerankDocuments(
            RagRequest request,
            List<RetrievedDocument> documents) {
        
        if (!request.retrievalConfig().enableReranking() || documents.isEmpty()) {
            return Uni.createFrom().item(documents);
        }
        
        return rerankingService.rerank(
            request.query(),
            documents,
            request.retrievalConfig().rerankingModel(),
            request.retrievalConfig().topK()
        );
    }
    
    /**
     * Generate answer using LLM
     */
    private Uni<String> generateAnswer(
            RagRequest request,
            AssembledContext context) {
        
        // Build prompt with context
        String prompt = buildPrompt(request, context);
        
        LlmRequest llmRequest = new LlmRequest(
            prompt,
            request.generationConfig().provider(),
            request.generationConfig().model(),
            request.generationConfig().temperature(),
            request.generationConfig().maxTokens(),
            request.generationConfig().systemPrompt(),
            request.generationConfig().enableStreaming(),
            request.tenantId(),
            request.metadata()
        );
        
        return llmService.generate(llmRequest);
    }
    
    /**
     * Build prompt with retrieved context
     */
    private String buildPrompt(RagRequest request, AssembledContext context) {
        String template = request.generationConfig().userPromptTemplate();
        
        // Replace placeholders
        String prompt = template
            .replace("{context}", context.formattedContext())
            .replace("{query}", request.query());
        
        // Add citation instructions if enabled
        if (request.generationConfig().enableCitations()) {
            prompt += "\n\nPlease cite your sources using the document IDs provided in the context.";
        }
        
        return prompt;
    }
    
    /**
     * Build execution metrics
     */
    private RagMetrics buildMetrics(
            Map<String, Long> timings,
            AssembledContext context,
            String answer) {
        
        long totalTime = timings.values().stream()
            .mapToLong(Long::longValue).sum();
        
        return new RagMetrics(
            totalTime,
            timings.getOrDefault("embedding", 0L),
            timings.getOrDefault("retrieval", 0L),
            timings.getOrDefault("reranking", 0L),
            timings.getOrDefault("generation", 0L),
            context.documents().size(),
            context.documents().size(), // All retrieved docs are used
            context.totalTokens(),
            estimateTokens(answer),
            calculateAverageScore(context.documents()),
            Map.of(
                "analysisMs", timings.getOrDefault("analysis", 0L)
            )
        );
    }
    
    /**
     * Parse RAG request from task context
     */
    private RagRequest parseRagRequest(NodeExecutionTask task) {
        Map<String, Object> context = task.context();
        
        String query = (String) context.get("query");
        if (query == null || query.isBlank()) {
            throw new IllegalArgumentException("Query is required");
        }
        
        // Parse RAG mode
        RagMode mode = context.containsKey("ragMode") ?
            RagMode.valueOf((String) context.get("ragMode")) :
            RagMode.STANDARD;
        
        // Parse search strategy
        SearchStrategy strategy = context.containsKey("searchStrategy") ?
            SearchStrategy.valueOf((String) context.get("searchStrategy")) :
            SearchStrategy.HYBRID;
        
        // Parse retrieval config
        @SuppressWarnings("unchecked")
        Map<String, Object> retrievalCfg = 
            (Map<String, Object>) context.getOrDefault("retrievalConfig", Map.of());
        RetrievalConfig retrievalConfig = parseRetrievalConfig(retrievalCfg);
        
        // Parse generation config
        @SuppressWarnings("unchecked")
        Map<String, Object> generationCfg = 
            (Map<String, Object>) context.getOrDefault("generationConfig", Map.of());
        GenerationConfig generationConfig = parseGenerationConfig(generationCfg);
        
        // Extract tenant ID from context
        String tenantId = (String) context.getOrDefault("tenantId", "default");
        String userId = (String) context.get("userId");
        String sessionId = (String) context.getOrDefault("sessionId", 
            UUID.randomUUID().toString());
        
        @SuppressWarnings("unchecked")
        List<String> collections = (List<String>) context.get("collections");
        
        @SuppressWarnings("unchecked")
        Map<String, Object> filters = 
            (Map<String, Object>) context.getOrDefault("filters", Map.of());
        
        @SuppressWarnings("unchecked")
        Map<String, Object> metadata = 
            (Map<String, Object>) context.getOrDefault("metadata", Map.of());
        
        return new RagRequest(
            query,
            mode,
            strategy,
            retrievalConfig,
            generationConfig,
            metadata,
            tenantId,
            userId,
            sessionId,
            collections,
            filters
        );
    }
    
    /**
     * Parse retrieval configuration
     */
    private RetrievalConfig parseRetrievalConfig(Map<String, Object> config) {
        if (config.isEmpty()) {
            return RetrievalConfig.defaults();
        }
        
        return new RetrievalConfig(
            (int) config.getOrDefault("topK", 5),
            ((Number) config.getOrDefault("minSimilarity", 0.7)).floatValue(),
            (int) config.getOrDefault("maxChunkSize", 512),
            (int) config.getOrDefault("chunkOverlap", 50),
            (boolean) config.getOrDefault("enableReranking", true),
            config.containsKey("rerankingModel") ?
                RerankingModel.valueOf((String) config.get("rerankingModel")) :
                RerankingModel.CROSS_ENCODER_MS_MARCO,
            (boolean) config.getOrDefault("enableHybridSearch", true),
            ((Number) config.getOrDefault("hybridAlpha", 0.5)).floatValue(),
            (boolean) config.getOrDefault("enableMultiQuery", false),
            (int) config.getOrDefault("numQueryVariations", 3),
            (boolean) config.getOrDefault("enableParentRetrieval", false),
            (int) config.getOrDefault("parentChunkSize", 2048),
            new HashMap<>(),
            List.of(),
            (boolean) config.getOrDefault("enableCitation", true),
            (boolean) config.getOrDefault("enableDeduplication", true)
        );
    }
    
    /**
     * Parse generation configuration
     */
    private GenerationConfig parseGenerationConfig(Map<String, Object> config) {
        if (config.isEmpty()) {
            return GenerationConfig.defaults();
        }
        
        return new GenerationConfig(
            (String) config.getOrDefault("provider", "openai"),
            (String) config.getOrDefault("model", "gpt-4-turbo-preview"),
            ((Number) config.getOrDefault("temperature", 0.7)).floatValue(),
            (int) config.getOrDefault("maxTokens", 1024),
            ((Number) config.getOrDefault("topP", 0.95)).floatValue(),
            ((Number) config.getOrDefault("frequencyPenalty", 0.0)).floatValue(),
            ((Number) config.getOrDefault("presencePenalty", 0.0)).floatValue(),
            List.of(),
            (String) config.get("systemPrompt"),
            (String) config.get("userPromptTemplate"),
            (boolean) config.getOrDefault("enableStreaming", false),
            (boolean) config.getOrDefault("enableCitations", true),
            config.containsKey("citationStyle") ?
                CitationStyle.valueOf((String) config.get("citationStyle")) :
                CitationStyle.INLINE_NUMBERED,
            (boolean) config.getOrDefault("enableFactChecking", false),
            (boolean) config.getOrDefault("enableGuardrails", true),
            new HashMap<>()
        );
    }
    
    /**
     * Validate RAG request
     */
    private void validateRequest(RagRequest request) {
        if (request.query().length() > 10000) {
            throw new IllegalArgumentException("Query too long (max 10000 chars)");
        }
        
        if (request.retrievalConfig().topK() > 100) {
            throw new IllegalArgumentException("topK too large (max 100)");
        }
    }
    
    /**
     * Get embedding model from config or default
     */
    private String getEmbeddingModel(RagRequest request) {
        return (String) request.metadata()
            .getOrDefault("embeddingModel", "text-embedding-3-small");
    }
    
    /**
     * Average multiple embeddings
     */
    private float[] averageEmbeddings(List<float[]> embeddings) {
        if (embeddings.isEmpty()) {
            return new float[0];
        }
        
        int dim = embeddings.get(0).length;
        float[] avg = new float[dim];
        
        for (float[] embedding : embeddings) {
            for (int i = 0; i < dim; i++) {
                avg[i] += embedding[i];
            }
        }
        
        for (int i = 0; i < dim; i++) {
            avg[i] /= embeddings.size();
        }
        
        return avg;
    }
    
    /**
     * Calculate average relevance score
     */
    private float calculateAverageScore(List<RetrievedDocument> documents) {
        if (documents.isEmpty()) return 0f;
        
        return (float) documents.stream()
            .mapToDouble(RetrievedDocument::getFinalScore)
            .average()
            .orElse(0.0);
    }
    
    /**
     * Estimate token count
     */
    private int estimateTokens(String text) {
        // Rough estimate: ~4 chars per token
        return text.length() / 4;
    }
    
    /**
     * Create success result
     */
    private NodeExecutionResult successResult(
            NodeExecutionTask task,
            RagResponse response) {
        
        Map<String, Object> output = new HashMap<>();
        output.put("answer", response.answer());
        output.put("sourceDocuments", response.sourceDocuments());
        output.put("citations", response.citations());
        output.put("metrics", response.metrics());
        output.put("sessionId", response.sessionId());
        output.put("timestamp", response.timestamp().toString());
        
        return new NodeExecutionResult(
            task.runId(),
            task.nodeId(),
            task.attempt(),
            NodeExecutionStatus.COMPLETED,
            output,
            null,
            task.token()
        );
    }
    
    /**
     * Create failure result
     */
    private NodeExecutionResult failureResult(
            NodeExecutionTask task,
            Exception error) {
        
        return new NodeExecutionResult(
            task.runId(),
            task.nodeId(),
            task.attempt(),
            NodeExecutionStatus.FAILED,
            Map.of(),
            new ErrorInfo(
                "RAG_EXECUTION_FAILED",
                error.getMessage(),
                getStackTrace(error),
                Map.of()
            ),
            task.token()
        );
    }
    
    private String getStackTrace(Exception e) {
        java.io.StringWriter sw = new java.io.StringWriter();
        e.printStackTrace(new java.io.PrintWriter(sw));
        return sw.toString();
    }
}

// ==================== SUPPORTING CLASSES ====================

/**
 * Assembled context from retrieved documents
 */
record AssembledContext(
    String formattedContext,
    List<RetrievedDocument> documents,
    int totalTokens,
    Map<String, Object> metadata
) {}

/**
 * Vector search request
 */
record VectorSearchRequest(
    float[] queryEmbedding,
    String queryText,
    int topK,
    float minSimilarity,
    String tenantId,
    List<String> collections,
    Map<String, Object> filters,
    SearchStrategy strategy,
    float hybridAlpha
) {}

/**
 * LLM generation request
 */
record LlmRequest(
    String prompt,
    String provider,
    String model,
    float temperature,
    int maxTokens,
    String systemPrompt,
    boolean streaming,
    String tenantId,
    Map<String, Object> metadata
) {}

