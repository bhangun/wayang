// ============================================================================
// pom.xml
// ============================================================================
/*
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    
    <groupId>com.example</groupId>
    <artifactId>llama-inference-server</artifactId>
    <version>1.0.0</version>
    
    
</project>
*/

// ============================================================================
// src/main/resources/application.yml
// ============================================================================
/*
llama:
  library-path: "/usr/local/lib/libllama.so"
  model-path: "/path/to/models/llama-2-7b-chat.gguf"
  context-size: 4096
  threads: 8
  batch-size: 512
  gpu-layers: 32
  rope-freq-base: 10000.0
  rope-freq-scale: 1.0
  seed: -1
  
  sampling:
    temperature: 0.8
    top-k: 40
    top-p: 0.95
    min-p: 0.05
    repeat-penalty: 1.1
    repeat-last-n: 64
    presence-penalty: 0.0
    frequency-penalty: 0.0

quarkus:
  http:
    port: 8080
    cors:
      ~: true
      origins: "*"
  log:
    level: INFO
    category:
      "com.example.llama":
        level: DEBUG
*/

// ============================================================================
// src/main/java/com/example/llama/config/LlamaConfig.java
// ============================================================================
package com.example.llama.config;

import io.smallrye.config.ConfigMapping;
import io.smallrye.config.WithDefault;

@ConfigMapping(prefix = "llama")
public interface LlamaConfig {
    String libraryPath();
    String modelPath();
    
    @WithDefault("4096")
    int contextSize();
    
    @WithDefault("8")
    int threads();
    
    @WithDefault("512")
    int batchSize();
    
    @WithDefault("0")
    int gpuLayers();
    
    @WithDefault("10000.0")
    float ropeFreqBase();
    
    @WithDefault("1.0")
    float ropeFreqScale();
    
    @WithDefault("-1")
    int seed();
    
    SamplingConfig sampling();
    
    interface SamplingConfig {
        @WithDefault("0.8")
        float temperature();
        
        @WithDefault("40")
        int topK();
        
        @WithDefault("0.95")
        float topP();
        
        @WithDefault("0.05")
        float minP();
        
        @WithDefault("1.1")
        float repeatPenalty();
        
        @WithDefault("64")
        int repeatLastN();
        
        @WithDefault("0.0")
        float presencePenalty();
        
        @WithDefault("0.0")
        float frequencyPenalty();
    }
}

// ============================================================================
// src/main/java/com/example/llama/model/ChatMessage.java
// ============================================================================
package com.example.llama.model;

import com.fasterxml.jackson.annotation.JsonProperty;

public record ChatMessage(
    String role,  // "system", "user", "assistant"
    String content
) {
    public ChatMessage {
        if (role == null || role.isBlank()) {
            throw new IllegalArgumentException("Role cannot be null or blank");
        }
        if (content == null) {
            content = "";
        }
    }
}

// ============================================================================
// src/main/java/com/example/llama/model/ChatRequest.java
// ============================================================================
package com.example.llama.model;

import com.fasterxml.jackson.annotation.JsonProperty;
import java.util.List;

public record ChatRequest(
    List<ChatMessage> messages,
    @JsonProperty("max_tokens") Integer maxTokens,
    Float temperature,
    @JsonProperty("top_p") Float topP,
    @JsonProperty("top_k") Integer topK,
    @JsonProperty("min_p") Float minP,
    @JsonProperty("repeat_penalty") Float repeatPenalty,
    @JsonProperty("presence_penalty") Float presencePenalty,
    @JsonProperty("frequency_penalty") Float frequencyPenalty,
    Boolean stream,
    List<String> stop
) {
    public ChatRequest {
        if (messages == null || messages.isEmpty()) {
            throw new IllegalArgumentException("Messages cannot be null or empty");
        }
        if (maxTokens == null) maxTokens = 512;
        if (stream == null) stream = false;
    }
}

// ============================================================================
// src/main/java/com/example/llama/model/ChatResponse.java
// ============================================================================
package com.example.llama.model;

import com.fasterxml.jackson.annotation.JsonProperty;

public record ChatResponse(
    String id,
    String object,
    long created,
    String model,
    Choice choice,
    Usage usage
) {
    public record Choice(
        int index,
        ChatMessage message,
        @JsonProperty("finish_reason") String finishReason
    ) {}
    
    public record Usage(
        @JsonProperty("prompt_tokens") int promptTokens,
        @JsonProperty("completion_tokens") int completionTokens,
        @JsonProperty("total_tokens") int totalTokens
    ) {}
}

// ============================================================================
// src/main/java/com/example/llama/model/StreamChunk.java
// ============================================================================
package com.example.llama.model;

import com.fasterxml.jackson.annotation.JsonProperty;

public record StreamChunk(
    String id,
    String object,
    long created,
    String model,
    Delta delta,
    @JsonProperty("finish_reason") String finishReason
) {
    public record Delta(
        String role,
        String content
    ) {}
}

// ============================================================================
// src/main/java/com/example/llama/llama/LlamaStructs.java
// ============================================================================
package com.example.llama.llama;

import java.lang.foreign.*;

public class LlamaStructs {
    
    // llama_model_params struct layout
    public static final StructLayout MODEL_PARAMS_LAYOUT = MemoryLayout.structLayout(
        ValueLayout.JAVA_INT.withName("n_gpu_layers"),
        ValueLayout.JAVA_INT.withName("split_mode"),
        ValueLayout.JAVA_INT.withName("main_gpu"),
        MemoryLayout.paddingLayout(4),
        ValueLayout.ADDRESS.withName("tensor_split"),
        ValueLayout.JAVA_BOOLEAN.withName("vocab_only"),
        ValueLayout.JAVA_BOOLEAN.withName("use_mmap"),
        ValueLayout.JAVA_BOOLEAN.withName("use_mlock"),
        MemoryLayout.paddingLayout(5)
    );
    
    // llama_context_params struct layout
    public static final StructLayout CONTEXT_PARAMS_LAYOUT = MemoryLayout.structLayout(
        ValueLayout.JAVA_INT.withName("seed"),
        ValueLayout.JAVA_INT.withName("n_ctx"),
        ValueLayout.JAVA_INT.withName("n_batch"),
        ValueLayout.JAVA_INT.withName("n_ubatch"),
        ValueLayout.JAVA_INT.withName("n_seq_max"),
        ValueLayout.JAVA_INT.withName("n_threads"),
        ValueLayout.JAVA_INT.withName("n_threads_batch"),
        ValueLayout.JAVA_INT.withName("rope_scaling_type"),
        ValueLayout.JAVA_INT.withName("pooling_type"),
        ValueLayout.JAVA_INT.withName("attention_type"),
        ValueLayout.JAVA_FLOAT.withName("rope_freq_base"),
        ValueLayout.JAVA_FLOAT.withName("rope_freq_scale"),
        ValueLayout.JAVA_FLOAT.withName("yarn_ext_factor"),
        ValueLayout.JAVA_FLOAT.withName("yarn_attn_factor"),
        ValueLayout.JAVA_FLOAT.withName("yarn_beta_fast"),
        ValueLayout.JAVA_FLOAT.withName("yarn_beta_slow"),
        ValueLayout.JAVA_INT.withName("yarn_orig_ctx"),
        ValueLayout.JAVA_FLOAT.withName("defrag_thold"),
        MemoryLayout.paddingLayout(4),
        ValueLayout.ADDRESS.withName("cb_eval"),
        ValueLayout.ADDRESS.withName("cb_eval_user_data"),
        ValueLayout.JAVA_INT.withName("type_k"),
        ValueLayout.JAVA_INT.withName("type_v"),
        ValueLayout.JAVA_BOOLEAN.withName("logits_all"),
        ValueLayout.JAVA_BOOLEAN.withName("embeddings"),
        ValueLayout.JAVA_BOOLEAN.withName("offload_kqv"),
        ValueLayout.JAVA_BOOLEAN.withName("flash_attn"),
        MemoryLayout.paddingLayout(4),
        ValueLayout.ADDRESS.withName("abort_callback"),
        ValueLayout.ADDRESS.withName("abort_callback_data")
    );
    
    // llama_batch struct
    public static final StructLayout BATCH_LAYOUT = MemoryLayout.structLayout(
        ValueLayout.JAVA_INT.withName("n_tokens"),
        MemoryLayout.paddingLayout(4),
        ValueLayout.ADDRESS.withName("token"),
        ValueLayout.ADDRESS.withName("embd"),
        ValueLayout.ADDRESS.withName("pos"),
        ValueLayout.ADDRESS.withName("n_seq_id"),
        ValueLayout.ADDRESS.withName("seq_id"),
        ValueLayout.ADDRESS.withName("logits")
    );
    
    public static MemorySegment createBatch(Arena arena, int[] tokens, int[] positions, boolean[] logits) {
        MemorySegment batch = arena.allocate(BATCH_LAYOUT);
        int nTokens = tokens.length;
        
        batch.set(ValueLayout.JAVA_INT, 0, nTokens);
        
        MemorySegment tokenSegment = arena.allocate(ValueLayout.JAVA_INT, nTokens);
        MemorySegment posSegment = arena.allocate(ValueLayout.JAVA_INT, nTokens);
        MemorySegment logitsSegment = arena.allocate(ValueLayout.JAVA_BYTE, nTokens);
        
        for (int i = 0; i < nTokens; i++) {
            tokenSegment.setAtIndex(ValueLayout.JAVA_INT, i, tokens[i]);
            posSegment.setAtIndex(ValueLayout.JAVA_INT, i, positions[i]);
            logitsSegment.setAtIndex(ValueLayout.JAVA_BYTE, i, (byte) (logits[i] ? 1 : 0));
        }
        
        batch.set(ValueLayout.ADDRESS, 8, tokenSegment);
        batch.set(ValueLayout.ADDRESS, 16, MemorySegment.NULL); // embd
        batch.set(ValueLayout.ADDRESS, 24, posSegment);
        batch.set(ValueLayout.ADDRESS, 32, MemorySegment.NULL); // n_seq_id
        batch.set(ValueLayout.ADDRESS, 40, MemorySegment.NULL); // seq_id
        batch.set(ValueLayout.ADDRESS, 48, logitsSegment);
        
        return batch;
    }
}

// ============================================================================
// src/main/java/com/example/llama/llama/LlamaCppBinding.java
// ============================================================================
package com.example.llama.llama;

import java.lang.foreign.*;
import java.lang.invoke.MethodHandle;
import java.nio.charset.StandardCharsets;

public class LlamaCppBinding {
    private final SymbolLookup lookup;
    
    // Core functions
    private final MethodHandle llamaBackendInit;
    private final MethodHandle llamaBackendFree;
    private final MethodHandle llamaModelDefaultParams;
    private final MethodHandle llamaLoadModelFromFile;
    private final MethodHandle llamaFreeModel;
    private final MethodHandle llamaContextDefaultParams;
    private final MethodHandle llamaNewContextWithModel;
    private final MethodHandle llamaFreeContext;
    private final MethodHandle llamaNVocab;
    private final MethodHandle llamaNCtxTrain;
    private final MethodHandle llamaModelDesc;
    
    // Tokenization
    private final MethodHandle llamaTokenize;
    private final MethodHandle llamaTokenToString;
    private final MethodHandle llamaTokenGetText;
    private final MethodHandle llamaTokenBos;
    private final MethodHandle llamaTokenEos;
    private final MethodHandle llamaTokenNl;
    
    // Batch and inference
    private final MethodHandle llamaBatchInit;
    private final MethodHandle llamaBatchFree;
    private final MethodHandle llamaDecode;
    private final MethodHandle llamaGetLogits;
    private final MethodHandle llamaGetLogitsIth;
    
    // Sampling
    private final MethodHandle llamaSamplingInit;
    private final MethodHandle llamaSamplingFree;
    private final MethodHandle llamaSamplingSetRngSeed;
    private final MethodHandle llamaSamplingAccept;
    private final MethodHandle llamaSamplingSample;
    
    // KV cache
    private final MethodHandle llamaKvCacheClear;
    private final MethodHandle llamaKvCacheSeqRm;
    private final MethodHandle llamaKvCacheSeqCp;
    private final MethodHandle llamaKvCacheSeqKeep;
    private final MethodHandle llamaKvCacheSeqAdd;
    
    public LlamaCppBinding(String libraryPath) {
        System.load(libraryPath);
        this.lookup = SymbolLookup.loaderLookup();
        
        try {
            var linker = Linker.nativeLinker();
            
            // Initialize core functions
            this.llamaBackendInit = findFunction(linker, "llama_backend_init", 
                FunctionDescriptor.ofVoid());
            
            this.llamaBackendFree = findFunction(linker, "llama_backend_free",
                FunctionDescriptor.ofVoid());
            
            this.llamaModelDefaultParams = findFunction(linker, "llama_model_default_params",
                FunctionDescriptor.of(LlamaStructs.MODEL_PARAMS_LAYOUT));
            
            this.llamaLoadModelFromFile = findFunction(linker, "llama_load_model_from_file",
                FunctionDescriptor.of(ValueLayout.ADDRESS, ValueLayout.ADDRESS, LlamaStructs.MODEL_PARAMS_LAYOUT));
            
            this.llamaFreeModel = findFunction(linker, "llama_free_model",
                FunctionDescriptor.ofVoid(ValueLayout.ADDRESS));
            
            this.llamaContextDefaultParams = findFunction(linker, "llama_context_default_params",
                FunctionDescriptor.of(LlamaStructs.CONTEXT_PARAMS_LAYOUT));
            
            this.llamaNewContextWithModel = findFunction(linker, "llama_new_context_with_model",
                FunctionDescriptor.of(ValueLayout.ADDRESS, ValueLayout.ADDRESS, LlamaStructs.CONTEXT_PARAMS_LAYOUT));
            
            this.llamaFreeContext = findFunction(linker, "llama_free",
                FunctionDescriptor.ofVoid(ValueLayout.ADDRESS));
            
            this.llamaNVocab = findFunction(linker, "llama_n_vocab",
                FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));
            
            this.llamaNCtxTrain = findFunction(linker, "llama_n_ctx_train",
                FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));
            
            this.llamaModelDesc = findFunction(linker, "llama_model_desc",
                FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS, ValueLayout.ADDRESS, ValueLayout.JAVA_LONG));
            
            // Tokenization
            this.llamaTokenize = findFunction(linker, "llama_tokenize",
                FunctionDescriptor.of(ValueLayout.JAVA_INT,
                    ValueLayout.ADDRESS, ValueLayout.ADDRESS, ValueLayout.JAVA_INT,
                    ValueLayout.ADDRESS, ValueLayout.JAVA_INT, 
                    ValueLayout.JAVA_BOOLEAN, ValueLayout.JAVA_BOOLEAN));
            
            this.llamaTokenToString = findFunction(linker, "llama_token_to_piece",
                FunctionDescriptor.of(ValueLayout.JAVA_INT,
                    ValueLayout.ADDRESS, ValueLayout.JAVA_INT,
                    ValueLayout.ADDRESS, ValueLayout.JAVA_INT, ValueLayout.JAVA_INT,
                    ValueLayout.JAVA_BOOLEAN));
            
            this.llamaTokenGetText = findFunction(linker, "llama_token_get_text",
                FunctionDescriptor.of(ValueLayout.ADDRESS, ValueLayout.ADDRESS, ValueLayout.JAVA_INT));
            
            this.llamaTokenBos = findFunction(linker, "llama_token_bos",
                FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));
            
            this.llamaTokenEos = findFunction(linker, "llama_token_eos",
                FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));
            
            this.llamaTokenNl = findFunction(linker, "llama_token_nl",
                FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));
            
            // Batch and inference
            this.llamaBatchInit = findFunction(linker, "llama_batch_init",
                FunctionDescriptor.of(LlamaStructs.BATCH_LAYOUT, ValueLayout.JAVA_INT, ValueLayout.JAVA_INT, ValueLayout.JAVA_INT));
            
            this.llamaBatchFree = findFunction(linker, "llama_batch_free",
                FunctionDescriptor.ofVoid(LlamaStructs.BATCH_LAYOUT));
            
            this.llamaDecode = findFunction(linker, "llama_decode",
                FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS, LlamaStructs.BATCH_LAYOUT));
            
            this.llamaGetLogits = findFunction(linker, "llama_get_logits",
                FunctionDescriptor.of(ValueLayout.ADDRESS, ValueLayout.ADDRESS));
            
            this.llamaGetLogitsIth = findFunction(linker, "llama_get_logits_ith",
                FunctionDescriptor.of(ValueLayout.ADDRESS, ValueLayout.ADDRESS, ValueLayout.JAVA_INT));
            
            // Sampling - using simplified greedy approach
            this.llamaSamplingInit = findFunction(linker, "llama_sampler_chain_init",
                FunctionDescriptor.of(ValueLayout.ADDRESS, LlamaStructs.CONTEXT_PARAMS_LAYOUT));
            
            this.llamaSamplingFree = findFunction(linker, "llama_sampler_free",
                FunctionDescriptor.ofVoid(ValueLayout.ADDRESS));
            
            this.llamaSamplingSetRngSeed = findFunction(linker, "llama_sampler_chain_add",
                FunctionDescriptor.ofVoid(ValueLayout.ADDRESS, ValueLayout.ADDRESS));
            
            this.llamaSamplingAccept = findFunction(linker, "llama_sampler_accept",
                FunctionDescriptor.ofVoid(ValueLayout.ADDRESS, ValueLayout.JAVA_INT));
            
            this.llamaSamplingSample = findFunction(linker, "llama_sampler_sample",
                FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS, ValueLayout.ADDRESS, ValueLayout.JAVA_INT));
            
            // KV cache
            this.llamaKvCacheClear = findFunction(linker, "llama_kv_cache_clear",
                FunctionDescriptor.ofVoid(ValueLayout.ADDRESS));
            
            this.llamaKvCacheSeqRm = findFunction(linker, "llama_kv_cache_seq_rm",
                FunctionDescriptor.of(ValueLayout.JAVA_BOOLEAN, ValueLayout.ADDRESS, 
                    ValueLayout.JAVA_INT, ValueLayout.JAVA_INT, ValueLayout.JAVA_INT));
            
            this.llamaKvCacheSeqCp = findFunction(linker, "llama_kv_cache_seq_cp",
                FunctionDescriptor.ofVoid(ValueLayout.ADDRESS, 
                    ValueLayout.JAVA_INT, ValueLayout.JAVA_INT, ValueLayout.JAVA_INT, ValueLayout.JAVA_INT));
            
            this.llamaKvCacheSeqKeep = findFunction(linker, "llama_kv_cache_seq_keep",
                FunctionDescriptor.ofVoid(ValueLayout.ADDRESS, ValueLayout.JAVA_INT));
            
            this.llamaKvCacheSeqAdd = findFunction(linker, "llama_kv_cache_seq_add",
                FunctionDescriptor.ofVoid(ValueLayout.ADDRESS, 
                    ValueLayout.JAVA_INT, ValueLayout.JAVA_INT, ValueLayout.JAVA_INT, ValueLayout.JAVA_INT));
            
        } catch (Throwable e) {
            throw new RuntimeException("Failed to initialize llama.cpp bindings", e);
        }
    }
    
    private MethodHandle findFunction(Linker linker, String name, FunctionDescriptor descriptor) {
        return lookup.find(name)
            .map(addr -> linker.downcallHandle(addr, descriptor))
            .orElseThrow(() -> new RuntimeException("Function not found: " + name));
    }
    
    // Core API
    public void backendInit() throws Throwable {
        llamaBackendInit.invoke();
    }
    
    public void backendFree() throws Throwable {
        llamaBackendFree.invoke();
    }
    
    public MemorySegment getModelDefaultParams(Arena arena) throws Throwable {
        MemorySegment params = arena.allocate(LlamaStructs.MODEL_PARAMS_LAYOUT);
        MemorySegment defaultParams = (MemorySegment) llamaModelDefaultParams.invoke();
        MemorySegment.copy(defaultParams, 0, params, 0, LlamaStructs.MODEL_PARAMS_LAYOUT.byteSize());
        return params;
    }
    
    public MemorySegment loadModel(Arena arena, String path, int gpuLayers) throws Throwable {
        MemorySegment params = getModelDefaultParams(arena);
        params.set(ValueLayout.JAVA_INT, 0, gpuLayers);
        
        MemorySegment pathSeg = arena.allocateFrom(path, StandardCharsets.UTF_8);
        return (MemorySegment) llamaLoadModelFromFile.invoke(pathSeg, params);
    }
    
    public void freeModel(MemorySegment model) throws Throwable {
        llamaFreeModel.invoke(model);
    }
    
    public MemorySegment getContextDefaultParams(Arena arena) throws Throwable {
        MemorySegment params = arena.allocate(LlamaStructs.CONTEXT_PARAMS_LAYOUT);
        MemorySegment defaultParams = (MemorySegment) llamaContextDefaultParams.invoke();
        MemorySegment.copy(defaultParams, 0, params, 0, LlamaStructs.CONTEXT_PARAMS_LAYOUT.byteSize());
        return params;
    }
    
    public MemorySegment createContext(Arena arena, MemorySegment model, int ctx, int batch, int threads, int seed, float ropeBase, float ropeScale) throws Throwable {
        MemorySegment params = getContextDefaultParams(arena);
        params.set(ValueLayout.JAVA_INT, 0, seed);
        params.set(ValueLayout.JAVA_INT, 4, ctx);
        params.set(ValueLayout.JAVA_INT, 8, batch);
        params.set(ValueLayout.JAVA_INT, 12, batch);
        params.set(ValueLayout.JAVA_INT, 16, 1);
        params.set(ValueLayout.JAVA_INT, 20, threads);
        params.set(ValueLayout.JAVA_INT, 24, threads);
        params.set(ValueLayout.JAVA_FLOAT, 40, ropeBase);
        params.set(ValueLayout.JAVA_FLOAT, 44, ropeScale);
        
        return (MemorySegment) llamaNewContextWithModel.invoke(model, params);
    }
    
    public void freeContext(MemorySegment ctx) throws Throwable {
        llamaFreeContext.invoke(ctx);
    }
    
    public int nVocab(MemorySegment model) throws Throwable {
        return (int) llamaNVocab.invoke(model);
    }
    
    public int nCtxTrain(MemorySegment model) throws Throwable {
        return (int) llamaNCtxTrain.invoke(model);
    }
    
    public String modelDesc(Arena arena, MemorySegment model) throws Throwable {
        MemorySegment buffer = arena.allocate(256);
        int len = (int) llamaModelDesc.invoke(model, buffer, 256L);
        return buffer.getString(0, StandardCharsets.UTF_8);
    }
    
    // Tokenization
    public int[] tokenize(Arena arena, MemorySegment model, String text, boolean addBos, boolean special) throws Throwable {
        MemorySegment textSeg = arena.allocateFrom(text, StandardCharsets.UTF_8);
        int maxTokens = text.length() * 2 + 10;
        MemorySegment tokens = arena.allocate(ValueLayout.JAVA_INT, maxTokens);
        
        int n = (int) llamaTokenize.invoke(model, textSeg, text.length(), tokens, maxTokens, addBos, special);
        
        if (n < 0) {
            maxTokens = -n;
            tokens = arena.allocate(ValueLayout.JAVA_INT, maxTokens);
            n = (int) llamaTokenize.invoke(model, textSeg, text.length(), tokens, maxTokens, addBos, special);
        }
        
        int[] result = new int[n];
        for (int i = 0; i < n; i++) {
            result[i] = tokens.getAtIndex(ValueLayout.JAVA_INT, i);
        }
        return result;
    }
    
    public String tokenToString(Arena arena, MemorySegment model, int token) throws Throwable {
        MemorySegment buffer = arena.allocate(64);
        int len = (int) llamaTokenToString.invoke(model, token, buffer, 64, 0, true);
        
        if (len < 0) {
            return "";
        }
        
        byte[] bytes = new byte[len];
        MemorySegment.copy(buffer, ValueLayout.JAVA_BYTE, 0, bytes, 0, len);
        return new String(bytes, StandardCharsets.UTF_8);
    }
    
    public int tokenBos(MemorySegment model) throws Throwable {
        return (int) llamaTokenBos.invoke(model);
    }
    
    public int tokenEos(MemorySegment model) throws Throwable {
        return (int) llamaTokenEos.invoke(model);
    }
    
    public int tokenNl(MemorySegment model) throws Throwable {
        return (int) llamaTokenNl.invoke(model);
    }
    
    // Inference
    public int decode(MemorySegment ctx, MemorySegment batch) throws Throwable {
        return (int) llamaDecode.invoke(ctx, batch);
    }
    
    public MemorySegment getLogits(MemorySegment ctx) throws Throwable {
        return (MemorySegment) llamaGetLogits.invoke(ctx);
    }
    
    public MemorySegment getLogitsIth(MemorySegment ctx, int i) throws Throwable {
        return (MemorySegment) llamaGetLogitsIth.invoke(ctx, i);
    }
    
    // KV Cache management
    public void kvCacheClear(MemorySegment ctx) throws Throwable {
        llamaKvCacheClear.invoke(ctx);
    }
    
    public boolean kvCacheSeqRm(MemorySegment ctx, int seqId, int p0, int p1) throws Throwable {
        return (boolean) llamaKvCacheSeqRm.invoke(ctx, seqId, p0, p1);
    }
    
    public void kvCacheSeqCp(MemorySegment ctx, int seqIdSrc, int seqIdDst, int p0, int p1) throws Throwable {
        llamaKvCacheSeqCp.invoke(ctx, seqIdSrc, seqIdDst, p0, p1);
    }
    
    public void kvCacheSeqKeep(MemorySegment ctx, int seqId) throws Throwable {
        llamaKvCacheSeqKeep.invoke(ctx, seqId);
    }
    
    public void kvCacheSeqAdd(MemorySegment ctx, int seqId, int p0, int p1, int delta) throws Throwable {
        llamaKvCacheSeqAdd.invoke(ctx, seqId, p0, p1, delta);
    }
}

// ============================================================================
// src/main/java/com/example/llama/llama/SamplingParams.java
// ============================================================================
package com.example.llama.llama;



// ============================================================================
// src/main/java/com/example/llama/llama/Sampler.java
// ============================================================================
package com.example.llama.llama;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.*;

public class Sampler {
    private final SamplingParams params;
    private final Random rng;
    private final List<Integer> lastTokens;
    
    public Sampler(SamplingParams params) {
        this.params = params;
        this.rng = params.seed() >= 0 ? new Random(params.seed()) : new Random();
        this.lastTokens = new ArrayList<>(params.repeatLastN());
    }
    
    public int sample(Arena arena, MemorySegment logits, int vocabSize, Set<Integer> stopTokens) {
        // Copy logits to array
        float[] logitsArray = new float[vocabSize];
        for (int i = 0; i < vocabSize; i++) {
            logitsArray[i] = logits.getAtIndex(ValueLayout.JAVA_FLOAT, i);
        }
        
        // Apply repeat penalty
        if (params.repeatPenalty() != 1.0f && !lastTokens.isEmpty()) {
            for (int token : lastTokens) {
                if (logitsArray[token] > 0) {
                    logitsArray[token] /= params.repeatPenalty();
                } else {
                    logitsArray[token] *= params.repeatPenalty();
                }
            }
        }
        
        // Apply frequency and presence penalties
        if (params.frequencyPenalty() != 0.0f || params.presencePenalty() != 0.0f) {
            Map<Integer, Integer> tokenCounts = new HashMap<>();
            for (int token : lastTokens) {
                tokenCounts.merge(token, 1, Integer::sum);
            }
            
            for (Map.Entry<Integer, Integer> entry : tokenCounts.entrySet()) {
                int token = entry.getKey();
                int count = entry.getValue();
                float penalty = params.frequencyPenalty() * count + params.presencePenalty();
                logitsArray[token] -= penalty;
            }
        }
        
        // Top-K filtering
        if (params.topK() > 0 && params.topK() < vocabSize) {
            float[] sortedLogits = logitsArray.clone();
            Arrays.sort(sortedLogits);
            float kthLargest = sortedLogits[vocabSize - params.topK()];
            for (int i = 0; i < vocabSize; i++) {
                if (logitsArray[i] < kthLargest) {
                    logitsArray[i] = Float.NEGATIVE_INFINITY;
                }
            }
        }
        
        // Temperature scaling
        if (params.temperature() > 0 && params.temperature() != 1.0f) {
            for (int i = 0; i < vocabSize; i++) {
                logitsArray[i] /= params.temperature();
            }
        }
        
        // Softmax
        float maxLogit = Float.NEGATIVE_INFINITY;
        for (float logit : logitsArray) {
            if (logit > maxLogit && !Float.isInfinite(logit)) {
                maxLogit = logit;
            }
        }
        
        float sumExp = 0.0f;
        float[] probs = new float[vocabSize];
        for (int i = 0; i < vocabSize; i++) {
            if (Float.isInfinite(logitsArray[i])) {
                probs[i] = 0.0f;
            } else {
                probs[i] = (float) Math.exp(logitsArray[i] - maxLogit);
                sumExp += probs[i];
            }
        }
        
        for (int i = 0; i < vocabSize; i++) {
            probs[i] /= sumExp;
        }
        
        // Min-P filtering
        if (params.minP() > 0.0f) {
            float maxProb = 0.0f;
            for (float p : probs) {
                if (p > maxProb) maxProb = p;
            }
            float threshold = params.minP() * maxProb;
            for (int i = 0; i < vocabSize; i++) {
                if (probs[i] < threshold) {
                    probs[i] = 0.0f;
                }
            }
            
            // Renormalize
            sumExp = 0.0f;
            for (float p : probs) sumExp += p;
            if (sumExp > 0) {
                for (int i = 0; i < vocabSize; i++) {
                    probs[i] /= sumExp;
                }
            }
        }
        
        // Top-P (nucleus) sampling
        if (params.topP() < 1.0f) {
            List<TokenProb> sorted = new ArrayList<>();
            for (int i = 0; i < vocabSize; i++) {
                if (probs[i] > 0) {
                    sorted.add(new TokenProb(i, probs[i]));
                }
            }
            sorted.sort((a, b) -> Float.compare(b.prob, a.prob));
            
            float cumProb = 0.0f;
            int cutoff = 0;
            for (int i = 0; i < sorted.size(); i++) {
                cumProb += sorted.get(i).prob;
                if (cumProb >= params.topP()) {
                    cutoff = i + 1;
                    break;
                }
            }
            
            Arrays.fill(probs, 0.0f);
            sumExp = 0.0f;
            for (int i = 0; i < cutoff; i++) {
                TokenProb tp = sorted.get(i);
                probs[tp.token] = tp.prob;
                sumExp += tp.prob;
            }
            
            // Renormalize
            if (sumExp > 0) {
                for (int i = 0; i < vocabSize; i++) {
                    probs[i] /= sumExp;
                }
            }
        }
        
        // Sample from distribution
        float random = rng.nextFloat();
        float cumulative = 0.0f;
        for (int i = 0; i < vocabSize; i++) {
            cumulative += probs[i];
            if (random < cumulative) {
                accept(i);
                return i;
            }
        }
        
        // Fallback to last token with non-zero probability
        for (int i = vocabSize - 1; i >= 0; i--) {
            if (probs[i] > 0) {
                accept(i);
                return i;
            }
        }
        
        accept(0);
        return 0;
    }
    
    public void accept(int token) {
        lastTokens.add(token);
        if (lastTokens.size() > params.repeatLastN()) {
            lastTokens.remove(0);
        }
    }
    
    public void reset() {
        lastTokens.clear();
    }
    
    private record TokenProb(int token, float prob) {}
}

// ============================================================================
// src/main/java/com/example/llama/service/ConversationContext.java
// ============================================================================
package com.example.llama.service;

import com.example.llama.model.ChatMessage;
import java.util.*;

public class ConversationContext {
    private final String id;
    private final List<ChatMessage> messages;
    private final List<Integer> tokens;
    private final long createdAt;
    private long lastAccessedAt;
    
    public ConversationContext(String id) {
        this.id = id;
        this.messages = new ArrayList<>();
        this.tokens = new ArrayList<>();
        this.createdAt = System.currentTimeMillis();
        this.lastAccessedAt = createdAt;
    }
    
    public void addMessage(ChatMessage message, List<Integer> messageTokens) {
        messages.add(message);
        tokens.addAll(messageTokens);
        lastAccessedAt = System.currentTimeMillis();
    }
    
    public void truncate(int maxTokens) {
        if (tokens.size() <= maxTokens) return;
        
        int tokensToRemove = tokens.size() - maxTokens;
        tokens.subList(0, tokensToRemove).clear();
        
        // Try to keep message boundaries
        while (!messages.isEmpty() && tokens.size() < maxTokens / 2) {
            messages.remove(0);
        }
    }
    
    public String getId() { return id; }
    public List<ChatMessage> getMessages() { return Collections.unmodifiableList(messages); }
    public List<Integer> getTokens() { return Collections.unmodifiableList(tokens); }
    public long getCreatedAt() { return createdAt; }
    public long getLastAccessedAt() { return lastAccessedAt; }
}

// ============================================================================
// src/main/java/com/example/llama/service/LlamaInferenceService.java
// ============================================================================
package com.example.llama.service;

import com.example.llama.config.LlamaConfig;
import com.example.llama.llama.*;
import com.example.llama.model.*;
import io.smallrye.mutiny.Multi;
import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.jboss.logging.Logger;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

@ApplicationScoped
public class LlamaInferenceService {
    
    private static final Logger LOG = Logger.getLogger(LlamaInferenceService.class);
    private static final int MAX_CONTEXTS = 100;
    private static final long CONTEXT_TIMEOUT_MS = 30 * 60 * 1000; // 30 minutes
    
    @Inject
    LlamaConfig config;
    
    private LlamaCppBinding binding;
    private Arena arena;
    private MemorySegment model;
    private MemorySegment context;
    private int vocabSize;
    private int bosToken;
    private int eosToken;
    private int nlToken;
    private String modelDesc;
    
    private final Map<String, ConversationContext> conversations = new ConcurrentHashMap<>();
    private final AtomicInteger requestCounter = new AtomicInteger(0);
    
    @PostConstruct
    void initialize() {
        LOG.info("Initializing Llama.cpp inference service...");
        
        try {
            arena = Arena.ofShared();
            binding = new LlamaCppBinding(config.libraryPath());
            binding.backendInit();
            
            LOG.infof("Loading model: %s", config.modelPath());
            model = binding.loadModel(arena, config.modelPath(), config.gpuLayers());
            
            if (model.address() == 0) {
                throw new RuntimeException("Failed to load model");
            }
            
            vocabSize = binding.nVocab(model);
            bosToken = binding.tokenBos(model);
            eosToken = binding.tokenEos(model);
            nlToken = binding.tokenNl(model);
            modelDesc = binding.modelDesc(arena, model);
            
            LOG.infof("Model loaded: %s, vocab_size=%d", modelDesc, vocabSize);
            
            context = binding.createContext(arena, model, 
                config.contextSize(), config.batchSize(), config.threads(),
                config.seed(), config.ropeFreqBase(), config.ropeFreqScale());
            
            if (context.address() == 0) {
                throw new RuntimeException("Failed to create context");
            }
            
            LOG.infof("Context created: ctx_size=%d, batch=%d, threads=%d", 
                config.contextSize(), config.batchSize(), config.threads());
            
            // Start cleanup task
            startCleanupTask();
            
            LOG.info("Llama.cpp inference service initialized successfully");
            
        } catch (Throwable e) {
            LOG.error("Failed to initialize inference service", e);
            throw new RuntimeException(e);
        }
    }
    
    @PreDestroy
    void cleanup() {
        LOG.info("Shutting down Llama.cpp inference service...");
        
        try {
            conversations.clear();
            
            if (context != null && context.address() != 0) {
                binding.freeContext(context);
            }
            
            if (model != null && model.address() != 0) {
                binding.freeModel(model);
            }
            
            binding.backendFree();
            
            if (arena != null) {
                arena.close();
            }
            
            LOG.info("Shutdown complete");
        } catch (Throwable e) {
            LOG.error("Error during shutdown", e);
        }
    }
    
    public ChatResponse generateChat(ChatRequest request) {
        String id = "chatcmpl-" + requestCounter.incrementAndGet();
        long created = System.currentTimeMillis() / 1000;
        
        try {
            String prompt = formatChatPrompt(request.messages());
            SamplingParams samplingParams = createSamplingParams(request);
            
            int[] promptTokens = binding.tokenize(arena, model, prompt, true, false);
            LOG.infof("Request %s: prompt_tokens=%d", id, promptTokens.length);
            
            List<String> pieces = new ArrayList<>();
            int completionTokens = generateTokens(promptTokens, request.maxTokens(), 
                samplingParams, request.stop(), pieces::add);
            
            String content = String.join("", pieces);
            
            return new ChatResponse(
                id, "chat.completion", created, modelDesc,
                new ChatResponse.Choice(0, 
                    new ChatMessage("assistant", content),
                    "stop"),
                new ChatResponse.Usage(promptTokens.length, completionTokens, 
                    promptTokens.length + completionTokens)
            );
            
        } catch (Throwable e) {
            LOG.error("Generation failed", e);
            throw new RuntimeException("Generation failed: " + e.getMessage(), e);
        }
    }
    
    public Multi<String> generateChatStream(ChatRequest request) {
        String id = "chatcmpl-" + requestCounter.incrementAndGet();
        long created = System.currentTimeMillis() / 1000;
        
        return Multi.createFrom().emitter(emitter -> {
            try {
                String prompt = formatChatPrompt(request.messages());
                SamplingParams samplingParams = createSamplingParams(request);
                
                int[] promptTokens = binding.tokenize(arena, model, prompt, true, false);
                LOG.infof("Stream request %s: prompt_tokens=%d", id, promptTokens.length);
                
                // Send initial chunk
                emitter.emit(formatStreamChunk(id, created, "assistant", null, null));
                
                generateTokens(promptTokens, request.maxTokens(), samplingParams, 
                    request.stop(), piece -> {
                        emitter.emit(formatStreamChunk(id, created, null, piece, null));
                    });
                
                // Send final chunk
                emitter.emit(formatStreamChunk(id, created, null, null, "stop"));
                emitter.emit("data: [DONE]\n\n");
                emitter.complete();
                
            } catch (Throwable e) {
                LOG.error("Stream generation failed", e);
                emitter.fail(e);
            }
        });
    }
    
    private synchronized int generateTokens(int[] promptTokens, int maxTokens,
                                           SamplingParams samplingParams, List<String> stopStrings,
                                           java.util.function.Consumer<String> callback) throws Throwable {
        
        Arena sessionArena = Arena.ofConfined();
        
        try {
            // Clear KV cache
            binding.kvCacheClear(context);
            
            // Process prompt
            int nPrompt = promptTokens.length;
            for (int i = 0; i < nPrompt; i += config.batchSize()) {
                int batchSize = Math.min(config.batchSize(), nPrompt - i);
                int[] batchTokens = Arrays.copyOfRange(promptTokens, i, i + batchSize);
                int[] positions = new int[batchSize];
                boolean[] logits = new boolean[batchSize];
                
                for (int j = 0; j < batchSize; j++) {
                    positions[j] = i + j;
                    logits[j] = (i + j == nPrompt - 1); // Only last token needs logits
                }
                
                MemorySegment batch = LlamaStructs.createBatch(sessionArena, batchTokens, positions, logits);
                
                int ret = binding.decode(context, batch);
                if (ret != 0) {
                    throw new RuntimeException("Decode failed: " + ret);
                }
            }
            
            // Generate tokens
            Sampler sampler = new Sampler(samplingParams);
            Set<Integer> stopTokens = new HashSet<>();
            stopTokens.add(eosToken);
            
            StringBuilder currentText = new StringBuilder();
            int nGenerated = 0;
            int pos = nPrompt;
            
            for (int i = 0; i < maxTokens; i++) {
                MemorySegment logits = binding.getLogitsIth(context, 0);
                int nextToken = sampler.sample(sessionArena, logits, vocabSize, stopTokens);
                
                if (nextToken == eosToken) {
                    break;
                }
                
                String piece = binding.tokenToString(sessionArena, model, nextToken);
                currentText.append(piece);
                nGenerated++;
                
                // Check stop strings
                boolean shouldStop = false;
                if (stopStrings != null && !stopStrings.isEmpty()) {
                    String fullText = currentText.toString();
                    for (String stop : stopStrings) {
                        if (fullText.endsWith(stop)) {
                            piece = piece.substring(0, piece.length() - stop.length());
                            shouldStop = true;
                            break;
                        }
                    }
                }
                
                if (!piece.isEmpty()) {
                    callback.accept(piece);
                }
                
                if (shouldStop) {
                    break;
                }
                
                // Decode next token
                int[] nextTokens = {nextToken};
                int[] positions = {pos++};
                boolean[] logitsFlags = {true};
                
                MemorySegment batch = LlamaStructs.createBatch(sessionArena, nextTokens, positions, logitsFlags);
                int ret = binding.decode(context, batch);
                if (ret != 0) {
                    throw new RuntimeException("Decode failed: " + ret);
                }
            }
            
            return nGenerated;
            
        } finally {
            sessionArena.close();
        }
    }
    
    private String formatChatPrompt(List<ChatMessage> messages) {
        // Llama-2-chat format
        StringBuilder prompt = new StringBuilder();
        prompt.append("<s>");
        
        for (ChatMessage msg : messages) {
            if ("system".equals(msg.role())) {
                prompt.append("[INST] <<SYS>>\n")
                      .append(msg.content())
                      .append("\n<</SYS>>\n\n");
            } else if ("user".equals(msg.role())) {
                prompt.append("[INST] ")
                      .append(msg.content())
                      .append(" [/INST]");
            } else if ("assistant".equals(msg.role())) {
                prompt.append(" ")
                      .append(msg.content())
                      .append(" </s><s>");
            }
        }
        
        return prompt.toString();
    }
    
    private SamplingParams createSamplingParams(ChatRequest request) {
        var defaultSampling = config.sampling();
        
        return new SamplingParams(
            request.temperature() != null ? request.temperature() : defaultSampling.temperature(),
            request.topK() != null ? request.topK() : defaultSampling.topK(),
            request.topP() != null ? request.topP() : defaultSampling.topP(),
            request.minP() != null ? request.minP() : defaultSampling.minP(),
            request.repeatPenalty() != null ? request.repeatPenalty() : defaultSampling.repeatPenalty(),
            defaultSampling.repeatLastN(),
            request.presencePenalty() != null ? request.presencePenalty() : defaultSampling.presencePenalty(),
            request.frequencyPenalty() != null ? request.frequencyPenalty() : defaultSampling.frequencyPenalty(),
            config.seed()
        );
    }
    
    private String formatStreamChunk(String id, long created, String role, String content, String finishReason) {
        var delta = role != null ? 
            new StreamChunk.Delta(role, null) : 
            new StreamChunk.Delta(null, content);
        
        var chunk = new StreamChunk(id, "chat.completion.chunk", created, modelDesc, delta, finishReason);
        
        try {
            return "data: " + new com.fasterxml.jackson.databind.ObjectMapper().writeValueAsString(chunk) + "\n\n";
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }
    
    private void startCleanupTask() {
        new Thread(() -> {
            while (true) {
                try {
                    Thread.sleep(60000); // Check every minute
                    cleanupOldContexts();
                } catch (InterruptedException e) {
                    break;
                }
            }
        }, "context-cleanup").start();
    }
    
    private void cleanupOldContexts() {
        long now = System.currentTimeMillis();
        conversations.entrySet().removeIf(entry -> 
            now - entry.getValue().getLastAccessedAt() > CONTEXT_TIMEOUT_MS
        );
    }
}

// ============================================================================
// src/main/java/com/example/llama/resource/ChatResource.java
// ============================================================================
package com.example.llama.resource;

import com.example.llama.model.*;
import com.example.llama.service.LlamaInferenceService;
import io.smallrye.mutiny.Multi;
import jakarta.inject.Inject;
import jakarta.ws.rs.*;
import jakarta.ws.rs.core.MediaType;
import org.jboss.logging.Logger;
import org.jboss.resteasy.reactive.RestStreamElementType;

@Path("/v1/chat")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public class ChatResource {
    
    private static final Logger LOG = Logger.getLogger(ChatResource.class);
    
    @Inject
    LlamaInferenceService inferenceService;
    
    @POST
    @Path("/completions")
    public Object completions(ChatRequest request) {
        LOG.infof("Chat request: messages=%d, stream=%b", 
            request.messages().size(), request.stream());
        
        if (request.stream()) {
            return streamCompletions(request);
        } else {
            return inferenceService.generateChat(request);
        }
    }
    
    @POST
    @Path("/completions")
    @Produces(MediaType.SERVER_SENT_EVENTS)
    @RestStreamElementType(MediaType.TEXT_PLAIN)
    public Multi<String> streamCompletions(ChatRequest request) {
        return inferenceService.generateChatStream(request);
    }
}

// ============================================================================
// src/main/java/com/example/llama/resource/ModelResource.java
// ============================================================================
package com.example.llama.resource;

import jakarta.ws.rs.*;
import jakarta.ws.rs.core.MediaType;
import java.util.List;
import java.util.Map;

@Path("/v1/models")
@Produces(MediaType.APPLICATION_JSON)
public class ModelResource {
    
    @GET
    public Map<String, Object> listModels() {
        return Map.of(
            "object", "list",
            "data", List.of(
                Map.of(
                    "id", "llama-2-7b-chat",
                    "object", "model",
                    "created", System.currentTimeMillis() / 1000,
                    "owned_by", "meta"
                )
            )
        );
    }
}
