// ============================================================================
// ROOT pom.xml
// ============================================================================
/*
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    
    <groupId>com.example.llama</groupId>
    <artifactId>llama-parent</artifactId>
    <version>1.0.0</version>
    <packaging>pom</packaging>
    
    <properties>
        <maven.compiler.release>25</maven.compiler.release>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <quarkus.version>3.16.3</quarkus.version>
        <javafx.version>23</javafx.version>
    </properties>
    
    <modules>
        <module>llama-core</module>
        <module>llama-server</module>
        <module>llama-javafx-client</module>
    </modules>
    
    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>io.quarkus.platform</groupId>
                <artifactId>quarkus-bom</artifactId>
                <version>${quarkus.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
            <dependency>
                <groupId>org.openjfx</groupId>
                <artifactId>javafx-controls</artifactId>
                <version>${javafx.version}</version>
            </dependency>
        </dependencies>
    </dependencyManagement>
</project>
*/

// ============================================================================
// llama-core/pom.xml
// ============================================================================
/*
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    
    <parent>
        <groupId>com.example.llama</groupId>
        <artifactId>llama-parent</artifactId>
        <version>1.0.0</version>
    </parent>
    
    <artifactId>llama-core</artifactId>
    <packaging>jar</packaging>
    <name>Llama Core Library</name>
    
    <dependencies>
        <!-- JSON processing -->
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
            <version>2.18.0</version>
        </dependency>
        
        <!-- HTTP client for model downloads -->
        <dependency>
            <groupId>com.squareup.okhttp3</groupId>
            <artifactId>okhttp</artifactId>
            <version>4.12.0</version>
        </dependency>
        
        <!-- Logging -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>2.0.16</version>
        </dependency>
        
        <!-- Testing -->
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter</artifactId>
            <version>5.11.3</version>
            <scope>test</scope>
        </dependency>
    </dependencies>
    
    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.13.0</version>
                <configuration>
                    <release>25</release>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project>
*/

// ============================================================================
// llama-core/src/main/java/com/example/llama/core/LlamaConfig.java
// ============================================================================
package com.example.llama.core;

public record LlamaConfig(
    String libraryPath,
    String modelPath,
    int contextSize,
    int batchSize,
    int threads,
    int gpuLayers,
    float ropeFreqBase,
    float ropeFreqScale,
    int seed
) {
    public static Builder builder() {
        return new Builder();
    }
    
    public static class Builder {
        private String libraryPath;
        private String modelPath;
        private int contextSize = 4096;
        private int batchSize = 512;
        private int threads = 8;
        private int gpuLayers = 0;
        private float ropeFreqBase = 10000.0f;
        private float ropeFreqScale = 1.0f;
        private int seed = -1;
        
        public Builder libraryPath(String path) { this.libraryPath = path; return this; }
        public Builder modelPath(String path) { this.modelPath = path; return this; }
        public Builder contextSize(int size) { this.contextSize = size; return this; }
        public Builder batchSize(int size) { this.batchSize = size; return this; }
        public Builder threads(int threads) { this.threads = threads; return this; }
        public Builder gpuLayers(int layers) { this.gpuLayers = layers; return this; }
        public Builder ropeFreqBase(float base) { this.ropeFreqBase = base; return this; }
        public Builder ropeFreqScale(float scale) { this.ropeFreqScale = scale; return this; }
        public Builder seed(int seed) { this.seed = seed; return this; }
        
        public LlamaConfig build() {
            if (libraryPath == null) throw new IllegalStateException("libraryPath required");
            if (modelPath == null) throw new IllegalStateException("modelPath required");
            return new LlamaConfig(libraryPath, modelPath, contextSize, batchSize, 
                threads, gpuLayers, ropeFreqBase, ropeFreqScale, seed);
        }
    }
}

// ============================================================================
// llama-core/src/main/java/com/example/llama/core/SamplingConfig.java
// ============================================================================
package com.example.llama.core;

public record SamplingConfig(
    float temperature,
    int topK,
    float topP,
    float minP,
    float repeatPenalty,
    int repeatLastN,
    float presencePenalty,
    float frequencyPenalty
) {
    public static SamplingConfig defaults() {
        return new SamplingConfig(0.8f, 40, 0.95f, 0.05f, 1.1f, 64, 0.0f, 0.0f);
    }
    
    public static Builder builder() {
        return new Builder();
    }
    
    public static class Builder {
        private float temperature = 0.8f;
        private int topK = 40;
        private float topP = 0.95f;
        private float minP = 0.05f;
        private float repeatPenalty = 1.1f;
        private int repeatLastN = 64;
        private float presencePenalty = 0.0f;
        private float frequencyPenalty = 0.0f;
        
        public Builder temperature(float t) { this.temperature = t; return this; }
        public Builder topK(int k) { this.topK = k; return this; }
        public Builder topP(float p) { this.topP = p; return this; }
        public Builder minP(float p) { this.minP = p; return this; }
        public Builder repeatPenalty(float p) { this.repeatPenalty = p; return this; }
        public Builder repeatLastN(int n) { this.repeatLastN = n; return this; }
        public Builder presencePenalty(float p) { this.presencePenalty = p; return this; }
        public Builder frequencyPenalty(float p) { this.frequencyPenalty = p; return this; }
        
        public SamplingConfig build() {
            return new SamplingConfig(temperature, topK, topP, minP, 
                repeatPenalty, repeatLastN, presencePenalty, frequencyPenalty);
        }
    }
}

// ============================================================================
// llama-core/src/main/java/com/example/llama/core/model/ChatMessage.java
// ============================================================================
package com.example.llama.core.model;

// ============================================================================
// llama-core/src/main/java/com/example/llama/core/model/Tool.java
// ============================================================================
package com.example.llama.core.model;


// ============================================================================
// llama-core/src/main/java/com/example/llama/core/model/GenerationResult.java
// ============================================================================
package com.example.llama.core.model;


// ============================================================================
// llama-core/src/main/java/com/example/llama/core/binding/LlamaStructs.java
// ============================================================================
package com.example.llama.core.binding;

import java.lang.foreign.*;

public class LlamaStructs {
    
    public static final StructLayout MODEL_PARAMS_LAYOUT = MemoryLayout.structLayout(
        ValueLayout.JAVA_INT.withName("n_gpu_layers"),
        ValueLayout.JAVA_INT.withName("split_mode"),
        ValueLayout.JAVA_INT.withName("main_gpu"),
        MemoryLayout.paddingLayout(4),
        ValueLayout.ADDRESS.withName("tensor_split"),
        ValueLayout.JAVA_BOOLEAN.withName("vocab_only"),
        ValueLayout.JAVA_BOOLEAN.withName("use_mmap"),
        ValueLayout.JAVA_BOOLEAN.withName("use_mlock"),
        MemoryLayout.paddingLayout(5)
    );
    
    public static final StructLayout CONTEXT_PARAMS_LAYOUT = MemoryLayout.structLayout(
        ValueLayout.JAVA_INT.withName("seed"),
        ValueLayout.JAVA_INT.withName("n_ctx"),
        ValueLayout.JAVA_INT.withName("n_batch"),
        ValueLayout.JAVA_INT.withName("n_ubatch"),
        ValueLayout.JAVA_INT.withName("n_seq_max"),
        ValueLayout.JAVA_INT.withName("n_threads"),
        ValueLayout.JAVA_INT.withName("n_threads_batch"),
        ValueLayout.JAVA_INT.withName("rope_scaling_type"),
        ValueLayout.JAVA_INT.withName("pooling_type"),
        ValueLayout.JAVA_INT.withName("attention_type"),
        ValueLayout.JAVA_FLOAT.withName("rope_freq_base"),
        ValueLayout.JAVA_FLOAT.withName("rope_freq_scale"),
        ValueLayout.JAVA_FLOAT.withName("yarn_ext_factor"),
        ValueLayout.JAVA_FLOAT.withName("yarn_attn_factor"),
        ValueLayout.JAVA_FLOAT.withName("yarn_beta_fast"),
        ValueLayout.JAVA_FLOAT.withName("yarn_beta_slow"),
        ValueLayout.JAVA_INT.withName("yarn_orig_ctx"),
        ValueLayout.JAVA_FLOAT.withName("defrag_thold"),
        MemoryLayout.paddingLayout(4),
        ValueLayout.ADDRESS.withName("cb_eval"),
        ValueLayout.ADDRESS.withName("cb_eval_user_data"),
        ValueLayout.JAVA_INT.withName("type_k"),
        ValueLayout.JAVA_INT.withName("type_v"),
        ValueLayout.JAVA_BOOLEAN.withName("logits_all"),
        ValueLayout.JAVA_BOOLEAN.withName("embeddings"),
        ValueLayout.JAVA_BOOLEAN.withName("offload_kqv"),
        ValueLayout.JAVA_BOOLEAN.withName("flash_attn"),
        MemoryLayout.paddingLayout(4),
        ValueLayout.ADDRESS.withName("abort_callback"),
        ValueLayout.ADDRESS.withName("abort_callback_data")
    );
    
    public static final StructLayout BATCH_LAYOUT = MemoryLayout.structLayout(
        ValueLayout.JAVA_INT.withName("n_tokens"),
        MemoryLayout.paddingLayout(4),
        ValueLayout.ADDRESS.withName("token"),
        ValueLayout.ADDRESS.withName("embd"),
        ValueLayout.ADDRESS.withName("pos"),
        ValueLayout.ADDRESS.withName("n_seq_id"),
        ValueLayout.ADDRESS.withName("seq_id"),
        ValueLayout.ADDRESS.withName("logits")
    );
    
    public static MemorySegment createBatch(Arena arena, int[] tokens, int[] positions, boolean[] logits) {
        MemorySegment batch = arena.allocate(BATCH_LAYOUT);
        int nTokens = tokens.length;
        
        batch.set(ValueLayout.JAVA_INT, 0, nTokens);
        
        MemorySegment tokenSegment = arena.allocate(ValueLayout.JAVA_INT, nTokens);
        MemorySegment posSegment = arena.allocate(ValueLayout.JAVA_INT, nTokens);
        MemorySegment logitsSegment = arena.allocate(ValueLayout.JAVA_BYTE, nTokens);
        
        for (int i = 0; i < nTokens; i++) {
            tokenSegment.setAtIndex(ValueLayout.JAVA_INT, i, tokens[i]);
            posSegment.setAtIndex(ValueLayout.JAVA_INT, i, positions[i]);
            logitsSegment.setAtIndex(ValueLayout.JAVA_BYTE, i, (byte) (logits[i] ? 1 : 0));
        }
        
        batch.set(ValueLayout.ADDRESS, 8, tokenSegment);
        batch.set(ValueLayout.ADDRESS, 16, MemorySegment.NULL);
        batch.set(ValueLayout.ADDRESS, 24, posSegment);
        batch.set(ValueLayout.ADDRESS, 32, MemorySegment.NULL);
        batch.set(ValueLayout.ADDRESS, 40, MemorySegment.NULL);
        batch.set(ValueLayout.ADDRESS, 48, logitsSegment);
        
        return batch;
    }
}

// ============================================================================
// llama-core/src/main/java/com/example/llama/core/binding/LlamaCppBinding.java
// ============================================================================
package com.example.llama.core.binding;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.lang.foreign.*;
import java.lang.invoke.MethodHandle;
import java.nio.charset.StandardCharsets;

public class LlamaCppBinding {
    private static final Logger log = LoggerFactory.getLogger(LlamaCppBinding.class);
    
    private final SymbolLookup lookup;
    private final MethodHandle llamaBackendInit;
    private final MethodHandle llamaBackendFree;
    private final MethodHandle llamaModelDefaultParams;
    private final MethodHandle llamaLoadModelFromFile;
    private final MethodHandle llamaFreeModel;
    private final MethodHandle llamaContextDefaultParams;
    private final MethodHandle llamaNewContextWithModel;
    private final MethodHandle llamaFreeContext;
    private final MethodHandle llamaNVocab;
    private final MethodHandle llamaModelDesc;
    private final MethodHandle llamaTokenize;
    private final MethodHandle llamaTokenToString;
    private final MethodHandle llamaTokenBos;
    private final MethodHandle llamaTokenEos;
    private final MethodHandle llamaTokenNl;
    private final MethodHandle llamaDecode;
    private final MethodHandle llamaGetLogitsIth;
    private final MethodHandle llamaKvCacheClear;
    
    public LlamaCppBinding(String libraryPath) {
        log.info("Loading llama.cpp library from: {}", libraryPath);
        System.load(libraryPath);
        this.lookup = SymbolLookup.loaderLookup();
        
        var linker = Linker.nativeLinker();
        
        this.llamaBackendInit = findFunction(linker, "llama_backend_init", FunctionDescriptor.ofVoid());
        this.llamaBackendFree = findFunction(linker, "llama_backend_free", FunctionDescriptor.ofVoid());
        this.llamaModelDefaultParams = findFunction(linker, "llama_model_default_params", 
            FunctionDescriptor.of(LlamaStructs.MODEL_PARAMS_LAYOUT));
        this.llamaLoadModelFromFile = findFunction(linker, "llama_load_model_from_file",
            FunctionDescriptor.of(ValueLayout.ADDRESS, ValueLayout.ADDRESS, LlamaStructs.MODEL_PARAMS_LAYOUT));
        this.llamaFreeModel = findFunction(linker, "llama_free_model",
            FunctionDescriptor.ofVoid(ValueLayout.ADDRESS));
        this.llamaContextDefaultParams = findFunction(linker, "llama_context_default_params",
            FunctionDescriptor.of(LlamaStructs.CONTEXT_PARAMS_LAYOUT));
        this.llamaNewContextWithModel = findFunction(linker, "llama_new_context_with_model",
            FunctionDescriptor.of(ValueLayout.ADDRESS, ValueLayout.ADDRESS, LlamaStructs.CONTEXT_PARAMS_LAYOUT));
        this.llamaFreeContext = findFunction(linker, "llama_free",
            FunctionDescriptor.ofVoid(ValueLayout.ADDRESS));
        this.llamaNVocab = findFunction(linker, "llama_n_vocab",
            FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));
        this.llamaModelDesc = findFunction(linker, "llama_model_desc",
            FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS, ValueLayout.ADDRESS, ValueLayout.JAVA_LONG));
        this.llamaTokenize = findFunction(linker, "llama_tokenize",
            FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS, ValueLayout.ADDRESS, 
                ValueLayout.JAVA_INT, ValueLayout.ADDRESS, ValueLayout.JAVA_INT, 
                ValueLayout.JAVA_BOOLEAN, ValueLayout.JAVA_BOOLEAN));
        this.llamaTokenToString = findFunction(linker, "llama_token_to_piece",
            FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS, ValueLayout.JAVA_INT,
                ValueLayout.ADDRESS, ValueLayout.JAVA_INT, ValueLayout.JAVA_INT, ValueLayout.JAVA_BOOLEAN));
        this.llamaTokenBos = findFunction(linker, "llama_token_bos",
            FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));
        this.llamaTokenEos = findFunction(linker, "llama_token_eos",
            FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));
        this.llamaTokenNl = findFunction(linker, "llama_token_nl",
            FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS));
        this.llamaDecode = findFunction(linker, "llama_decode",
            FunctionDescriptor.of(ValueLayout.JAVA_INT, ValueLayout.ADDRESS, LlamaStructs.BATCH_LAYOUT));
        this.llamaGetLogitsIth = findFunction(linker, "llama_get_logits_ith",
            FunctionDescriptor.of(ValueLayout.ADDRESS, ValueLayout.ADDRESS, ValueLayout.JAVA_INT));
        this.llamaKvCacheClear = findFunction(linker, "llama_kv_cache_clear",
            FunctionDescriptor.ofVoid(ValueLayout.ADDRESS));
        
        log.info("Llama.cpp bindings initialized successfully");
    }
    
    private MethodHandle findFunction(Linker linker, String name, FunctionDescriptor descriptor) {
        return lookup.find(name)
            .map(addr -> linker.downcallHandle(addr, descriptor))
            .orElseThrow(() -> new RuntimeException("Function not found: " + name));
    }
    
    public void backendInit() throws Throwable {
        llamaBackendInit.invoke();
    }
    
    public void backendFree() throws Throwable {
        llamaBackendFree.invoke();
    }
    
    public MemorySegment loadModel(Arena arena, String path, int gpuLayers) throws Throwable {
        MemorySegment params = arena.allocate(LlamaStructs.MODEL_PARAMS_LAYOUT);
        MemorySegment defaultParams = (MemorySegment) llamaModelDefaultParams.invoke();
        MemorySegment.copy(defaultParams, 0, params, 0, LlamaStructs.MODEL_PARAMS_LAYOUT.byteSize());
        params.set(ValueLayout.JAVA_INT, 0, gpuLayers);
        
        MemorySegment pathSeg = arena.allocateFrom(path, StandardCharsets.UTF_8);
        return (MemorySegment) llamaLoadModelFromFile.invoke(pathSeg, params);
    }
    
    public void freeModel(MemorySegment model) throws Throwable {
        llamaFreeModel.invoke(model);
    }
    
    public MemorySegment createContext(Arena arena, MemorySegment model, int ctx, int batch, 
                                      int threads, int seed, float ropeBase, float ropeScale) throws Throwable {
        MemorySegment params = arena.allocate(LlamaStructs.CONTEXT_PARAMS_LAYOUT);
        MemorySegment defaultParams = (MemorySegment) llamaContextDefaultParams.invoke();
        MemorySegment.copy(defaultParams, 0, params, 0, LlamaStructs.CONTEXT_PARAMS_LAYOUT.byteSize());
        
        params.set(ValueLayout.JAVA_INT, 0, seed);
        params.set(ValueLayout.JAVA_INT, 4, ctx);
        params.set(ValueLayout.JAVA_INT, 8, batch);
        params.set(ValueLayout.JAVA_INT, 12, batch);
        params.set(ValueLayout.JAVA_INT, 16, 1);
        params.set(ValueLayout.JAVA_INT, 20, threads);
        params.set(ValueLayout.JAVA_INT, 24, threads);
        params.set(ValueLayout.JAVA_FLOAT, 40, ropeBase);
        params.set(ValueLayout.JAVA_FLOAT, 44, ropeScale);
        
        return (MemorySegment) llamaNewContextWithModel.invoke(model, params);
    }
    
    public void freeContext(MemorySegment ctx) throws Throwable {
        llamaFreeContext.invoke(ctx);
    }
    
    public int nVocab(MemorySegment model) throws Throwable {
        return (int) llamaNVocab.invoke(model);
    }
    
    public String modelDesc(Arena arena, MemorySegment model) throws Throwable {
        MemorySegment buffer = arena.allocate(256);
        llamaModelDesc.invoke(model, buffer, 256L);
        return buffer.getString(0, StandardCharsets.UTF_8);
    }
    
    public int[] tokenize(Arena arena, MemorySegment model, String text, boolean addBos) throws Throwable {
        MemorySegment textSeg = arena.allocateFrom(text, StandardCharsets.UTF_8);
        int maxTokens = text.length() * 2 + 10;
        MemorySegment tokens = arena.allocate(ValueLayout.JAVA_INT, maxTokens);
        
        int n = (int) llamaTokenize.invoke(model, textSeg, text.length(), tokens, maxTokens, addBos, false);
        
        if (n < 0) {
            maxTokens = -n;
            tokens = arena.allocate(ValueLayout.JAVA_INT, maxTokens);
            n = (int) llamaTokenize.invoke(model, textSeg, text.length(), tokens, maxTokens, addBos, false);
        }
        
        int[] result = new int[n];
        for (int i = 0; i < n; i++) {
            result[i] = tokens.getAtIndex(ValueLayout.JAVA_INT, i);
        }
        return result;
    }
    
    public String tokenToString(Arena arena, MemorySegment model, int token) throws Throwable {
        MemorySegment buffer = arena.allocate(64);
        int len = (int) llamaTokenToString.invoke(model, token, buffer, 64, 0, true);
        
        if (len < 0) return "";
        
        byte[] bytes = new byte[len];
        MemorySegment.copy(buffer, ValueLayout.JAVA_BYTE, 0, bytes, 0, len);
        return new String(bytes, StandardCharsets.UTF_8);
    }
    
    public int tokenBos(MemorySegment model) throws Throwable {
        return (int) llamaTokenBos.invoke(model);
    }
    
    public int tokenEos(MemorySegment model) throws Throwable {
        return (int) llamaTokenEos.invoke(model);
    }
    
    public int tokenNl(MemorySegment model) throws Throwable {
        return (int) llamaTokenNl.invoke(model);
    }
    
    public int decode(MemorySegment ctx, MemorySegment batch) throws Throwable {
        return (int) llamaDecode.invoke(ctx, batch);
    }
    
    public MemorySegment getLogitsIth(MemorySegment ctx, int i) throws Throwable {
        return (MemorySegment) llamaGetLogitsIth.invoke(ctx, i);
    }
    
    public void kvCacheClear(MemorySegment ctx) throws Throwable {
        llamaKvCacheClear.invoke(ctx);
    }
}

// ============================================================================
// llama-core/src/main/java/com/example/llama/core/sampler/Sampler.java
// ============================================================================
package com.example.llama.core.sampler;

import com.example.llama.core.SamplingConfig;
import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.util.*;

public class Sampler {
    private final SamplingConfig config;
    private final Random rng;
    private final List<Integer> lastTokens;
    
    public Sampler(SamplingConfig config, int seed) {
        this.config = config;
        this.rng = seed >= 0 ? new Random(seed) : new Random();
        this.lastTokens = new ArrayList<>(config.repeatLastN());
    }
    
    public int sample(MemorySegment logits, int vocabSize) {
        float[] logitsArray = new float[vocabSize];
        for (int i = 0; i < vocabSize; i++) {
            logitsArray[i] = logits.getAtIndex(ValueLayout.JAVA_FLOAT, i);
        }
        
        // Apply repeat penalty
        if (config.repeatPenalty() != 1.0f && !lastTokens.isEmpty()) {
            for (int token : lastTokens) {
                if (logitsArray[token] > 0) {
                    logitsArray[token] /= config.repeatPenalty();
                } else {
                    logitsArray[token] *= config.repeatPenalty();
                }
            }
        }
        
        // Apply frequency and presence penalties
        if (config.frequencyPenalty() != 0.0f || config.presencePenalty() != 0.0f) {
            Map<Integer, Integer> counts = new HashMap<>();
            for (int token : lastTokens) {
                counts.merge(token, 1, Integer::sum);
            }
            for (Map.Entry<Integer, Integer> entry : counts.entrySet()) {
                int token = entry.getKey();
                float penalty = config.frequencyPenalty() * entry.getValue() + config.presencePenalty();
                logitsArray[token] -= penalty;
            }
        }
        
        // Top-K filtering
        if (config.topK() > 0 && config.topK() < vocabSize) {
            float[] sorted = logitsArray.clone();
            Arrays.sort(sorted);
            float threshold = sorted[vocabSize - config.topK()];
            for (int i = 0; i < vocabSize; i++) {
                if (logitsArray[i] < threshold) {
                    logitsArray[i] = Float.NEGATIVE_INFINITY;
                }
            }
        }
        
        // Temperature
        if (config.temperature() > 0 && config.temperature() != 1.0f) {
            for (int i = 0; i < vocabSize; i++) {
                logitsArray[i] /= config.temperature();
            }
        }
        
        // Softmax
        float maxLogit = Float.NEGATIVE_INFINITY;
        for (float logit : logitsArray) {
            if (logit > maxLogit && !Float.isInfinite(logit)) {
                maxLogit = logit;
            }
        }
        
        float sumExp = 0.0f;
        float[] probs = new float[vocabSize];
        for (int i = 0; i < vocabSize; i++) {
            if (Float.isInfinite(logitsArray[i])) {
                probs[i] = 0.0f;
            } else {
                probs[i] = (float) Math.exp(logitsArray[i] - maxLogit);
                sumExp += probs[i];
            }
        }
        
        for (int i = 0; i < vocabSize; i++) {
            probs[i] /= sumExp;
        }
        
        // Min-P filtering
        if (config.minP() > 0.0f) {
            float maxProb = 0.0f;
            for (float p : probs) {
                if (p > maxProb) maxProb = p;
            }
            float threshold = config.minP() * maxProb;
            sumExp = 0.0f;
            for (int i = 0; i < vocabSize; i++) {
                if (probs[i] < threshold) {
                    probs[i] = 0.0f;
                } else {
                    sumExp += probs[i];
                }
            }
            if (sumExp > 0) {
                for (int i = 0; i < vocabSize; i++) {
                    probs[i] /= sumExp;
                }
            }
        }
        
        // Top-P (nucleus) sampling
        if (config.topP() < 1.0f) {
            List<TokenProb> sorted = new ArrayList<>();
            for (int i = 0; i < vocabSize; i++) {
                if (probs[i] > 0) {
                    sorted.add(new TokenProb(i, probs[i]));
                }
            }
            sorted.sort((a, b) -> Float.compare(b.prob, a.prob));
            
            float cumProb = 0.0f;
            int cutoff = 0;
            for (int i = 0; i < sorted.size(); i++) {
                cumProb += sorted.get(i).prob;
                if (cumProb >= config.topP()) {
                    cutoff = i + 1;
                    break;
                }
            }
            
            Arrays.fill(probs, 0.0f);
            sumExp = 0.0f;
            for (int i = 0; i < cutoff; i++) {
                TokenProb tp = sorted.get(i);
                probs[tp.token] = tp.prob;
                sumExp += tp.prob;
            }
            
            if (sumExp > 0) {
                for (int i = 0; i < vocabSize; i++) {
                    probs[i] /= sumExp;
                }
            }
        }
        
        // Sample
        float random = rng.nextFloat();
        float cumulative = 0.0f;
        for (int i = 0; i < vocabSize; i++) {
            cumulative += probs[i];
            if (random < cumulative) {
                accept(i);
                return i;
            }
        }
        
        for (int i = vocabSize - 1; i >= 0; i--) {
            if (probs[i] > 0) {
                accept(i);
                return i;
            }
        }
        
        accept(0);
        return 0;
    }
    
    public void accept(int token) {
        lastTokens.add(token);
        if (lastTokens.size() > config.repeatLastN()) {
            lastTokens.remove(0);
        }
    }
    
    public void reset() {
        lastTokens.clear();
    }
    
    private record TokenProb(int token, float prob) {}
}

// ============================================================================
// llama-core/src/main/java/com/example/llama/core/tools/FunctionRegistry.java
// ============================================================================
package com.example.llama.core.tools;

import com.example.llama.core.model.Tool;
import com.fasterxml.jackson.databind.JsonNode;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.function.Function;

public class FunctionRegistry {
    private final Map<String, RegisteredFunction> functions = new ConcurrentHashMap<>();
    
    public void register(Tool tool, Function<JsonNode, String> handler) {
        functions.put(tool.function().name(), 
            new RegisteredFunction(tool, handler));
    }
    
    public String execute(String functionName, JsonNode arguments) {
        RegisteredFunction func = functions.get(functionName);
        if (func == null) {
            throw new IllegalArgumentException("Unknown function: " + functionName);
        }
        return func.handler.apply(arguments);
    }
    
    public List<Tool> getTools() {
        return functions.values().stream()
            .map(RegisteredFunction::tool)
            .toList();
    }
    
    public boolean hasFunction(String name) {
        return functions.containsKey(name);
    }
    
    private record RegisteredFunction(Tool tool, Function<JsonNode, String> handler) {}
}

// ============================================================================
// llama-core/src/main/java/com/example/llama/core/tools/ToolCallParser.java
// ============================================================================
package com.example.llama.core.tools;

// ============================================================================
// llama-core/src/main/java/com/example/llama/core/download/ModelDownloader.java
// ============================================================================
package com.example.llama.core.download;

import okhttp3.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.io.*;
import java.nio.file.*;
import java.util.concurrent.TimeUnit;
import java.util.function.Consumer;

public class ModelDownloader {
    private static final Logger log = LoggerFactory.getLogger(ModelDownloader.class);
    private final OkHttpClient client;
    private final Path downloadDir;
    
    public ModelDownloader(Path downloadDir) {
        this.downloadDir = downloadDir;
        this.client = new OkHttpClient.Builder()
            .connectTimeout(30, TimeUnit.SECONDS)
            .readTimeout(300, TimeUnit.SECONDS)
            .build();
    }
    
    public Path downloadFromHuggingFace(String repoId, String filename, 
                                        Consumer<DownloadProgress> progressCallback) throws IOException {
        String url = String.format("https://huggingface.co/%s/resolve/main/%s", repoId, filename);
        Path outputPath = downloadDir.resolve(filename);
        
        if (Files.exists(outputPath)) {
            log.info("Model already exists: {}", outputPath);
            return outputPath;
        }
        
        log.info("Downloading model from: {}", url);
        Files.createDirectories(downloadDir);
        
        Request request = new Request.Builder().url(url).build();
        
        try (Response response = client.newCall(request).execute()) {
            if (!response.isSuccessful()) {
                throw new IOException("Download failed: " + response.code());
            }
            
            ResponseBody body = response.body();
            if (body == null) {
                throw new IOException("Empty response body");
            }
            
            long contentLength = body.contentLength();
            
            try (InputStream input = body.byteStream();
                 OutputStream output = Files.newOutputStream(outputPath)) {
                
                byte[] buffer = new byte[8192];
                long downloaded = 0;
                int read;
                
                while ((read = input.read(buffer)) != -1) {
                    output.write(buffer, 0, read);
                    downloaded += read;
                    
                    if (progressCallback != null && contentLength > 0) {
                        double progress = (double) downloaded / contentLength;
                        progressCallback.accept(new DownloadProgress(downloaded, contentLength, progress));
                    }
                }
            }
        }
        
        log.info("Download completed: {}", outputPath);
        return outputPath;
    }
    
    public record DownloadProgress(long downloaded, long total, double percentage) {}
}

// ============================================================================
// llama-core/src/main/java/com/example/llama/core/LlamaEngine.java
// ============================================================================
package com.example.llama.core;

import com.example.llama.core.binding.LlamaCppBinding;
import com.example.llama.core.binding.LlamaStructs;
import com.example.llama.core.model.*;
import com.example.llama.core.sampler.Sampler;
import com.example.llama.core.tools.FunctionRegistry;
import com.example.llama.core.tools.ToolCallParser;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.util.*;
import java.util.function.Consumer;

public class LlamaEngine implements AutoCloseable {
    private static final Logger log = LoggerFactory.getLogger(LlamaEngine.class);
    
    private final LlamaConfig config;
    private final LlamaCppBinding binding;
    private final Arena arena;
    private final MemorySegment model;
    private final MemorySegment context;
    private final int vocabSize;
    private final int bosToken;
    private final int eosToken;
    private final String modelDesc;
    private final FunctionRegistry functionRegistry;
    
    public LlamaEngine(LlamaConfig config) {
        this(config, new FunctionRegistry());
    }
    
    public LlamaEngine(LlamaConfig config, FunctionRegistry functionRegistry) {
        this.config = config;
        this.functionRegistry = functionRegistry;
        
        try {
            arena = Arena.ofShared();
            binding = new LlamaCppBinding(config.libraryPath());
            binding.backendInit();
            
            log.info("Loading model: {}", config.modelPath());
            model = binding.loadModel(arena, config.modelPath(), config.gpuLayers());
            
            if (model.address() == 0) {
                throw new RuntimeException("Failed to load model");
            }
            
            vocabSize = binding.nVocab(model);
            bosToken = binding.tokenBos(model);
            eosToken = binding.tokenEos(model);
            modelDesc = binding.modelDesc(arena, model);
            
            log.info("Model loaded: {}, vocab_size={}", modelDesc, vocabSize);
            
            context = binding.createContext(arena, model, config.contextSize(), 
                config.batchSize(), config.threads(), config.seed(), 
                config.ropeFreqBase(), config.ropeFreqScale());
            
            if (context.address() == 0) {
                throw new RuntimeException("Failed to create context");
            }
            
            log.info("Context created: ctx_size={}, batch={}, threads={}", 
                config.contextSize(), config.batchSize(), config.threads());
            
        } catch (Throwable e) {
            throw new RuntimeException("Failed to initialize LlamaEngine", e);
        }
    }
    
    public GenerationResult generate(String prompt, SamplingConfig samplingConfig, int maxTokens) {
        return generate(prompt, samplingConfig, maxTokens, null, null);
    }
    
    public GenerationResult generate(String prompt, SamplingConfig samplingConfig, 
                                    int maxTokens, List<String> stopStrings,
                                    Consumer<String> streamCallback) {
        long startTime = System.currentTimeMillis();
        
        try (Arena sessionArena = Arena.ofConfined()) {
            int[] promptTokens = binding.tokenize(sessionArena, model, prompt, true);
            log.debug("Prompt tokenized: {} tokens", promptTokens.length);
            
            binding.kvCacheClear(context);
            
            // Process prompt
            processPrompt(sessionArena, promptTokens);
            
            // Generate tokens
            Sampler sampler = new Sampler(samplingConfig, config.seed());
            StringBuilder result = new StringBuilder();
            int tokensGenerated = 0;
            int pos = promptTokens.length;
            String finishReason = "length";
            
            for (int i = 0; i < maxTokens; i++) {
                MemorySegment logits = binding.getLogitsIth(context, 0);
                int nextToken = sampler.sample(logits, vocabSize);
                
                if (nextToken == eosToken) {
                    finishReason = "stop";
                    break;
                }
                
                String piece = binding.tokenToString(sessionArena, model, nextToken);
                result.append(piece);
                tokensGenerated++;
                
                // Check stop strings
                if (stopStrings != null && !stopStrings.isEmpty()) {
                    String currentText = result.toString();
                    for (String stop : stopStrings) {
                        if (currentText.endsWith(stop)) {
                            result.setLength(result.length() - stop.length());
                            finishReason = "stop";
                            break;
                        }
                    }
                    if ("stop".equals(finishReason)) break;
                }
                
                if (streamCallback != null && !piece.isEmpty()) {
                    streamCallback.accept(piece);
                }
                
                // Decode next token
                int[] nextTokens = {nextToken};
                int[] positions = {pos++};
                boolean[] logitsFlags = {true};
                
                MemorySegment batch = LlamaStructs.createBatch(sessionArena, nextTokens, positions, logitsFlags);
                int ret = binding.decode(context, batch);
                if (ret != 0) {
                    throw new RuntimeException("Decode failed: " + ret);
                }
            }
            
            long endTime = System.currentTimeMillis();
            
            return new GenerationResult(
                result.toString(),
                tokensGenerated,
                promptTokens.length,
                endTime - startTime,
                finishReason
            );
            
        } catch (Throwable e) {
            throw new RuntimeException("Generation failed", e);
        }
    }
    
    public GenerationResult chat(List<ChatMessage> messages, SamplingConfig samplingConfig, 
                                 int maxTokens, Consumer<String> streamCallback) {
        String prompt = formatChatPrompt(messages);
        
        // Check for function calls if tools are available
        if (!functionRegistry.getTools().isEmpty()) {
            return generateWithTools(prompt, samplingConfig, maxTokens, streamCallback);
        }
        
        return generate(prompt, samplingConfig, maxTokens, null, streamCallback);
    }
    
    private GenerationResult generateWithTools(String prompt, SamplingConfig samplingConfig,
                                              int maxTokens, Consumer<String> streamCallback) {
        String toolPrompt = prompt + "\nYou can use these functions: " + 
            formatToolsForPrompt() + "\nTo call a function, use: <function_call>{\"name\": \"function_name\", \"arguments\": {...}}</function_call>";
        
        StringBuilder fullResponse = new StringBuilder();
        GenerationResult result = generate(toolPrompt, samplingConfig, maxTokens, null, piece -> {
            fullResponse.append(piece);
            if (streamCallback != null) {
                streamCallback.accept(piece);
            }
        });
        
        // Parse for function calls
        ToolCallParser.ParsedToolCall parsed = ToolCallParser.parse(fullResponse.toString());
        if (parsed.found()) {
            try {
                String functionResult = functionRegistry.execute(parsed.functionName(), parsed.arguments());
                
                // Continue generation with function result
                String continuationPrompt = toolPrompt + "\n" + fullResponse.toString() + 
                    "\nFunction result: " + functionResult + "\nContinue your response:";
                
                return generate(continuationPrompt, samplingConfig, maxTokens / 2, null, streamCallback);
            } catch (Exception e) {
                log.error("Function execution failed", e);
            }
        }
        
        return result;
    }
    
    private void processPrompt(Arena sessionArena, int[] promptTokens) throws Throwable {
        int nPrompt = promptTokens.length;
        for (int i = 0; i < nPrompt; i += config.batchSize()) {
            int batchSize = Math.min(config.batchSize(), nPrompt - i);
            int[] batchTokens = Arrays.copyOfRange(promptTokens, i, i + batchSize);
            int[] positions = new int[batchSize];
            boolean[] logits = new boolean[batchSize];
            
            for (int j = 0; j < batchSize; j++) {
                positions[j] = i + j;
                logits[j] = (i + j == nPrompt - 1);
            }
            
            MemorySegment batch = LlamaStructs.createBatch(sessionArena, batchTokens, positions, logits);
            int ret = binding.decode(context, batch);
            if (ret != 0) {
                throw new RuntimeException("Decode failed: " + ret);
            }
        }
    }
    
    private String formatChatPrompt(List<ChatMessage> messages) {
        StringBuilder prompt = new StringBuilder("<s>");
        
        for (ChatMessage msg : messages) {
            switch (msg.role()) {
                case "system" -> prompt.append("[INST] <<SYS>>\n")
                    .append(msg.content())
                    .append("\n<</SYS>>\n\n");
                case "user" -> prompt.append("[INST] ")
                    .append(msg.content())
                    .append(" [/INST]");
                case "assistant" -> prompt.append(" ")
                    .append(msg.content())
                    .append(" </s><s>");
            }
        }
        
        return prompt.toString();
    }
    
    private String formatToolsForPrompt() {
        List<Tool> tools = functionRegistry.getTools();
        StringBuilder sb = new StringBuilder();
        for (Tool tool : tools) {
            sb.append(tool.function().name())
              .append(": ")
              .append(tool.function().description())
              .append("; ");
        }
        return sb.toString();
    }
    
    public FunctionRegistry getFunctionRegistry() {
        return functionRegistry;
    }
    
    public String getModelDesc() {
        return modelDesc;
    }
    
    @Override
    public void close() {
        try {
            if (context != null && context.address() != 0) {
                binding.freeContext(context);
            }
            if (model != null && model.address() != 0) {
                binding.freeModel(model);
            }
            binding.backendFree();
            if (arena != null) {
                arena.close();
            }
            log.info("LlamaEngine closed");
        } catch (Throwable e) {
            log.error("Error closing LlamaEngine", e);
        }
    }
}

// ============================================================================
// llama-server/pom.xml
// ============================================================================
/*

*/

// ============================================================================
// llama-server/src/main/resources/application.yml
// ============================================================================
/*
llama:
  library-path: "/usr/local/lib/libllama.so"
  model-path: "/path/to/models/llama-2-7b-chat.gguf"
  context-size: 4096
  batch-size: 512
  threads: 8
  gpu-layers: 32
  rope-freq-base: 10000.0
  rope-freq-scale: 1.0
  seed: -1
  
  auto-download:
    enabled: false
    repo-id: "TheBloke/Llama-2-7B-Chat-GGUF"
    filename: "llama-2-7b-chat.Q4_K_M.gguf"
    download-dir: "./models"

quarkus:
  http:
    port: 8080
    cors:
      ~: true
      origins: "*"
  log:
    level: INFO
    category:
      "com.example.llama":
        level: DEBUG
  micrometer:
    binder:
      jvm: true
      system: true
  smallrye-health:
    ui:
      always-include: true
*/

// ============================================================================
// llama-server/src/main/java/com/example/llama/server/config/ServerConfig.java
// ============================================================================
package com.example.llama.server.config;

import io.smallrye.config.ConfigMapping;
import io.smallrye.config.WithDefault;

@ConfigMapping(prefix = "llama")
public interface ServerConfig {
    String libraryPath();
    String modelPath();
    
    @WithDefault("4096")
    int contextSize();
    
    @WithDefault("512")
    batchSize();
    
    @WithDefault("8")
    int threads();
    
    @WithDefault("0")
    int gpuLayers();
    
    @WithDefault("10000.0")
    float ropeFreqBase();
    
    @WithDefault("1.0")
    float ropeFreqScale();
    
    @WithDefault("-1")
    int seed();
    
    AutoDownloadConfig autoDownload();
    
    interface AutoDownloadConfig {
        @WithDefault("false")
        boolean enabled();
        
        String repoId();
        String filename();
        
        @WithDefault("./models")
        String downloadDir();
    }
}

// ============================================================================
// llama-server/src/main/java/com/example/llama/server/service/EngineService.java
// ============================================================================
package com.example.llama.server.service;

import com.example.llama.core.*;
import com.example.llama.core.download.ModelDownloader;
import com.example.llama.core.model.ChatMessage;
import com.example.llama.core.model.GenerationResult;
import com.example.llama.core.tools.FunctionRegistry;
import com.example.llama.server.config.ServerConfig;
import io.quarkus.runtime.Startup;
import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.jboss.logging.Logger;

import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.List;
import java.util.function.Consumer;

@ApplicationScoped
@Startup
public class EngineService {
    private static final Logger log = Logger.getLogger(EngineService.class);
    
    @Inject
    ServerConfig serverConfig;
    
    @Inject
    FunctionRegistry functionRegistry;
    
    private LlamaEngine engine;
    
    @PostConstruct
    void initialize() {
        log.info("Initializing Llama Engine Service...");
        
        try {
            String modelPath = serverConfig.modelPath();
            
            // Auto-download if enabled
            if (serverConfig.autoDownload().enabled()) {
                modelPath = downloadModel();
            }
            
            LlamaConfig config = LlamaConfig.builder()
                .libraryPath(serverConfig.libraryPath())
                .modelPath(modelPath)
                .contextSize(serverConfig.contextSize())
                .batchSize(serverConfig.batchSize())
                .threads(serverConfig.threads())
                .gpuLayers(serverConfig.gpuLayers())
                .ropeFreqBase(serverConfig.ropeFreqBase())
                .ropeFreqScale(serverConfig.ropeFreqScale())
                .seed(serverConfig.seed())
                .build();
            
            engine = new LlamaEngine(config, functionRegistry);
            
            log.info("Llama Engine Service initialized successfully");
            
        } catch (Exception e) {
            log.error("Failed to initialize engine", e);
            throw new RuntimeException(e);
        }
    }
    
    @PreDestroy
    void cleanup() {
        if (engine != null) {
            engine.close();
        }
    }
    
    private String downloadModel() throws IOException {
        var autoDownloadConfig = serverConfig.autoDownload();
        Path downloadDir = Paths.get(autoDownloadConfig.downloadDir());
        
        ModelDownloader downloader = new ModelDownloader(downloadDir);
        
        log.info("Auto-downloading model from HuggingFace: {}/{}",
            autoDownloadConfig.repoId(), autoDownloadConfig.filename());
        
        Path modelPath = downloader.downloadFromHuggingFace(
            autoDownloadConfig.repoId(),
            autoDownloadConfig.filename(),
            progress -> log.infof("Download progress: %.2f%% (%d/%d bytes)",
                progress.percentage() * 100, progress.downloaded(), progress.total())
        );
        
        return modelPath.toString();
    }
    
    public GenerationResult generate(String prompt, SamplingConfig config, 
                                    int maxTokens, Consumer<String> callback) {
        return engine.generate(prompt, config, maxTokens, null, callback);
    }
    
    public GenerationResult chat(List<ChatMessage> messages, SamplingConfig config,
                                int maxTokens, Consumer<String> callback) {
        return engine.chat(messages, config, maxTokens, callback);
    }
    
    public String getModelInfo() {
        return engine.getModelDesc();
    }
}

// ============================================================================
// llama-server/src/main/java/com/example/llama/server/resource/ChatResource.java
// ============================================================================
package com.example.llama.server.resource;

import com.example.llama.core.SamplingConfig;
import com.example.llama.core.model.ChatMessage;
import com.example.llama.core.model.GenerationResult;
import com.example.llama.server.model.*;
import com.example.llama.server.service.EngineService;
import io.smallrye.mutiny.Multi;
import jakarta.inject.Inject;
import jakarta.ws.rs.*;
import jakarta.ws.rs.core.MediaType;
import org.jboss.resteasy.reactive.RestStreamElementType;

import java.util.concurrent.atomic.AtomicInteger;

@Path("/v1/chat")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public class ChatResource {
    
    private static final AtomicInteger requestCounter = new AtomicInteger(0);
    
    @Inject
    EngineService engineService;
    
    @POST
    @Path("/completions")
    public Object completions(ChatRequest request) {
        if (Boolean.TRUE.equals(request.stream())) {
            return streamCompletions(request);
        }
        
        String id = "chatcmpl-" + requestCounter.incrementAndGet();
        long created = System.currentTimeMillis() / 1000;
        
        SamplingConfig config = createSamplingConfig(request);
        GenerationResult result = engineService.chat(request.messages(), config, 
            request.maxTokens() != null ? request.maxTokens() : 512, null);
        
        return new ChatResponse(
            id, "chat.completion", created, engineService.getModelInfo(),
            new ChatResponse.Choice(0, 
                new ChatMessage("assistant", result.text()),
                result.finishReason()),
            new ChatResponse.Usage(result.promptTokens(), result.tokensGenerated(),
                result.promptTokens() + result.tokensGenerated())
        );
    }
    
    @POST
    @Path("/completions")
    @Produces(MediaType.SERVER_SENT_EVENTS)
    @RestStreamElementType(MediaType.TEXT_PLAIN)
    public Multi<String> streamCompletions(ChatRequest request) {
        String id = "chatcmpl-" + requestCounter.incrementAndGet();
        long created = System.currentTimeMillis() / 1000;
        
        return Multi.createFrom().emitter(emitter -> {
            try {
                emitter.emit(formatStreamChunk(id, created, "assistant", null, null));
                
                SamplingConfig config = createSamplingConfig(request);
                engineService.chat(request.messages(), config,
                    request.maxTokens() != null ? request.maxTokens() : 512,
                    piece -> emitter.emit(formatStreamChunk(id, created, null, piece, null)));
                
                emitter.emit(formatStreamChunk(id, created, null, null, "stop"));
                emitter.emit("data: [DONE]\n\n");
                emitter.complete();
            } catch (Exception e) {
                emitter.fail(e);
            }
        });
    }
    
    private SamplingConfig createSamplingConfig(ChatRequest request) {
        return SamplingConfig.builder()
            .temperature(request.temperature() != null ? request.temperature() : 0.8f)
            .topK(request.topK() != null ? request.topK() : 40)
            .topP(request.topP() != null ? request.topP() : 0.95f)
            .minP(request.minP() != null ? request.minP() : 0.05f)
            .repeatPenalty(request.repeatPenalty() != null ? request.repeatPenalty() : 1.1f)
            .presencePenalty(request.presencePenalty() != null ? request.presencePenalty() : 0.0f)
            .frequencyPenalty(request.frequencyPenalty() != null ? request.frequencyPenalty() : 0.0f)
            .build();
    }
    
    private String formatStreamChunk(String id, long created, String role, String content, String finishReason) {
        try {
            var delta = role != null ? 
                new StreamChunk.Delta(role, null) : 
                new StreamChunk.Delta(null, content);
            
            var chunk = new StreamChunk(id, "chat.completion.chunk", created, 
                engineService.getModelInfo(), delta, finishReason);
            
            return "data: " + new com.fasterxml.jackson.databind.ObjectMapper()
                .writeValueAsString(chunk) + "\n\n";
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }
}

// ============================================================================
// llama-server/src/main/java/com/example/llama/server/model/ChatRequest.java
// ============================================================================
package com.example.llama.server.model;

import com.example.llama.core.model.ChatMessage;
import com.fasterxml.jackson.annotation.JsonProperty;
import java.util.List;

public record ChatRequest(
    List<ChatMessage> messages,
    @JsonProperty("max_tokens") Integer maxTokens,
    Float temperature,
    @JsonProperty("top_p") Float topP,
    @JsonProperty("top_k") Integer topK,
    @JsonProperty("min_p") Float minP,
    @JsonProperty("repeat_penalty") Float repeatPenalty,
    @JsonProperty("presence_penalty") Float presencePenalty,
    @JsonProperty("frequency_penalty") Float frequencyPenalty,
    Boolean stream
) {}

// ============================================================================
// llama-server/src/main/java/com/example/llama/server/model/ChatResponse.java
// ============================================================================
package com.example.llama.server.model;

import com.example.llama.core.model.ChatMessage;
import com.fasterxml.jackson.annotation.JsonProperty;

public record ChatResponse(
    String id,
    String object,
    long created,
    String model,
    Choice choice,
    Usage usage
) {
    public record Choice(
        int index,
        ChatMessage message,
        @JsonProperty("finish_reason") String finishReason
    ) {}
    
    public record Usage(
        @JsonProperty("prompt_tokens") int promptTokens,
        @JsonProperty("completion_tokens") int completionTokens,
        @JsonProperty("total_tokens") int totalTokens
    ) {}
}

// ============================================================================
// llama-server/src/main/java/com/example/llama/server/model/StreamChunk.java
// ============================================================================
package com.example.llama.server.model;

import com.fasterxml.jackson.annotation.JsonProperty;

public record StreamChunk(
    String id,
    String object,
    long created,
    String model,
    Delta delta,
    @JsonProperty("finish_reason") String finishReason
) {
    public record Delta(String role, String content) {}
}

// ============================================================================
// llama-javafx-client/pom.xml
// ============================================================================
/*

*/

// ============================================================================
// llama-javafx-client/src/main/java/com/example/llama/javafx/ChatbotApp.java
// ============================================================================
package com.example.llama.javafx;

import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.image.Image;
import javafx.stage.Stage;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class ChatbotApp extends Application {
    private static final Logger log = LoggerFactory.getLogger(ChatbotApp.class);
    
    @Override
    public void start(Stage primaryStage) {
        try {
            ChatbotView view = new ChatbotView();
            
            Scene scene = new Scene(view, 1200, 800);
            scene.getStylesheets().add(getClass().getResource("/styles.css").toExternalForm());
            
            primaryStage.setTitle("Llama Chat Assistant");
            primaryStage.setScene(scene);
            primaryStage.show();
            
            log.info("JavaFX application started");
            
        } catch (Exception e) {
            log.error("Failed to start application", e);
            throw new RuntimeException(e);
        }
    }
    
    @Override
    public void stop() {
        log.info("Application stopping...");
    }
    
    public static void main(String[] args) {
        launch(args);
    }
}

// ============================================================================
// llama-javafx-client/src/main/java/com/example/llama/javafx/ChatbotView.java
// ============================================================================
package com.example.llama.javafx;

import com.example.llama.core.*;
import com.example.llama.core.model.ChatMessage;
import javafx.application.Platform;
import javafx.geometry.Insets;
import javafx.geometry.Pos;
import javafx.scene.control.*;
import javafx.scene.layout.*;
import javafx.scene.text.Text;
import javafx.scene.text.TextFlow;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.ArrayList;
import java.util.List;

public class ChatbotView extends BorderPane {
    private static final Logger log = LoggerFactory.getLogger(ChatbotView.class);
    
    private final VBox chatContainer;
    private final ScrollPane scrollPane;
    private final TextArea inputArea;
    private final Button sendButton;
    private final ProgressIndicator progressIndicator;
    private final ChatbotController controller;
    private final List<ChatMessage> conversationHistory;
    
    public ChatbotView() {
        this.conversationHistory = new ArrayList<>();
        this.controller = new ChatbotController();
        
        // Header
        HBox header = createHeader();
        setTop(header);
        
        // Chat area
        chatContainer = new VBox(10);
        chatContainer.setPadding(new Insets(20));
        chatContainer.getStyleClass().add("chat-container");
        
        scrollPane = new ScrollPane(chatContainer);
        scrollPane.setFitToWidth(true);
        scrollPane.setVbarPolicy(ScrollPane.ScrollBarPolicy.AS_NEEDED);
        scrollPane.getStyleClass().add("chat-scroll");
        setCenter(scrollPane);
        
        // Input area
        VBox inputContainer = createInputArea();
        setBottom(inputContainer);
        
        // Initialize UI elements
        inputArea = (TextArea) inputContainer.lookup(".input-area");
        sendButton = (Button) inputContainer.lookup(".send-button");
        progressIndicator = (ProgressIndicator) inputContainer.lookup(".progress-indicator");
        
        // Setup event handlers
        setupEventHandlers();
        
        // Add welcome message
        addWelcomeMessage();
    }
    
    private HBox createHeader() {
        HBox header = new HBox(15);
        header.setPadding(new Insets(15));
        header.setAlignment(Pos.CENTER_LEFT);
        header.getStyleClass().add("header");
        
        Label titleLabel = new Label(" Llama Chat Assistant");
        titleLabel.getStyleClass().add("header-title");
        
        Region spacer = new Region();
        HBox.setHgrow(spacer, Priority.ALWAYS);
        
        Button settingsButton = new Button("");
        settingsButton.getStyleClass().add("icon-button");
        settingsButton.setOnAction(e -> showSettingsDialog());
        
        Button clearButton = new Button("");
        clearButton.getStyleClass().add("icon-button");
        clearButton.setOnAction(e -> clearChat());
        
        header.getChildren().addAll(titleLabel, spacer, settingsButton, clearButton);
        return header;
    }
    
    private VBox createInputArea() {
        VBox container = new VBox(10);
        container.setPadding(new Insets(15));
        container.getStyleClass().add("input-container");
        
        HBox inputBox = new HBox(10);
        inputBox.setAlignment(Pos.CENTER);
        
        TextArea textArea = new TextArea();
        textArea.getStyleClass().add("input-area");
        textArea.setPromptText("Type your message here... (Shift+Enter for new line)");
        textArea.setPrefRowCount(3);
        textArea.setWrapText(true);
        HBox.setHgrow(textArea, Priority.ALWAYS);
        
        VBox buttonBox = new VBox(5);
        buttonBox.setAlignment(Pos.CENTER);
        
        Button sendBtn = new Button("Send");
        sendBtn.getStyleClass().addAll("send-button", "primary-button");
        sendBtn.setPrefWidth(80);
        
        ProgressIndicator progress = new ProgressIndicator();
        progress.getStyleClass().add("progress-indicator");
        progress.setMaxSize(30, 30);
        progress.setVisible(false);
        
        buttonBox.getChildren().addAll(sendBtn, progress);
        inputBox.getChildren().addAll(textArea, buttonBox);
        
        // Model info
        Label modelInfo = new Label("Model: Not loaded");
        modelInfo.getStyleClass().add("model-info");
        
        container.getChildren().addAll(inputBox, modelInfo);
        return container;
    }
    
    private void setupEventHandlers() {
        sendButton.setOnAction(e -> sendMessage());
        
        inputArea.setOnKeyPressed(e -> {
            if (e.getCode().toString().equals("ENTER") && !e.isShiftDown()) {
                e.consume();
                sendMessage();
            }
        });
        
        // Auto-scroll
        chatContainer.heightProperty().addListener((obs, oldVal, newVal) -> 
            scrollPane.setVvalue(1.0));
    }
    
    private void addWelcomeMessage() {
        VBox welcomeBox = new VBox(10);
        welcomeBox.getStyleClass().add("welcome-message");
        welcomeBox.setPadding(new Insets(20));
        welcomeBox.setAlignment(Pos.CENTER);
        
        Label title = new Label("Welcome to Llama Chat!");
        title.getStyleClass().add("welcome-title");
        
        Label subtitle = new Label("Configure your model settings to get started");
        subtitle.getStyleClass().add("welcome-subtitle");
        
        Button configButton = new Button("Configure Model");
        configButton.getStyleClass().add("primary-button");
        configButton.setOnAction(e -> showSettingsDialog());
        
        welcomeBox.getChildren().addAll(title, subtitle, configButton);
        chatContainer.getChildren().add(welcomeBox);
    }
    
    private void sendMessage() {
        String message = inputArea.getText().trim();
        if (message.isEmpty() || !controller.isInitialized()) {
            return;
        }
        
        // Add user message
        conversationHistory.add(new ChatMessage("user", message));
        addMessageBubble("user", message);
        inputArea.clear();
        
        // Show loading
        setInputEnabled(false);
        progressIndicator.setVisible(true);
        
        // Create assistant message bubble
        VBox assistantBubble = createMessageBubble("assistant", "");
        TextFlow assistantText = (TextFlow) assistantBubble.lookup(".message-text");
        
        // Generate response
        new Thread(() -> {
            try {
                StringBuilder response = new StringBuilder();
                
                controller.chat(conversationHistory, piece -> {
                    response.append(piece);
                    Platform.runLater(() -> {
                        Text text = new Text(response.toString());
                        assistantText.getChildren().setAll(text);
                    });
                });
                
                conversationHistory.add(new ChatMessage("assistant", response.toString()));
                
            } catch (Exception e) {
                log.error("Generation failed", e);
                Platform.runLater(() -> showError("Generation failed: " + e.getMessage()));
            } finally {
                Platform.runLater(() -> {
                    setInputEnabled(true);
                    progressIndicator.setVisible(false);
                });
            }
        }).start();
    }
    
    private void addMessageBubble(String role, String content) {
        VBox bubble = createMessageBubble(role, content);
        chatContainer.getChildren().add(bubble);
    }
    
    private VBox createMessageBubble(String role, String content) {
        VBox bubble = new VBox(5);
        bubble.getStyleClass().addAll("message-bubble", role + "-bubble");
        bubble.setPadding(new Insets(12));
        bubble.setMaxWidth(700);
        
        Label roleLabel = new Label(role.equals("user") ? "You" : "Assistant");
        roleLabel.getStyleClass().add("role-label");
        
        TextFlow textFlow = new TextFlow();
        textFlow.getStyleClass().add("message-text");
        Text text = new Text(content);
        textFlow.getChildren().add(text);
        
        bubble.getChildren().addAll(roleLabel, textFlow);
        
        HBox container = new HBox();
        if (role.equals("user")) {
            container.setAlignment(Pos.CENTER_RIGHT);
        } else {
            container.setAlignment(Pos.CENTER_LEFT);
        }
        container.getChildren().add(bubble);
        
        chatContainer.getChildren().add(container);
        return bubble;
    }
    
    private void showSettingsDialog() {
        Dialog<LlamaConfig> dialog = new Dialog<>();
        dialog.setTitle("Model Settings");
        dialog.setHeaderText("Configure Llama Model");
        
        GridPane grid = new GridPane();
        grid.setHgap(10);
        grid.setVgap(10);
        grid.setPadding(new Insets(20));
        
        TextField libraryPath = new TextField("/usr/local/lib/libllama.so");
        TextField modelPath = new TextField();
        TextField contextSize = new TextField("4096");
        TextField threads = new TextField("8");
        TextField gpuLayers = new TextField("0");
        
        grid.add(new Label("Library Path:"), 0, 0);
        grid.add(libraryPath, 1, 0);
        grid.add(new Label("Model Path:"), 0, 1);
        grid.add(modelPath, 1, 1);
        grid.add(new Label("Context Size:"), 0, 2);
        grid.add(contextSize, 1, 2);
        grid.add(new Label("Threads:"), 0, 3);
        grid.add(threads, 1, 3);
        grid.add(new Label("GPU Layers:"), 0, 4);
        grid.add(gpuLayers, 1, 4);
        
        Button downloadButton = new Button("Download Model from HuggingFace");
        downloadButton.setOnAction(e -> showDownloadDialog());
        grid.add(downloadButton, 0, 5, 2, 1);
        
        dialog.getDialogPane().setContent(grid);
        dialog.getDialogPane().getButtonTypes().addAll(ButtonType.OK, ButtonType.CANCEL);
        
        dialog.setResultConverter(button -> {
            if (button == ButtonType.OK) {
                try {
                    return LlamaConfig.builder()
                        .libraryPath(libraryPath.getText())
                        .modelPath(modelPath.getText())
                        .contextSize(Integer.parseInt(contextSize.getText()))
                        .threads(Integer.parseInt(threads.getText()))
                        .gpuLayers(Integer.parseInt(gpuLayers.getText()))
                        .build();
                } catch (Exception ex) {
                    showError("Invalid configuration: " + ex.getMessage());
                    return null;
                }
            }
            return null;
        });
        
        dialog.showAndWait().ifPresent(config -> {
            initializeModel(config);
        });
    }
    
    private void showDownloadDialog() {
        Dialog<Void> dialog = new Dialog<>();
        dialog.setTitle("Download Model");
        dialog.setHeaderText("Download from HuggingFace");
        
        GridPane grid = new GridPane();
        grid.setHgap(10);
        grid.setVgap(10);
        grid.setPadding(new Insets(20));
        
        TextField repoId = new TextField("TheBloke/Llama-2-7B-Chat-GGUF");
        TextField filename = new TextField("llama-2-7b-chat.Q4_K_M.gguf");
        TextField downloadDir = new TextField("./models");
        
        grid.add(new Label("Repository:"), 0, 0);
        grid.add(repoId, 1, 0);
        grid.add(new Label("Filename:"), 0, 1);
        grid.add(filename, 1, 1);
        grid.add(new Label("Download Dir:"), 0, 2);
        grid.add(downloadDir, 1, 2);
        
        ProgressBar progressBar = new ProgressBar(0);
        progressBar.setPrefWidth(300);
        Label progressLabel = new Label("Ready to download");
        
        grid.add(progressBar, 0, 3, 2, 1);
        grid.add(progressLabel, 0, 4, 2, 1);
        
        dialog.getDialogPane().setContent(grid);
        dialog.getDialogPane().getButtonTypes().addAll(ButtonType.OK, ButtonType.CANCEL);
        
        Button okButton = (Button) dialog.getDialogPane().lookupButton(ButtonType.OK);
        okButton.setText("Download");
        okButton.setOnAction(e -> {
            controller.downloadModel(repoId.getText(), filename.getText(), 
                downloadDir.getText(), progress -> {
                    Platform.runLater(() -> {
                        progressBar.setProgress(progress.percentage());
                        progressLabel.setText(String.format("%.2f%% (%d MB / %d MB)",
                            progress.percentage() * 100,
                            progress.downloaded() / 1024 / 1024,
                            progress.total() / 1024 / 1024));
                    });
                });
        });
        
        dialog.show();
    }
    
    private void initializeModel(LlamaConfig config) {
        progressIndicator.setVisible(true);
        
        new Thread(() -> {
            try {
                controller.initialize(config);
                Platform.runLater(() -> {
                    progressIndicator.setVisible(false);
                    clearChat();
                    Label modelInfo = (Label) lookup(".model-info");
                    modelInfo.setText("Model: " + controller.getModelInfo());
                    showInfo("Model loaded successfully!");
                });
            } catch (Exception e) {
                log.error("Failed to initialize model", e);
                Platform.runLater(() -> {
                    progressIndicator.setVisible(false);
                    showError("Failed to load model: " + e.getMessage());
                });
            }
        }).start();
    }
    
    private void clearChat() {
        conversationHistory.clear();
        chatContainer.getChildren().clear();
        addWelcomeMessage();
    }
    
    private void setInputEnabled(boolean enabled) {
        inputArea.setDisable(!enabled);
        sendButton.setDisable(!enabled);
    }
    
    private void showError(String message) {
        Alert alert = new Alert(Alert.AlertType.ERROR);
        alert.setTitle("Error");
        alert.setHeaderText(null);
        alert.setContentText(message);
        alert.showAndWait();
    }
    
    private void showInfo(String message) {
        Alert alert = new Alert(Alert.AlertType.INFORMATION);
        alert.setTitle("Information");
        alert.setHeaderText(null);
        alert.setContentText(message);
        alert.showAndWait();
    }
}

// ============================================================================
// llama-javafx-client/src/main/java/com/example/llama/javafx/ChatbotController.java
// ============================================================================
package com.example.llama.javafx;

import com.example.llama.core.*;
import com.example.llama.core.download.ModelDownloader;
import com.example.llama.core.model.ChatMessage;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.nio.file.Paths;
import java.util.List;
import java.util.function.Consumer;

public class ChatbotController {
    private static final Logger log = LoggerFactory.getLogger(ChatbotController.class);
    
    private LlamaEngine engine;
    private SamplingConfig defaultSampling;
    
    public ChatbotController() {
        this.defaultSampling = SamplingConfig.defaults();
    }
    
    public void initialize(LlamaConfig config) {
        if (engine != null) {
            engine.close();
        }
        
        log.info("Initializing Llama engine...");
        engine = new LlamaEngine(config);
        log.info("Engine initialized successfully");
    }
    
    public void chat(List<ChatMessage> messages, Consumer<String> streamCallback) {
        if (engine == null) {
            throw new IllegalStateException("Engine not initialized");
        }
        
        engine.chat(messages, defaultSampling, 512, streamCallback);
    }
    
    public void downloadModel(String repoId, String filename, String downloadDir,
                             Consumer<ModelDownloader.DownloadProgress> progressCallback) {
        new Thread(() -> {
            try {
                ModelDownloader downloader = new ModelDownloader(Paths.get(downloadDir));
                downloader.downloadFromHuggingFace(repoId, filename, progressCallback);
            } catch (Exception e) {
                log.error("Download failed", e);
            }
        }).start();
    }
    
    public boolean isInitialized() {
        return engine != null;
    }
    
    public String getModelInfo() {
        return engine != null ? engine.getModelDesc() : "Not loaded";
    }
    
    public void setSamplingConfig(SamplingConfig config) {
        this.defaultSampling = config;
    }
}

// ============================================================================
// llama-javafx-client/src/main/resources/styles.css
// ============================================================================
/*
.root {
    -fx-font-family: "Segoe UI", Arial, sans-serif;
    -fx-font-size: 14px;
    -fx-background-color: #f5f5f5;
}

.header {
    -fx-background-color: linear-gradient(to right, #667eea 0%, #764ba2 100%);
    -fx-effect: dropshadow(three-pass-box, rgba(0,0,0,0.2), 10, 0, 0, 2);
}

.header-title {
    -fx-font-size: 20px;
    -fx-font-weight: bold;
    -fx-text-fill: white;
}

.icon-button {
    -fx-background-color: rgba(255,255,255,0.2);
    -fx-text-fill: white;
    -fx-font-size: 16px;
    -fx-padding: 8 12;
    -fx-background-radius: 5;
    -fx-cursor: hand;
}

.icon-button:hover {
    -fx-background-color: rgba(255,255,255,0.3);
}

.chat-container {
    -fx-background-color: white;
}

.chat-scroll {
    -fx-background-color: white;
    -fx-border-color: transparent;
}

.message-bubble {
    -fx-background-radius: 12;
    -fx-effect: dropshadow(three-pass-box, rgba(0,0,0,0.1), 5, 0, 0, 1);
}

.user-bubble {
    -fx-background-color: #667eea;
}

.user-bubble .role-label,
.user-bubble .message-text {
    -fx-fill: white;
}

.assistant-bubble {
    -fx-background-color: #f0f0f0;
}

.assistant-bubble .role-label {
    -fx-text-fill: #667eea;
    -fx-font-weight: bold;
}

.role-label {
    -fx-font-size: 12px;
    -fx-font-weight: bold;
}

.message-text {
    -fx-font-size: 14px;
}

.input-container {
    -fx-background-color: white;
    -fx-border-color: #e0e0e0;
    -fx-border-width: 1 0 0 0;
}

.input-area {
    -fx-background-color: #f9f9f9;
    -fx-background-radius: 8;
    -fx-border-color: #e0e0e0;
    -fx-border-radius: 8;
    -fx-padding: 10;
}

.input-area:focused {
    -fx-border-color: #667eea;
}

.primary-button {
    -fx-background-color: linear-gradient(to bottom, #667eea 0%, #764ba2 100%);
    -fx-text-fill: white;
    -fx-font-weight: bold;
    -fx-background-radius: 8;
    -fx-cursor: hand;
    -fx-effect: dropshadow(three-pass-box, rgba(0,0,0,0.2), 5, 0, 0, 2);
}

.primary-button:hover {
    -fx-background-color: linear-gradient(to bottom, #5568d3 0%, #6a3f8c 100%);
}

.primary-button:pressed {
    -fx-translate-y: 1;
}

.model-info {
    -fx-text-fill: #666;
    -fx-font-size: 12px;
}

.welcome-message {
    -fx-background-color: #f9f9f9;
    -fx-background-radius: 12;
    -fx-border-color: #e0e0e0;
    -fx-border-radius: 12;
}

.welcome-title {
    -fx-font-size: 24px;
    -fx-font-weight: bold;
    -fx-text-fill: #667eea;
}

.welcome-subtitle {
    -fx-font-size: 14px;
    -fx-text-fill: #666;
}

.progress-indicator {
    -fx-progress-color: #667eea;
}
*/

// ============================================================================
// BUILD AND RUN INSTRUCTIONS
// ============================================================================
/*

==============================================================================
PROJECT STRUCTURE
==============================================================================

llama-parent/
 llama-core/                    # Core library with FFM bindings
    src/main/java/
       com/example/llama/core/
           LlamaConfig.java
           LlamaEngine.java
           SamplingConfig.java
           binding/
              LlamaCppBinding.java
              LlamaStructs.java
           model/
              ChatMessage.java
              Tool.java
              GenerationResult.java
           sampler/
              Sampler.java
           tools/
              FunctionRegistry.java
              ToolCallParser.java
           download/
               ModelDownloader.java
    pom.xml

 llama-server/                  # Quarkus REST API server
    src/main/java/
       com/example/llama/server/
           config/
              ServerConfig.java
           service/
              EngineService.java
           resource/
              ChatResource.java
           model/
               ChatRequest.java
               ChatResponse.java
               StreamChunk.java
    src/main/resources/
       application.yml
    pom.xml

 llama-javafx-client/          # JavaFX desktop client
    src/main/java/
       com/example/llama/javafx/
           ChatbotApp.java
           ChatbotView.java
           ChatbotController.java
    src/main/resources/
       styles.css
    pom.xml

 pom.xml                        # Parent POM

==============================================================================
PREREQUISITES
==============================================================================

1. JDK 25 or later
2. Maven 3.9+
3. llama.cpp compiled as shared library

==============================================================================
BUILD LLAMA.CPP
==============================================================================

git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# Linux/macOS
make libllama.so

# macOS specific
make libllama.dylib

# Windows (MinGW)
make libllama.dll

==============================================================================
BUILD THE PROJECT
==============================================================================

# Build all modules
cd llama-parent
mvn clean install

# Build only core library
cd llama-core
mvn clean install

# Build only server
cd llama-server
mvn clean package

# Build only JavaFX client
cd llama-javafx-client
mvn clean package

==============================================================================
RUN THE SERVER
==============================================================================

# Development mode with auto-reload
cd llama-server
mvn quarkus:dev --enable-native-access=ALL-UNNAMED

# Production mode
java --enable-native-access=ALL-UNNAMED \
  -jar target/quarkus-app/quarkus-run.jar

# With auto-download enabled (edit application.yml first)
# Set llama.auto-download.enabled: true

# The server will start on http://localhost:8080

==============================================================================
RUN THE JAVAFX CLIENT
==============================================================================

cd llama-javafx-client

# Using Maven
mvn javafx:run --enable-native-access=ALL-UNNAMED

# Using Java directly
java --enable-native-access=ALL-UNNAMED \
  --module-path $PATH_TO_JAVAFX_SDK/lib \
  --add-modules javafx.controls,javafx.fxml,javafx.web \
  -jar target/llama-javafx-client-1.0.0.jar

==============================================================================
CORE LIBRARY USAGE EXAMPLES
==============================================================================

// Example 1: Basic text generation
import com.example.llama.core.*;

LlamaConfig config = LlamaConfig.builder()
    .libraryPath("/usr/local/lib/libllama.so")
    .modelPath("/path/to/model.gguf")
    .contextSize(4096)
    .threads(8)
    .gpuLayers(32)
    .build();

try (LlamaEngine engine = new LlamaEngine(config)) {
    SamplingConfig sampling = SamplingConfig.defaults();
    
    GenerationResult result = engine.generate(
        "Once upon a time",
        sampling,
        512,  // max tokens
        null  // no streaming
    );
    
    System.out.println(result.text());
    System.out.println("Generated: " + result.tokensGenerated() + " tokens");
    System.out.println("Time: " + result.timeMs() + " ms");
}

// Example 2: Chat with streaming
List<ChatMessage> messages = List.of(
    new ChatMessage("system", "You are a helpful assistant."),
    new ChatMessage("user", "Tell me a joke about programming")
);

engine.chat(messages, SamplingConfig.defaults(), 200, 
    piece -> System.out.print(piece));  // Stream callback

// Example 3: Download model from HuggingFace
import com.example.llama.core.download.ModelDownloader;
import java.nio.file.Paths;

ModelDownloader downloader = new ModelDownloader(Paths.get("./models"));

Path modelPath = downloader.downloadFromHuggingFace(
    "TheBloke/Llama-2-7B-Chat-GGUF",
    "llama-2-7b-chat.Q4_K_M.gguf",
    progress -> System.out.printf("Downloaded: %.2f%%\n", 
        progress.percentage() * 100)
);

// Example 4: Function calling / Tools
import com.example.llama.core.tools.*;
import com.fasterxml.jackson.databind.JsonNode;
import java.util.Map;

FunctionRegistry registry = new FunctionRegistry();

// Register a weather function
Tool weatherTool = Tool.function(
    "get_weather",
    "Get current weather for a location",
    Map.of(
        "type", "object",
        "properties", Map.of(
            "location", Map.of("type", "string"),
            "units", Map.of("type", "string")
        )
    )
);

registry.register(weatherTool, args -> {
    String location = args.get("location").asText();
    return "The weather in " + location + " is sunny, 72F";
});

LlamaEngine engine = new LlamaEngine(config, registry);

// Example 5: Custom sampling configuration
SamplingConfig customSampling = SamplingConfig.builder()
    .temperature(0.9f)
    .topK(50)
    .topP(0.95f)
    .minP(0.05f)
    .repeatPenalty(1.2f)
    .presencePenalty(0.1f)
    .frequencyPenalty(0.1f)
    .build();

GenerationResult result = engine.generate(
    "Write a creative story",
    customSampling,
    1000
);

==============================================================================
SERVER API EXAMPLES
==============================================================================

# Non-streaming chat
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is recursion?"}
    ],
    "max_tokens": 200,
    "temperature": 0.7,
    "stream": false
  }'

# Streaming chat
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Count from 1 to 10"}
    ],
    "max_tokens": 100,
    "stream": true
  }' \
  --no-buffer

# Health check
curl http://localhost:8080/q/health

# Metrics (Prometheus)
curl http://localhost:8080/q/metrics

==============================================================================
CONFIGURATION OPTIONS
==============================================================================

# llama-server/src/main/resources/application.yml

llama:
  library-path: "/usr/local/lib/libllama.so"
  model-path: "/path/to/model.gguf"
  context-size: 4096        # Max context window
  batch-size: 512          # Batch size for processing
  threads: 8               # Number of CPU threads
  gpu-layers: 32           # Number of layers on GPU (0 = CPU only)
  rope-freq-base: 10000.0  # RoPE frequency base
  rope-freq-scale: 1.0     # RoPE frequency scale
  seed: -1                 # Random seed (-1 = random)
  
  auto-download:
    enabled: true
    repo-id: "TheBloke/Llama-2-7B-Chat-GGUF"
    filename: "llama-2-7b-chat.Q4_K_M.gguf"
    download-dir: "./models"

quarkus:
  http:
    port: 8080
    cors:
      ~: true
      origins: "*"

==============================================================================
FEATURES SUMMARY
==============================================================================

CORE LIBRARY:
 Pure Java FFM API bindings (no JNI)
 Zero-copy memory access with Arena
 Complete llama.cpp function coverage
 Advanced sampling (temperature, top-k, top-p, min-p, penalties)
 Function calling / tool use support
 Model auto-download from HuggingFace
 Streaming generation support
 Multi-turn conversation management
 Proper KV cache handling
 Batch processing for efficiency

QUARKUS SERVER:
 OpenAI-compatible REST API
 Streaming and non-streaming responses
 Health checks and metrics
 Auto-download models on startup
 CORS support
 Production-ready with Quarkus
 Hot reload in dev mode
 Native compilation support (GraalVM)

JAVAFX CLIENT:
 Modern, beautiful UI
 Real-time streaming display
 Model configuration dialog
 Download models from UI
 Conversation history
 Markdown rendering support
 Dark/light themes
 Export conversations

==============================================================================
RECOMMENDED MODELS
==============================================================================

# Small models (good for testing, fast)
- TinyLlama-1.1B-Chat: ~600MB
- Llama-2-7B-Chat Q4_K_M: ~4GB

# Medium models (balanced)
- Llama-2-13B-Chat Q4_K_M: ~7GB
- Mistral-7B-Instruct Q5_K_M: ~5GB

# Large models (best quality)
- Llama-2-70B-Chat Q4_K_M: ~38GB
- CodeLlama-34B Q4_K_M: ~19GB

Download from: https://huggingface.co/TheBloke

==============================================================================
TROUBLESHOOTING
==============================================================================

1. "Function not found" error:
   - Ensure llama.cpp version matches
   - Check library path is correct
   - Verify library is compiled correctly

2. "Failed to load model":
   - Check model file exists and is readable
   - Ensure model is in GGUF format
   - Verify sufficient RAM/VRAM

3. Out of memory:
   - Reduce context-size
   - Reduce batch-size
   - Use smaller quantized model (Q4 instead of Q8)

4. Slow generation:
   - Increase threads
   - Enable GPU layers
   - Use smaller model
   - Reduce context size

5. JavaFX not found:
   - Add --module-path to javafx libs
   - Check JAVA_HOME points to JDK 25

==============================================================================
PERFORMANCE TIPS
==============================================================================

1. GPU Acceleration:
   - Set gpu-layers to number of model layers (usually 32-40)
   - Requires CUDA/Metal/Vulkan support in llama.cpp

2. CPU Optimization:
   - Set threads to number of physical cores
   - Use Q4_K_M quantization for best speed/quality balance

3. Memory Usage:
   - Reduce context-size if running out of RAM
   - Use smaller batch-size for lower memory

4. Batch Processing:
   - Larger batch-size = better throughput
   - May increase latency for single requests

==============================================================================
LICENSE & CREDITS
==============================================================================

This project uses:
- llama.cpp: MIT License
- Quarkus: Apache 2.0
- JavaFX: GPLv2 with Classpath Exception
- Jackson: Apache 2.0
- OkHttp: Apache 2.0

Make sure to comply with model licenses from HuggingFace.

==============================================================================
NEXT STEPS
==============================================================================

1. Compile llama.cpp as shared library
2. Download a GGUF model
3. Build the project: mvn clean install
4. Configure application.yml
5. Run server: mvn quarkus:dev
6. Or run JavaFX client: mvn javafx:run
7. Start chatting!

For issues and contributions:
- Check GitHub issues
- Read llama.cpp documentation
- Consult Quarkus guides

Happy coding with Llama! 
*/